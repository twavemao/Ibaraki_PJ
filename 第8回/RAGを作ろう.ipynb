{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの設定\n",
    "data = \"カブトムシ_raw.txt\"\n",
    "\n",
    "with open(data, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#クエリの設定\n",
    "question = \"洗えますか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#チャンク分けの設定\n",
    "chunk_size = 100\n",
    "\n",
    "#システムプロンプトの設定\n",
    "prompt = \"\"\"\n",
    "以下の情報を参照して、質問に答えてください。\n",
    "情報に含まれている内容のみで回答してください。\n",
    "\n",
    "[情報]\n",
    "{context}\n",
    "\n",
    "[質問]\n",
    "{question}\n",
    "\n",
    "[回答]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モジュールのインポート\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pykakasi\n",
    "import os\n",
    "import socket\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "data = \"カブトムシ_raw.txt\"  # RAGデータファイル\n",
    "with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunk_size = 100\n",
    "\n",
    "question = \"カブトムシのオスとメスの違いは？\"  # 質問例\n",
    "\n",
    "prompt = \"\"\"\n",
    "以下の情報をもとに質問に簡潔に回答してください。\n",
    "与えられた情報の内容のみを使用し、他の情報は使用しないでください。\n",
    "\n",
    "[情報]\n",
    "{context}\n",
    "\n",
    "[質問]\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#モデルの設定\n",
    "BASE_MODEL_NAME = \"./sbintuitions/sarashina2.2-3B-instruct-v0.1\"\n",
    "EMBEDDING_MODEL_NAME = \"./bge-m3\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FAISS_INDEX_DIR = \"faiss_index\"\n",
    "if not os.path.exists(FAISS_INDEX_DIR):\n",
    "    os.makedirs(FAISS_INDEX_DIR)\n",
    "\n",
    "def load_models(llm_name, emb_name):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    llm_tok = AutoTokenizer.from_pretrained(llm_name)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    llm_model.eval()\n",
    "    \n",
    "    emb_tok = AutoTokenizer.from_pretrained(emb_name)\n",
    "    emb_model = AutoModel.from_pretrained(emb_name)\n",
    "    \n",
    "    # エンベディングモデルを適切なデバイスに移動\n",
    "    emb_model = emb_model.to(DEVICE)\n",
    "    emb_model.eval()\n",
    "    \n",
    "    return llm_tok, llm_model, emb_tok, emb_model\n",
    "\n",
    "def chunking(text, chunk_size):\n",
    "    chunks = []\n",
    "    step = max(1, chunk_size)\n",
    "    \n",
    "    for i in range(0, len(text), step):\n",
    "        piece = text[i:i + chunk_size]\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "    return chunks\n",
    "\n",
    "def convert_to_romaji(filename):\n",
    "    try:\n",
    "        kakasi = pykakasi.kakasi()\n",
    "        name_without_ext = Path(filename).stem\n",
    "        \n",
    "        romaji_result = kakasi.convert(name_without_ext)\n",
    "        \n",
    "        # pykakasiの結果がリストの場合は、hepburn形式の文字列を結合\n",
    "        if isinstance(romaji_result, list):\n",
    "            romaji_name = \"\"\n",
    "            for item in romaji_result:\n",
    "                if isinstance(item, dict) and 'hepburn' in item:\n",
    "                    romaji_name += item['hepburn']\n",
    "                elif isinstance(item, str):\n",
    "                    romaji_name += item\n",
    "        else:\n",
    "            romaji_name = str(romaji_result)\n",
    "        \n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', romaji_name)\n",
    "        safe_name = re.sub(r'_+', '_', safe_name)\n",
    "        safe_name = safe_name.strip('_')\n",
    "        \n",
    "        return safe_name if safe_name else \"data\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"data\"\n",
    "\n",
    "def get_faiss_index_path(data_filename):\n",
    "    japanese_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "    \n",
    "    print(f\"DEBUG: ファイル名: {data_filename}\")\n",
    "    print(f\"DEBUG: 日本語判定: {japanese_pattern.search(data_filename) is not None}\")\n",
    "    \n",
    "    if japanese_pattern.search(data_filename):\n",
    "        romaji_name = convert_to_romaji(data_filename)\n",
    "        print(f\"DEBUG: ローマ字変換結果: {romaji_name}\")\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{romaji_name}.faiss\")\n",
    "    else:\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', data_filename)\n",
    "        print(f\"DEBUG: 英数字処理結果: {safe_name}\")\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{safe_name}.faiss\")\n",
    "\n",
    "def update_rag_data(data_path, text, chunk_size, emb_tok, emb_model):\n",
    "    current_filename = Path(data_path).name\n",
    "    print(f\"DEBUG: データパス: {data_path}\")\n",
    "    print(f\"DEBUG: 現在のファイル名: {current_filename}\")\n",
    "    \n",
    "    faiss_index_path = get_faiss_index_path(current_filename)\n",
    "    print(f\"DEBUG: 最終的なFAISSパス: {faiss_index_path}\")\n",
    "    \n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            faiss_index = faiss.read_index(faiss_index_path)\n",
    "            chunk_data = chunking(text, chunk_size)\n",
    "            print(f\"既存のFAISSインデックスを読み込みました: {faiss_index_path}\")\n",
    "            return faiss_index, chunk_data\n",
    "        except Exception as e:\n",
    "            print(f\"既存インデックスの読み込みに失敗: {e}\")\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        chunk_data = chunking(text, chunk_size)\n",
    "        chunk_embs = embed_texts(chunk_data, emb_tok, emb_model)\n",
    "        faiss_index = build_faiss_index(chunk_embs)\n",
    "        \n",
    "        faiss.write_index(faiss_index, faiss_index_path)\n",
    "        print(f\"新しいFAISSインデックスを保存しました: {faiss_index_path}\")\n",
    "        \n",
    "        return faiss_index, chunk_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"FAISSインデックス構築エラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts, tok: AutoTokenizer, model: AutoModel, batch_size: int = 32) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = model(**inputs)\n",
    "        emb = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "        emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "        vecs.append(emb.cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "def vector_search(question, chunks, emb_tok: AutoTokenizer, emb_model, index, k=5):\n",
    "    with torch.no_grad():\n",
    "        q_inputs = emb_tok([question], padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        q_out = emb_model(**q_inputs)\n",
    "        q_vec = mean_pooling(q_out, q_inputs[\"attention_mask\"])\n",
    "        q_vec = torch.nn.functional.normalize(q_vec, p=2, dim=1).cpu().numpy()\n",
    "    sims, ids = index.search(q_vec, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], sims[0]) if 0 <= i < len(chunks)]\n",
    "\n",
    "def _safe_format(template, **kwargs):\n",
    "    try:\n",
    "        return template.format(**kwargs)\n",
    "    except KeyError as e:\n",
    "        print(f\"フォーマットエラー: {e}\")\n",
    "        return template\n",
    "\n",
    "def build_prompt(prompt_template, question, retrieved_chunks):\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    user_content = _safe_format(prompt_template, question=question, context=context)\n",
    "    return user_content\n",
    "\n",
    "def generate_response(llm_tok, llm_model, prompt, max_length=512):\n",
    "    try:\n",
    "        inputs = llm_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = llm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=llm_tok.eos_token_id,\n",
    "                eos_token_id=llm_tok.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = llm_tok.decode(outputs[0], skip_special_tokens=True)\n",
    "        if prompt in response:\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"回答生成エラー: {e}\")\n",
    "        return \"回答の生成中にエラーが発生しました。\"\n",
    "\n",
    "def run_pipeline(question, chunks, emb_tok, emb_model, index, llm_tok, llm_model, prompt_template, k=5):\n",
    "    search_results = vector_search(question, chunks, emb_tok, emb_model, index, k)\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for chunk_id, similarity in search_results:\n",
    "        if 0 <= chunk_id < len(chunks):\n",
    "            retrieved_chunks.append(chunks[chunk_id])\n",
    "    \n",
    "    if not retrieved_chunks:\n",
    "        return \"関連する情報が見つかりませんでした。\"\n",
    "    \n",
    "    prompt = build_prompt(prompt_template, question, retrieved_chunks)\n",
    "    response = generate_response(llm_tok, llm_model, prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def main_rag_pipeline(data_path, text, chunk_size, question, prompt_template, k=5):\n",
    "    print(\"モデルを読み込み中...\")\n",
    "    llm_tok, llm_model, emb_tok, emb_model = load_models(BASE_MODEL_NAME, EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    print(\"RAGデータを更新中...\")\n",
    "    faiss_index, chunk_data = update_rag_data(data_path, text, chunk_size, emb_tok, emb_model)\n",
    "    \n",
    "    if faiss_index is None or chunk_data is None:\n",
    "        return \"RAGデータの準備に失敗しました。\"\n",
    "    \n",
    "    print(\"質問に回答中...\")\n",
    "    response = run_pipeline(\n",
    "        question, \n",
    "        chunk_data, \n",
    "        emb_tok, \n",
    "        emb_model, \n",
    "        faiss_index, \n",
    "        llm_tok, \n",
    "        llm_model, \n",
    "        prompt, \n",
    "        k\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response = main_rag_pipeline(data, text, chunk_size, question, prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
