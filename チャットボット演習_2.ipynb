{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c318f9",
   "metadata": {},
   "source": [
    "### ç¬¬1ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb7fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGãƒ‡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "DATA_FILE_PATH = \"å¤§è°·ç¿”å¹³_raw.txt\"  # ã“ã®å¤‰æ•°ã‚’å¤‰æ›´ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582923e4",
   "metadata": {},
   "source": [
    "### ç¬¬2ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092034a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./checkpoint-160\"  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ï¼ˆä¾‹: \"./chackpoint-100\"ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeae983",
   "metadata": {},
   "source": [
    "### ç¬¬3ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\ibaraki\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆèµ·å‹• ===\n",
      "1/3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "   - ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: ./sbintuitions/sarashina2.2-3B-instruct-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd903a8ac724673b671191a0bfa0780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "2/3: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "3/3: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰é©ç”¨ä¸­...\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­: ./output/checkpoint-160\n",
      "LoRAè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨å®Œäº†\n",
      "âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨å®Œäº†\n",
      "   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: ./sbintuitions/sarashina2.2-3B-instruct-v0.1\n",
      "   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: ./output/checkpoint-160\n",
      "ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "UIã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...\n",
      "ãƒãƒ¼ãƒˆ 7880 ã§èµ·å‹•ã—ã¾ã™...\n",
      "* Running on local URL:  http://0.0.0.0:7880\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« + RAG\n",
      "å‡¦ç†æ™‚é–“: 16.30ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿\n",
      "å‡¦ç†æ™‚é–“: 1.85ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: FTãƒ¢ãƒ‡ãƒ«ã®ã¿\n",
      "å‡¦ç†æ™‚é–“: 1.97ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿\n",
      "å‡¦ç†æ™‚é–“: 7.95ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: FTãƒ¢ãƒ‡ãƒ«ã®ã¿\n",
      "å‡¦ç†æ™‚é–“: 8.50ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: FTãƒ¢ãƒ‡ãƒ« + RAG\n",
      "å‡¦ç†æ™‚é–“: 7.64ç§’\n",
      "æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n",
      "ãƒ¢ãƒ¼ãƒ‰: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« + RAG\n",
      "å‡¦ç†æ™‚é–“: 12.45ç§’\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import faiss\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time  # æ™‚é–“è¨ˆæ¸¬ç”¨\n",
    "import pykakasi\n",
    "import socket\n",
    "\n",
    "# OpenMPã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ç’°å¢ƒå¤‰æ•°è¨­å®š\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# ==============\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šï¼ˆå¤–éƒ¨ã§å¤‰æ›´å¯èƒ½ï¼‰\n",
    "# ==============\n",
    "# ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ã“ã“ã§è¨­å®š\n",
    "DATA_FILE_PATH = \"å¤§è°·ç¿”å¹³_raw.txt\"  # ã“ã®å¤‰æ•°ã‚’å¤‰æ›´ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š\n",
    "\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š\n",
    "DEFAULT_LLM_MODEL_NAME = \"./sbintuitions/sarashina2.2-3B-instruct-v0.1\"\n",
    "DEFAULT_EMB_MODEL_NAME = \"./bge-m3\"\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./output/checkpoint-160\"  # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’æŒ‡å®šï¼ˆä¾‹: \"./output/checkpoint-160\" ã¾ãŸã¯ \"./my-lora-adapter\"ï¼‰\n",
    "\n",
    "class ChangeLLMChatbot:\n",
    "    def __init__(self):\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå¤–éƒ¨ã§è¨­å®šå¯èƒ½ï¼‰\n",
    "        self.text_path = DATA_FILE_PATH\n",
    "        \n",
    "        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š\n",
    "        self.llm_model_name = DEFAULT_LLM_MODEL_NAME\n",
    "        self.emb_model_name = DEFAULT_EMB_MODEL_NAME\n",
    "        self.lora_adapter_path = DEFAULT_LORA_ADAPTER_PATH\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— : original or finetuning\n",
    "        self.model_type = \"original\"\n",
    "        \n",
    "        # ç”Ÿæˆè¨­å®š\n",
    "        self.temperature = 0.7\n",
    "        self.answer_token_limit = 200\n",
    "        self.answer_headroom = 50\n",
    "        self.top_k_retrieve = 3\n",
    "        self.gen_mode = \"RAG\"  # \"RAG\" or \"FT\"\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n",
    "        self.chunk_size = 100\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # RAGè¨­å®š\n",
    "        self.rag_user_prompt_template = \"\"\n",
    "        self.use_chat_template = True\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "        self.ft_system_prompt = \"\"\n",
    "        \n",
    "        # å®Ÿè¡Œç’°å¢ƒ\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "cfg = ChangeLLMChatbot()\n",
    "\n",
    "\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ\n",
    "llm_tok = None\n",
    "llm = None  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå¸¸ã«ä¿æŒï¼‰\n",
    "llm_with_lora = None  # LoRAé©ç”¨æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆä¸€åº¦ã ã‘ä½œæˆï¼‰\n",
    "emb_tok = None\n",
    "emb_model = None\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®è¨­å®š\n",
    "FAISS_INDEX_DIR = \"faiss_index\"\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
    "if not os.path.exists(FAISS_INDEX_DIR):\n",
    "    os.makedirs(FAISS_INDEX_DIR)\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ\n",
    "faiss_index = None\n",
    "chunk_data = None\n",
    "last_data_file = None  # æœ€å¾Œã«èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# ==============\n",
    "# RAGãƒ‡ãƒ¼ã‚¿ç®¡ç†\n",
    "# ==============\n",
    "def convert_to_romaji(filename):\n",
    "    \"\"\"æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ­ãƒ¼ãƒå­—ã«å¤‰æ›\"\"\"\n",
    "    try:\n",
    "        # pykakasiã®æ–°ã—ã„APIã‚’ä½¿ç”¨\n",
    "        kakasi = pykakasi.kakasi()\n",
    "        \n",
    "        # æ‹¡å¼µå­ã‚’é™¤å»\n",
    "        name_without_ext = Path(filename).stem\n",
    "        \n",
    "        # ãƒ­ãƒ¼ãƒå­—å¤‰æ›\n",
    "        romaji_result = kakasi.convert(name_without_ext)\n",
    "        \n",
    "        # pykakasiã®çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ã€hepburnå½¢å¼ã®æ–‡å­—åˆ—ã‚’çµåˆ\n",
    "        if isinstance(romaji_result, list):\n",
    "            romaji_name = \"\"\n",
    "            for item in romaji_result:\n",
    "                if isinstance(item, dict) and 'hepburn' in item:\n",
    "                    romaji_name += item['hepburn']\n",
    "                elif isinstance(item, str):\n",
    "                    romaji_name += item\n",
    "        else:\n",
    "            romaji_name = str(romaji_result)\n",
    "        \n",
    "        # è‹±æ•°å­—ä»¥å¤–ã®æ–‡å­—ã‚’é™¤å»ã—ã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', romaji_name)\n",
    "        safe_name = re.sub(r'_+', '_', safe_name)  # é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’1ã¤ã«\n",
    "        safe_name = safe_name.strip('_')  # å‰å¾Œã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»\n",
    "        \n",
    "        return safe_name if safe_name else \"data\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"data\"\n",
    "\n",
    "def get_faiss_index_path(data_filename):\n",
    "    \"\"\"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\"\"\"\n",
    "    # æ—¥æœ¬èªåˆ¤å®šï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰\n",
    "    japanese_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "    \n",
    "    if japanese_pattern.search(data_filename):\n",
    "        # æ—¥æœ¬èªã®å ´åˆã€ãƒ­ãƒ¼ãƒå­—å¤‰æ›\n",
    "        romaji_name = convert_to_romaji(data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{romaji_name}.faiss\")\n",
    "    else:\n",
    "        # æ—¥æœ¬èªã§ãªã„å ´åˆã€ãã®ã¾ã¾ä½¿ç”¨\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{safe_name}.faiss\")\n",
    "\n",
    "def update_rag_data():\n",
    "    \"\"\"RAGãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆFAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æ¡ä»¶åˆ†å²ï¼‰\"\"\"\n",
    "    global faiss_index, chunk_data, last_data_file\n",
    "    \n",
    "    current_file = cfg.text_path\n",
    "    current_filename = Path(current_file).name  # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­å«ã‚€ï¼‰\n",
    "    \n",
    "    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    faiss_index_path = get_faiss_index_path(current_filename)\n",
    "    \n",
    "    # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "            faiss_index = faiss.read_index(faiss_index_path)\n",
    "            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚‚å†æ§‹ç¯‰\n",
    "            text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "            chunk_data = create_chunks(text)\n",
    "            last_data_file = current_file\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰\n",
    "    if current_file != last_data_file or faiss_index is None:\n",
    "        try:\n",
    "            text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "            chunk_data = create_chunks(text)\n",
    "            \n",
    "            # æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰\n",
    "            chunk_embs = embed_texts(chunk_data, emb_tok, emb_model)\n",
    "            faiss_index = build_faiss_index(chunk_embs)\n",
    "            \n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "            faiss.write_index(faiss_index, faiss_index_path)\n",
    "            \n",
    "            last_data_file = current_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==============\n",
    "# æ–‡å­—æ­£è¦åŒ–\n",
    "# ==============\n",
    "def _normalize(text: str) -> str:\n",
    "    # æ”¹è¡Œå‰Šé™¤ï¼‹NFKCï¼‹lower\n",
    "    return unicodedata.normalize(\"NFKC\", text.replace(\"\\n\", \"\")).lower()\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = None, overlap: bool = False, overlap_size: int = 20) -> List[str]:\n",
    "    #  æ”¹è¡Œå‰Šé™¤ï¼‹NFKCï¼‹lower\n",
    "    text = _normalize(text)\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # è¨­å®šå€¤ã‚’ä½¿ç”¨\n",
    "    if chunk_size is None:\n",
    "        chunk_size = cfg.chunk_size\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    step = max(1, chunk_size - overlap_size) if overlap else chunk_size\n",
    "    for i in range(0, len(text), step):\n",
    "        piece = text[i:i + chunk_size]\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "    return chunks\n",
    "    \n",
    "def load_generation_model():\n",
    "    \"\"\"ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ–å¯¾å¿œï¼‰\"\"\"\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        # å¸¸ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "        model_name = cfg.llm_model_name\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_name}\")\n",
    "        \n",
    "        # 4bité‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰\n",
    "        print(\"4bité‡å­åŒ–è¨­å®šã‚’ä½œæˆä¸­...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,  # äºŒé‡é‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ï¼ˆAI_chatbot.pyã¨åŒã˜ï¼‰\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "        print(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        global llm_tok\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’4bité‡å­åŒ–ã§èª­ã¿è¾¼ã¿ï¼ˆLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ç„¡è¦–ï¼‰\n",
    "        print(\"4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        global llm\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,  # AI_chatbot.pyã¨åŒã˜\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ç„¡è¦–ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’èª­ã¿è¾¼ã¿\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # pad_tokenãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®å‡¦ç†\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # vocab ã¨ lm_head ã®å‡ºåŠ›æ¬¡å…ƒã‚’ä¸€è‡´ï¼‹å¯èƒ½ãªã‚‰tie\n",
    "        vocab_size = llm_tok.vocab_size\n",
    "        head = getattr(llm, \"lm_head\", None)\n",
    "        head_out = getattr(head, \"out_features\", None) if head is not None else None\n",
    "        if head_out is not None and head_out != vocab_size:\n",
    "            print(f\"[WARN] lm_head({head_out}) != vocab({vocab_size}) â†’ resize\")\n",
    "            llm.resize_token_embeddings(vocab_size)\n",
    "        try:\n",
    "            llm.tie_weights()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if hasattr(llm, \"lm_head\") and hasattr(llm, \"model\") and hasattr(llm.model, \"embed_tokens\"):\n",
    "                    llm.lm_head.weight = llm.model.embed_tokens.weight\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        print(\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        llm.eval()\n",
    "        \n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def apply_lora_adapter(base_model, adapter_path):\n",
    "    \"\"\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ï¼ˆä¸€åº¦ã ã‘å®Ÿè¡Œï¼‰\"\"\"\n",
    "    try:\n",
    "        # ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {adapter_path}\")\n",
    "            return None\n",
    "        \n",
    "        from peft import PeftModel, PeftConfig\n",
    "        import warnings\n",
    "        \n",
    "        print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­: {adapter_path}\")\n",
    "        \n",
    "        # LoRAè¨­å®šã®èª­ã¿è¾¼ã¿\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "        print(f\"LoRAè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {peft_config.base_model_name_or_path}\")\n",
    "        \n",
    "        # è­¦å‘Šã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"peft\")\n",
    "            \n",
    "            # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨\n",
    "            lora_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "                is_trainable=False,  # æ¨è«–æ™‚ã¯False\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨å®Œäº†\")\n",
    "        return lora_model\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨ã«å¤±æ•—: {e}\")\n",
    "        return None\n",
    "        \n",
    "# =========\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¤–å‡ºã—å¯¾å¿œï¼‰\n",
    "# =========\n",
    "DEFAULT_RAG_USER = (\n",
    "    \"ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã ã‘ã§æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "    \"ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã«ãªã„ã“ã¨ã¯æ¨æ¸¬ã§æ›¸ã‹ãšã€ã‚ã‹ã‚‰ãªã„å ´åˆã¯ã‚ã‹ã‚‰ãªã„ã¨è¨€ã£ã¦ãã ã•ã„ã€‚\\n\\n\"\n",
    "    \"[è³ªå•]\\n{question}\\n\\n\"\n",
    "    \"[æƒ…å ±]\\n{context}\\n\\n\"\n",
    "    \"[å›ç­”]\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_FT_PROMPT = \"æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚æ ¹æ‹ ãŒç„¡ã„æ¨æ¸¬ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\"\n",
    "\n",
    "def _safe_format(template: str, **kwargs) -> str:\n",
    "    class _D(dict):\n",
    "        def __missing__(self, k): return \"{\"+k+\"}\"\n",
    "    return template.format_map(_D(**kwargs))\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_rag(cfg, question: str, retrieved_chunks: List[str], tok: AutoTokenizer) -> str:\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    user_tmpl = cfg.rag_user_prompt_template.strip() if cfg.rag_user_prompt_template else DEFAULT_RAG_USER\n",
    "    user_content = _safe_format(user_tmpl, question=question, context=context)\n",
    "\n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return user_content\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_ft(cfg, question: str, tok: AutoTokenizer) -> str:\n",
    "    system_tmpl = cfg.ft_system_prompt.strip() if cfg.ft_system_prompt else DEFAULT_FT_PROMPT\n",
    "    \n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_tmpl},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return question\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«åŒ–\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "def embed_texts(texts: List[str], emb_tok, emb_model, batch_size: int = None) -> np.ndarray:\n",
    "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\"\"\"\n",
    "    if emb_tok is None or emb_model is None:\n",
    "        return None\n",
    "    \n",
    "    # è¨­å®šå€¤ã‚’ä½¿ç”¨\n",
    "    if batch_size is None:\n",
    "        batch_size = cfg.batch_size\n",
    "    \n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = emb_tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = emb_model(**inputs)\n",
    "            emb = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "            vecs.append(emb.cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "    \n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "\n",
    "def vector_search_top_k(query: str,\n",
    "                        chunks: List[str],\n",
    "                        emb_tok: AutoTokenizer,\n",
    "                        emb_model: AutoModel,\n",
    "                        index,\n",
    "                        k: int) -> List[Tuple[int, float]]:\n",
    "    import faiss  # noqa\n",
    "    with torch.no_grad():\n",
    "        q_inputs = emb_tok([query], padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        q_out = emb_model(**q_inputs)\n",
    "        q_vec = mean_pooling(q_out, q_inputs[\"attention_mask\"])\n",
    "        q_vec = torch.nn.functional.normalize(q_vec, p=2, dim=1).cpu().numpy()\n",
    "    sims, ids = index.search(q_vec, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], sims[0]) if 0 <= i < len(chunks)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _snap_to_sentence_boundary(text: str) -> str:\n",
    "    m = re.search(r'(.+?[ã€‚ï¼.!ï¼?ï¼Ÿ])(?:[^ã€‚ï¼.!ï¼?ï¼Ÿ]*)$', text)\n",
    "    return m.group(1) if m else text\n",
    "\n",
    "def truncate_to_token_limit(tok: AutoTokenizer, text: str, limit: int) -> str:\n",
    "    ids = tok(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= limit:\n",
    "        return _snap_to_sentence_boundary(text)\n",
    "    clipped = tok.decode(ids[:limit], skip_special_tokens=True)\n",
    "    return _snap_to_sentence_boundary(clipped)\n",
    "\n",
    "# =====\n",
    "# ç”Ÿæˆ\n",
    "# =====\n",
    "@torch.no_grad()\n",
    "def generate_with_limit(cfg,\n",
    "                        prompt: str,\n",
    "                        llm_tok: AutoTokenizer,\n",
    "                        llm: AutoModelForCausalLM) -> str:\n",
    "    inputs = llm_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(cfg.device)\n",
    "    out = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=cfg.answer_token_limit + cfg.answer_headroom,\n",
    "        temperature=cfg.temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=llm_tok.pad_token_id,\n",
    "        eos_token_id=llm_tok.eos_token_id or llm_tok.pad_token_id,\n",
    "    )\n",
    "    raw = llm_tok.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é™¤å»ã—ã¦å›ç­”éƒ¨åˆ†ã®ã¿ã‚’å–å¾—\n",
    "    if \"[å›ç­”]\" in raw:\n",
    "        # RAGãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "        text = raw.split(\"[å›ç­”]\", 1)[-1].strip()\n",
    "    else:\n",
    "        # RAGãªã—ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»\n",
    "        prompt_tokens = llm_tok(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "        generated_tokens = out[0][len(prompt_tokens):]\n",
    "        text = llm_tok.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return truncate_to_token_limit(llm_tok, text, cfg.answer_token_limit)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿çŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹é–¢æ•°\n",
    "def get_model_status():\n",
    "    \"\"\"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’å–å¾—\"\"\"\n",
    "    status = {\n",
    "        \"llm_loaded\": llm_tok is not None and llm is not None,\n",
    "        \"emb_loaded\": emb_tok is not None and emb_model is not None,\n",
    "        \"lora_loaded\": llm_with_lora is not None\n",
    "    }\n",
    "    return status\n",
    "\n",
    "def get_model_status_text():\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§å–å¾—\"\"\"\n",
    "    status = get_model_status()\n",
    "    \n",
    "    status_text = \"ğŸ“Š **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ³**:\\n\"\n",
    "    status_text += f\"â€¢ ç”Ÿæˆãƒ¢ãƒ‡ãƒ«: {'âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿' if status['llm_loaded'] else 'âŒ æœªèª­ã¿è¾¼ã¿'}\\n\"\n",
    "    status_text += f\"â€¢ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {'âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿' if status['emb_loaded'] else 'âŒ æœªèª­ã¿è¾¼ã¿'}\\n\"\n",
    "    status_text += f\"â€¢ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {'âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿' if status['lora_loaded'] else 'âŒ æœªèª­ã¿è¾¼ã¿'}\\n\"\n",
    "    \n",
    "    return status_text\n",
    "\n",
    "# å®Ÿè¡Œãƒ‘ã‚¹\n",
    "def run_pipeline(question, rag_mode=\"\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"æ¨è«–å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆäº‹å‰èª­ã¿è¾¼ã¿æ¸ˆã¿ï¼‰\n",
    "    global llm_tok, llm, emb_tok, emb_model, llm_with_lora\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "    if llm_tok is None or llm is None:\n",
    "        return \"ã‚¨ãƒ©ãƒ¼: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\", 0.0\n",
    "\n",
    "    # === ãƒ¢ãƒ‡ãƒ«é¸æŠã¨ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š ===\n",
    "    is_ft = (cfg.model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "    current_model = llm_with_lora if is_ft else llm\n",
    "    use_rag = (rag_mode == \"RAGã‚ã‚Š\")\n",
    "    \n",
    "    print(f\"ãƒ¢ãƒ¼ãƒ‰: {'FT' if is_ft else 'ãƒ™ãƒ¼ã‚¹'}ãƒ¢ãƒ‡ãƒ«{' + RAG' if use_rag else 'ã®ã¿'}\")\n",
    "\n",
    "    # === RAGå‡¦ç†ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰ ===\n",
    "    retrieved = None\n",
    "    if use_rag:\n",
    "        if emb_tok is None or emb_model is None:\n",
    "            return \"ã‚¨ãƒ©ãƒ¼: RAGãƒ¢ãƒ¼ãƒ‰ã«ã¯åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™\", time.time() - start_time\n",
    "        if not update_rag_data():\n",
    "            return \"ã‚¨ãƒ©ãƒ¼: RAGãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ\", time.time() - start_time\n",
    "\n",
    "        first_stage = vector_search_top_k(question, chunk_data, emb_tok, emb_model, faiss_index, k=cfg.top_k_retrieve)\n",
    "        cand_ids = [i for i, _ in first_stage]\n",
    "        retrieved = [chunk_data[i] for i in cand_ids]\n",
    "\n",
    "    # === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ã¨æ¨è«– ===\n",
    "    if use_rag:\n",
    "        # RAGã‚ã‚Šï¼ˆãƒ™ãƒ¼ã‚¹ãƒ»FTå…±é€šã§RAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼‰\n",
    "        prompt = build_prompt_rag(cfg, question, retrieved, llm_tok)\n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "        answer += f\"\\n\\nğŸ“š **å‚è€ƒ**:\\n{retrieved[0]}\"\n",
    "    else:\n",
    "        # RAGãªã—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ»FTå…±é€šã§FTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼‰\n",
    "        prompt = build_prompt_ft(cfg, question, llm_tok)\n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "\n",
    "    print(f\"å‡¦ç†æ™‚é–“: {time.time() - start_time:.2f}ç§’\")\n",
    "    return answer, time.time() - start_time\n",
    "\n",
    "def load_clean_base_model():\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        model_name = cfg.llm_model_name\n",
    "        print(f\"ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # 4bité‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # åŸºæœ¬çš„ãªè¨­å®š\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        llm.eval()\n",
    "        \n",
    "        print(\"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
    "with gr.Blocks(title=\"ChangeLLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\"), css=\"\"\"\n",
    "    .large-text textarea {\n",
    "        font-size: 20px !important;\n",
    "    }\n",
    "\"\"\") as demo:\n",
    "    gr.Markdown(\"# ChangeLLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ¤–\")\n",
    "    gr.Markdown(\"LLMã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\")\n",
    "    \n",
    "    with gr.Tab(\"ãƒãƒ£ãƒƒãƒˆ\"):\n",
    "        gr.Markdown(\"## è¨­å®š\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                rag_mode_radio = gr.Radio(\n",
    "                    choices=[\"RAGã‚ã‚Š\", \"RAGãªã—\"],\n",
    "                    value=\"RAGã‚ã‚Š\",\n",
    "                    label=\"RAGãƒ¢ãƒ¼ãƒ‰\",\n",
    "                    info=\"RAGã‚ã‚Šï¼šRAGãƒ¢ãƒ¼ãƒ‰ã€RAGãªã—ï¼šLLMã®ã¿\"\n",
    "                )\n",
    "            \n",
    "            with gr.Column():\n",
    "                model_radio = gr.Radio(\n",
    "                    choices=[\"original\", \"finetuning\"],\n",
    "                    value=\"original\",\n",
    "                    label=\"ãƒ¢ãƒ‡ãƒ«é¸æŠ\",\n",
    "                    info=\"originalï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®sarashina2.2-3B-instruct-v0.1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã€finetuningï¼šLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ï¼ˆDEFAULT_LORA_ADAPTER_PATHã§æŒ‡å®šï¼‰\"\n",
    "                )\n",
    "        \n",
    "        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º\n",
    "        gr.Markdown(f\"**ğŸ“ ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«**: {DATA_FILE_PATH}\")\n",
    "        \n",
    "        gr.Markdown(\"## ãƒãƒ£ãƒƒãƒˆ\")\n",
    "        \n",
    "        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º\n",
    "        chatbot = gr.Chatbot(label=\"Chatbot\", type=\"messages\")\n",
    "        \n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›\n",
    "        msg = gr.Textbox(label=\"è³ªå•\", placeholder=\"è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "        # é€ä¿¡ãƒœã‚¿ãƒ³\n",
    "        submit_btn = gr.Button(\"é€ä¿¡\", variant=\"primary\")\n",
    "        \n",
    "        def respond(message, history, rag_mode, model_mode):\n",
    "            if message.strip() == \"\":\n",
    "                return \"\", history\n",
    "            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ›´æ–°\n",
    "            cfg.model_type = model_mode\n",
    "            response, response_time = run_pipeline(message, rag_mode)\n",
    "            \n",
    "            # å¿œç­”æ™‚é–“ã‚’å«ã‚€å›ç­”ã‚’ä½œæˆ\n",
    "            response_with_time = f\"{response}\\n\\nâ±ï¸ å¿œç­”æ™‚é–“: {response_time:.2f}ç§’\"\n",
    "            \n",
    "            # type=\"messages\"ã®å ´åˆã€historyã¯è¾æ›¸ã®ãƒªã‚¹ãƒˆå½¢å¼\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": response_with_time})\n",
    "            \n",
    "            return \"\", history\n",
    "        \n",
    "        # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³é¸æŠå¤‰æ›´æ™‚ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼\n",
    "        def on_model_change(model_type):\n",
    "            if model_type == \"original\":\n",
    "                cfg.model_type = \"original\"\n",
    "            else:\n",
    "                cfg.model_type = \"finetuning\"\n",
    "        \n",
    "\n",
    "        \n",
    "        def on_rag_mode_change(rag_mode):\n",
    "            if rag_mode == \"RAGãªã—\":\n",
    "                cfg.gen_mode = \"FT\"\n",
    "            else:\n",
    "                cfg.gen_mode = \"RAG\"\n",
    "            return rag_mode\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼\n",
    "        submit_btn.click(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, rag_mode_radio, model_radio],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Enterã‚­ãƒ¼ã§ã‚‚é€ä¿¡\n",
    "        msg.submit(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, rag_mode_radio, model_radio],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®é¸æŠå¤‰æ›´ã‚’ç›£è¦–\n",
    "        model_radio.change(\n",
    "            fn=on_model_change,\n",
    "            inputs=[model_radio],\n",
    "            outputs=[]\n",
    "        )\n",
    "        \n",
    "        rag_mode_radio.change(\n",
    "            fn=on_rag_mode_change,\n",
    "            inputs=[rag_mode_radio],\n",
    "            outputs=[rag_mode_radio]\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆèµ·å‹• ===\")\n",
    "    \n",
    "    # 1. ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "    print(\"1/3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    print(\"   - ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    llm_tok, llm = load_clean_base_model()\n",
    "    if llm_tok is None or llm is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "        exit(1)\n",
    "    print(\"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    \n",
    "    # 2. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆbge-m3ï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "    print(\"2/3: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    try:\n",
    "        emb_tok = AutoTokenizer.from_pretrained(cfg.emb_model_name)\n",
    "        emb_model = AutoModel.from_pretrained(cfg.emb_model_name).to(cfg.device).eval()\n",
    "        print(\"âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰ã«é©ç”¨\n",
    "    print(\"3/3: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰é©ç”¨ä¸­...\")\n",
    "    if os.path.exists(DEFAULT_LORA_ADAPTER_PATH):\n",
    "        try:\n",
    "            llm_with_lora = apply_lora_adapter(llm, DEFAULT_LORA_ADAPTER_PATH)\n",
    "            if llm_with_lora is not None:\n",
    "                print(\"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨å®Œäº†\")\n",
    "                print(f\"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {cfg.llm_model_name}\")\n",
    "                print(f\"   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "            else:\n",
    "                print(\"âš ï¸ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨ã«å¤±æ•—ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "                llm_with_lora = None\n",
    "        except Exception as e:\n",
    "            print(f\"è­¦å‘Š: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "            print(\"ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "            llm_with_lora = None\n",
    "    else:\n",
    "        print(\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "        llm_with_lora = None\n",
    "    \n",
    "    print(\"ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"UIã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    port = find_available_port(7880)\n",
    "    if port is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ãƒãƒ¼ãƒˆ {port} ã§èµ·å‹•ã—ã¾ã™...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ãŒä½¿ç”¨ä¸­ã§ã™ã€‚åˆ¥ã®ãƒãƒ¼ãƒˆã‚’è©¦ã—ã¾ã™...\")\n",
    "            # åˆ¥ã®ãƒãƒ¼ãƒˆã§å†è©¦è¡Œ\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ã§å†èµ·å‹•ã—ã¾ã™...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
