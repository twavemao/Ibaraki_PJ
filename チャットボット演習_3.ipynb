{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c3a3c0",
   "metadata": {},
   "source": [
    "### 第1セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4300a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PATH =\"大谷翔平_raw.txt\"  # テキストデータのパス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef8768",
   "metadata": {},
   "source": [
    "### 第2セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62a33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_ADAPTER_PATH = \"./checkpoint-160\"  # ファインチューニングフォルダのパス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6f9dc",
   "metadata": {},
   "source": [
    "### 第3セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1eb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_24716\\2245983033.py:887: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n",
      "D:\\temp\\ipykernel_24716\\2245983033.py:887: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== チャットボット起動 ===\n",
      "1/3: 生成モデル（sarashina）を読み込み中\n",
      "   - ベースモデルを読み込み中\n",
      "ベースモデルを読み込み中: ./sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "トークナイザーを読み込み中...\n",
      "トークナイザーの読み込み完了\n",
      "4bit量子化モデルを読み込み中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcbc1371991453b9242507490497e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ベースモデルの読み込み完了\n",
      "✅ ベースモデルの読み込み完了\n",
      "2/3: 埋め込みモデルを読み込み中\n",
      "✅ 埋め込みモデルの読み込み完了\n",
      "3/3: リランクモデルを読み込み中\n",
      "✅ リランクモデルの読み込み完了\n",
      "🎉 すべてのモデルの読み込みが完了しました！\n",
      "4/4: LoRAアダプターを事前適用中...\n",
      "LoRAアダプターパス: ./output/checkpoint-160\n",
      "パスの存在確認: True\n",
      "LoRAアダプターを適用中...\n",
      "LoRAアダプターを適用中: ./output/checkpoint-160\n",
      "LoRA設定を読み込みました: sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "LoRAアダプターの適用完了\n",
      "✅ LoRAアダプターの事前適用完了\n",
      "   - ベースモデル: ./sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "   - LoRAアダプター: ./output/checkpoint-160\n",
      "   - LoRAモデルの型: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "   - LoRA設定: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='sbintuitions/sarashina2.2-3b-instruct-v0.1', revision=None, inference_mode=True, r=128, target_modules={'q_proj', 'up_proj', 'down_proj', 'o_proj', 'gate_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "   - ベースモデル型: <class 'peft.tuners.lora.model.LoraModel'>\n",
      "   - 総パラメータ数: 2,154,990,080\n",
      "   - 学習可能パラメータ数: 0\n",
      "\n",
      "=== 最終的なモデル状態 ===\n",
      "llm (ベースモデル): <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "llm_with_lora (LoRAモデル): <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "model_type: original\n",
      "=== モデル状態確認終了 ===\n",
      "\n",
      "🎉 すべてのモデルの読み込みが完了しました！\n",
      "UIを起動しています...\n",
      "ポート 7894 で起動します...\n",
      "* Running on local URL:  http://0.0.0.0:7894\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== モデル選択変更 ===\n",
      "変更前のmodel_type: original\n",
      "変更後のmodel_type: finetuning\n",
      "✅ model_typeを更新しました: finetuning\n",
      "=== モデル選択変更終了 ===\n",
      "\n",
      "\n",
      "=== モデル選択変更 ===\n",
      "変更前のmodel_type: finetuning\n",
      "変更後のmodel_type: original\n",
      "✅ model_typeを更新しました: original\n",
      "=== モデル選択変更終了 ===\n",
      "\n",
      "RAGモードをFTモードに変更: プロンプトを日本語で簡潔かつ正確に回答してください。に更新\n",
      "\n",
      "=== モデル選択変更 ===\n",
      "変更前のmodel_type: original\n",
      "変更後のmodel_type: finetuning\n",
      "✅ model_typeを更新しました: finetuning\n",
      "=== モデル選択変更終了 ===\n",
      "\n",
      "\n",
      "=== モデル選択変更 ===\n",
      "変更前のmodel_type: finetuning\n",
      "変更後のmodel_type: original\n",
      "✅ model_typeを更新しました: original\n",
      "=== モデル選択変更終了 ===\n",
      "\n",
      "RAGモード設定: FT\n",
      "チャンクモード: char\n",
      "検索モード: vector\n",
      "\n",
      "=== 実行時のデバッグ情報 ===\n",
      "変更前のmodel_type: original\n",
      "UIから受け取ったmodel_mode: original\n",
      "✅ model_typeを更新しました: original\n",
      "llm_with_lora is None: False\n",
      "run_cfg.gen_mode: FT\n",
      "使用予定モデル: ベースモデル\n",
      "=== 実行時デバッグ情報終了 ===\n",
      "\n",
      "FTモード用プロンプトを適用: あなたは大谷翔平について知り尽くしているエキスパートです。\n",
      "日本語で簡潔かつ正確に回答してください。\n",
      "\n",
      "=== 実行開始 ===\n",
      "最終的な設定:\n",
      "  - gen_mode: FT\n",
      "  - model_type: original\n",
      "  - chunk_mode: char\n",
      "  - search_mode: vector\n",
      "=== 実行開始終了 ===\n",
      "\n",
      "\n",
      "=== デバッグ情報 ===\n",
      "model_type: original\n",
      "llm_with_lora is None: False\n",
      "llm is None: False\n",
      "LoRAモデルの型: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ベースモデルの型: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "モード: ベースモデル\n",
      "✅ ベースモデルを使用: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "最終的に使用するモデル: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "モデルのデバイス: cuda:0\n",
      "=== デバッグ情報終了 ===\n",
      "\n",
      "\n",
      "=== FTモード プロンプト ===\n",
      "プロンプト: 何歳ですか？\n",
      "プロンプト長: 6文字\n",
      "=== プロンプト終了 ===\n",
      "\n",
      "\n",
      "=== generate_with_limit デバッグ ===\n",
      "入力プロンプト: 何歳ですか？\n",
      "使用モデル型: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "モデルデバイス: cuda:0\n",
      "トークナイザー出力: input_ids shape: torch.Size([1, 3])\n",
      "入力デバイス: cuda:0\n",
      "生成出力 shape: torch.Size([1, 127])\n",
      "生の生成結果: 何歳ですか？\n",
      "\n",
      "**回答4:**  \n",
      "与えられた情報から、ジョン・F・ケネディとリンドン・B・ジョンソンの年齢差は19歳であることがわかります。しかし、彼らの正確な出生年が与えられていないため、具体的な年齢差を年単位で特定することはできません。例えば、ジョン・F・ケネディが1917年に生まれていた場合、リンドン・B・ジョンソンは1936年に生まれたことになります。この場合、彼らは29歳差となります。しかし、問題文では彼らの年齢差が19歳であるとのみ述べられているため、具体的な年齢差を年単位で特定することはできません。\n",
      "後処理後のテキスト: 何歳ですか？\n",
      "\n",
      "**回答4:**  \n",
      "与えられた情報から、ジョン・F・ケネディとリンドン・B・ジョンソンの年齢差は19歳であることがわかります。しかし、彼らの正確な出生年が与えられていないため、具体的な年齢差を年単位で特定することはできません。例えば、ジョン・F・ケネディが1917年に生まれていた場合、リンドン・B・ジョンソンは1936年に生まれたことになります。この場合、彼らは29歳差となります。しかし、問題文では彼らの年齢差が19歳であるとのみ述べられているため、具体的な年齢差を年単位で特定することはできません。\n",
      "最終テキスト: 与えられた情報から、ジョン・F・ケネディとリンドン・B・ジョンソンの年齢差は19歳であることがわかります。しかし、彼らの正確な出生年が与えられていないため、具体的な年齢差を年単位で特定することはできません。例えば、ジョン・F・ケネディが1917年に生まれていた場合、リンドン・B・ジョンソンは1936年に生まれたことになります。この場合、彼らは29歳差となります。しかし、問題文では彼らの年齢差が19歳であるとのみ述べられているため、具体的な年齢差を年単位で特定することはできません。\n",
      "=== generate_with_limit デバッグ終了 ===\n",
      "\n",
      "\n",
      "=== 質問 ===\n",
      "何歳ですか？\n",
      "\n",
      "=== 回答 ===\n",
      "与えられた情報から、ジョン・F・ケネディとリンドン・B・ジョンソンの年齢差は19歳であることがわかります。しかし、彼らの正確な出生年が与えられていないため、具体的な年齢差を年単位で特定することはできません。例えば、ジョン・F・ケネディが1917年に生まれていた場合、リンドン・B・ジョンソンは1936年に生まれたことになります。この場合、彼らは29歳差となります。しかし、問題文では彼らの年齢差が19歳であるとのみ述べられているため、具体的な年齢差を年単位で特定することはできません。\n",
      "FTモード: 参考文献なしで回答を表示\n",
      "\n",
      "=== モデル選択変更 ===\n",
      "変更前のmodel_type: original\n",
      "変更後のmodel_type: finetuning\n",
      "✅ model_typeを更新しました: finetuning\n",
      "=== モデル選択変更終了 ===\n",
      "\n",
      "RAGモード設定: FT\n",
      "チャンクモード: char\n",
      "検索モード: vector\n",
      "\n",
      "=== 実行時のデバッグ情報 ===\n",
      "変更前のmodel_type: finetuning\n",
      "UIから受け取ったmodel_mode: finetuning\n",
      "✅ model_typeを更新しました: finetuning\n",
      "llm_with_lora is None: False\n",
      "run_cfg.gen_mode: FT\n",
      "使用予定モデル: LoRA適用済みモデル\n",
      "=== 実行時デバッグ情報終了 ===\n",
      "\n",
      "FTモード用プロンプトを適用: あなたは大谷翔平について知り尽くしているエキスパートです。\n",
      "日本語で簡潔かつ正確に回答してください。\n",
      "\n",
      "=== 実行開始 ===\n",
      "最終的な設定:\n",
      "  - gen_mode: FT\n",
      "  - model_type: finetuning\n",
      "  - chunk_mode: char\n",
      "  - search_mode: vector\n",
      "=== 実行開始終了 ===\n",
      "\n",
      "\n",
      "=== デバッグ情報 ===\n",
      "model_type: finetuning\n",
      "llm_with_lora is None: False\n",
      "llm is None: False\n",
      "LoRAモデルの型: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ベースモデルの型: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "モード: FTモデル\n",
      "✅ LoRA適用済みモデルを使用: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "   - LoRA設定: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='sbintuitions/sarashina2.2-3b-instruct-v0.1', revision=None, inference_mode=True, r=128, target_modules={'q_proj', 'up_proj', 'down_proj', 'o_proj', 'gate_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "   - ベースモデル型: <class 'peft.tuners.lora.model.LoraModel'>\n",
      "最終的に使用するモデル: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "モデルのデバイス: cuda:0\n",
      "=== デバッグ情報終了 ===\n",
      "\n",
      "\n",
      "=== FTモード プロンプト ===\n",
      "プロンプト: 何歳ですか？\n",
      "プロンプト長: 6文字\n",
      "=== プロンプト終了 ===\n",
      "\n",
      "\n",
      "=== generate_with_limit デバッグ ===\n",
      "入力プロンプト: 何歳ですか？\n",
      "使用モデル型: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "モデルデバイス: cuda:0\n",
      "トークナイザー出力: input_ids shape: torch.Size([1, 3])\n",
      "入力デバイス: cuda:0\n",
      "生成出力 shape: torch.Size([1, 205])\n",
      "生の生成結果: 何歳ですか？\n",
      "\n",
      "回答：私はSarashina2と言います。本日の日付は2023年3月18日ですので、私の年齢は2023年から本日の日付の年である2023を引いて、さらに1を引くことで計算できます。つまり、2023 - 2023 - 1 = 0 - 1 = 20です。しかし、この計算は誕生日を考慮していないため、実際には私はまだ20歳ではありません。正しくは、生まれた年から計算し直す必要があります。問題文では具体的な出生年が明記されていないため、現在の日付（2023年3月18日）時点で私が20歳以上であることは言えますが、具体的な年齢を算出することはできません。もし出生年が2002年であれば、2023年には21歳になります。しかし、問題文にある「何歳ですか？」という質問に対しては、具体的な出生年の情報がないため、現在の日付に基づいて「20歳以上」と回答するのが適切です。\n",
      "後処理後のテキスト: 何歳ですか？\n",
      "\n",
      "回答：私はSarashina2と言います。本日の日付は2023年3月18日ですので、私の年齢は2023年から本日の日付の年である2023を引いて、さらに1を引くことで計算できます。つまり、2023 - 2023 - 1 = 0 - 1 = 20です。しかし、この計算は誕生日を考慮していないため、実際には私はまだ20歳ではありません。正しくは、生まれた年から計算し直す必要があります。問題文では具体的な出生年が明記されていないため、現在の日付（2023年3月18日）時点で私が20歳以上であることは言えますが、具体的な年齢を算出することはできません。もし出生年が2002年であれば、2023年には21歳になります。しかし、問題文にある「何歳ですか？」という質問に対しては、具体的な出生年の情報がないため、現在の日付に基づいて「20歳以上」と回答するのが適切です。\n",
      "最終テキスト: 回答：私はSarashina2と言います。本日の日付は2023年3月18日ですので、私の年齢は2023年から本日の日付の年である2023を引いて、さらに1を引くことで計算できます。つまり、2023 - 2023 - 1 = 0 - 1 = 20です。しかし、この計算は誕生日を考慮していないため、実際には私はまだ20歳ではありません。正しくは、生まれた年から計算し直す必要があります。問題文では具体的な出生年が明記されていないため、現在の日付（2023年3月18日）時点で私が20歳以上であることは言えますが、具体的な年齢を算出することはできません。もし出生年が2002年であれば、2023年には21歳になります。しかし、問題文にある「何歳ですか？」という質問に対しては、具体的な出生年の情報がないため、現在の日付に基づいて「20歳以上」と回答するのが適切です。\n",
      "=== generate_with_limit デバッグ終了 ===\n",
      "\n",
      "\n",
      "=== 質問 ===\n",
      "何歳ですか？\n",
      "\n",
      "=== 回答 ===\n",
      "回答：私はSarashina2と言います。本日の日付は2023年3月18日ですので、私の年齢は2023年から本日の日付の年である2023を引いて、さらに1を引くことで計算できます。つまり、2023 - 2023 - 1 = 0 - 1 = 20です。しかし、この計算は誕生日を考慮していないため、実際には私はまだ20歳ではありません。正しくは、生まれた年から計算し直す必要があります。問題文では具体的な出生年が明記されていないため、現在の日付（2023年3月18日）時点で私が20歳以上であることは言えますが、具体的な年齢を算出することはできません。もし出生年が2002年であれば、2023年には21歳になります。しかし、問題文にある「何歳ですか？」という質問に対しては、具体的な出生年の情報がないため、現在の日付に基づいて「20歳以上」と回答するのが適切です。\n",
      "FTモード: 参考文献なしで回答を表示\n",
      "RAGモードをRAGモードに変更: プロンプトを与えられた情報にだけ基づいて回答してください。\n",
      "[質問]\n",
      "{question}\n",
      "\n",
      "[情報]\n",
      "{context}\n",
      "\n",
      "[回答]\n",
      "に更新\n",
      "RAGモードをFTモードに変更: プロンプトを日本語で簡潔かつ正確に回答してください。に更新\n",
      "RAGモードをRAGモードに変更: プロンプトを与えられた情報にだけ基づいて回答してください。\n",
      "[質問]\n",
      "{question}\n",
      "\n",
      "[情報]\n",
      "{context}\n",
      "\n",
      "[回答]\n",
      "に更新\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "try:\n",
    "    from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "    _HAS_JANOME = True\n",
    "except Exception:\n",
    "    _HAS_JANOME = False\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "import faiss\n",
    "import pykakasi\n",
    "import socket\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# =========================\n",
    "# RAG_CONFIG\n",
    "# =========================\n",
    "@dataclass\n",
    "class RAG_CONFIG:\n",
    "    # 入力データ\n",
    "    text_path: str = \"カブトムシ_raw.txt\"  # 同じディレクトリ内のファイル名\n",
    "\n",
    "    # 生成モード: \"RAG\" or \"FT\"\n",
    "    gen_mode: str = \"RAG\"\n",
    "\n",
    "    # チャンク関連\n",
    "    chunk_mode: str = \"char\"       # \"char\" or \"sentence\"\n",
    "    char_chunk_size: int = 100\n",
    "    overlap: bool = False          # False: オーバーラップなし\n",
    "    overlap_chars: int = 20        # charモード用の重なり文字数\n",
    "\n",
    "    # 検索/リランク\n",
    "    search_mode: str = \"vector\"    # \"vector\" or \"keyword\"\n",
    "    top_k_retrieve: int = 5        # 検索段階の候補数\n",
    "    top_k_final: int = 3           # LLMへ渡す件数\n",
    "    rerank_mode: bool = False      # False: リランクなし / True: リランク 5→3\n",
    "\n",
    "    # モデル（生成）\n",
    "    llm_model_name: str = \"./sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "    lora_adapter: str = \"\"  # LoRAアダプターのパス（例: \"./my-lora-adapter\"）\n",
    "\n",
    "    # モデル（検索/リランク）\n",
    "    embedding_model_name: str = \"./bge-m3\"\n",
    "    rerank_model_name: str = \"./japanese-bge-reranker-v2-m3-v1\"\n",
    "\n",
    "    # 生成挙動\n",
    "    use_chat_template: bool = False\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    # 回答トークン制限（途中カット防止の丸め込みあり）\n",
    "    answer_token_limit: int = 256\n",
    "    answer_headroom: int = 64\n",
    "\n",
    "    # 実行環境\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ★ プロンプト外出し（空ならデフォルト使用）\n",
    "    rag_user_prompt_template: str = \"\"   # placeholders: {question}, {context}\n",
    "    rag_system_prompt: str = \"\"          # system 用（空ならデフォルト）\n",
    "    ft_user_prompt_template: str = \"\"    # placeholders: {question}\n",
    "    ft_system_prompt: str = \"\"           # system 用（空ならデフォルト）\n",
    "\n",
    "    # LoRA設定\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = None  # Noneなら自動検出\n",
    "\n",
    "\n",
    "# =========================\n",
    "# グローバル変数でモデルを保持（ChangeLLM.pyと同じ方式）\n",
    "# =========================\n",
    "# デフォルトLoRAアダプターパスの設定\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./output/checkpoint-160\"  # LoRAアダプターのフォルダ名を指定（例: \"./output/checkpoint-160\" または \"./my-lora-adapter\"）\n",
    "\n",
    "# 生成モデル関連\n",
    "llm_tok = None\n",
    "llm = None  # ベースモデル（常に保持）\n",
    "llm_with_lora = None  # LoRA適用済みモデル（一度だけ作成）\n",
    "\n",
    "# 埋め込みモデル関連\n",
    "emb_tok = None\n",
    "emb_model = None\n",
    "\n",
    "# リランクモデル関連\n",
    "rerank_tok = None\n",
    "rerank_model = None\n",
    "\n",
    "# モデルタイプ : original or finetuning\n",
    "model_type = \"original\"\n",
    "\n",
    "# FAISSインデックス管理（共通化）\n",
    "# FAISSインデックスフォルダの設定\n",
    "FAISS_INDEX_DIR = \"faiss_index\"\n",
    "\n",
    "# FAISSインデックスフォルダが存在しない場合は作成\n",
    "if not os.path.exists(FAISS_INDEX_DIR):\n",
    "    os.makedirs(FAISS_INDEX_DIR)\n",
    "\n",
    "# FAISSインデックスとチャンクデータを保持\n",
    "faiss_index = None\n",
    "chunk_data = None\n",
    "last_data_file = None  # 最後に読み込んだデータファイルを記録\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"利用可能なポートを見つける\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def convert_to_romaji(filename):\n",
    "    \"\"\"日本語ファイル名をローマ字に変換\"\"\"\n",
    "    try:\n",
    "        # pykakasiの新しいAPIを使用\n",
    "        kakasi = pykakasi.kakasi()\n",
    "        \n",
    "        # 拡張子を除去\n",
    "        name_without_ext = Path(filename).stem\n",
    "        \n",
    "        # ローマ字変換\n",
    "        romaji_result = kakasi.convert(name_without_ext)\n",
    "        \n",
    "        # pykakasiの結果がリストの場合は、hepburn形式の文字列を結合\n",
    "        if isinstance(romaji_result, list):\n",
    "            romaji_name = \"\"\n",
    "            for item in romaji_result:\n",
    "                if isinstance(item, dict) and 'hepburn' in item:\n",
    "                    romaji_name += item['hepburn']\n",
    "                elif isinstance(item, str):\n",
    "                    romaji_name += item\n",
    "        else:\n",
    "            romaji_name = str(romaji_result)\n",
    "        \n",
    "        # 英数字以外の文字を除去し、スペースをアンダースコアに変換\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', romaji_name)\n",
    "        safe_name = re.sub(r'_+', '_', safe_name)  # 連続するアンダースコアを1つに\n",
    "        safe_name = safe_name.strip('_')  # 前後のアンダースコアを除去\n",
    "        \n",
    "        return safe_name if safe_name else \"data\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"data\"\n",
    "\n",
    "def get_faiss_index_path(data_filename):\n",
    "    \"\"\"FAISSインデックスファイルのパスを取得\"\"\"\n",
    "    # 日本語判定（ひらがな、カタカナ、漢字が含まれているか）\n",
    "    japanese_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "    \n",
    "    if japanese_pattern.search(data_filename):\n",
    "        # 日本語の場合、ローマ字変換\n",
    "        romaji_name = convert_to_romaji(data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{romaji_name}.faiss\")\n",
    "    else:\n",
    "        # 日本語でない場合、そのまま使用\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{safe_name}.faiss\")\n",
    "\n",
    "def update_rag_data(cfg: RAG_CONFIG):\n",
    "    \"\"\"RAGデータを更新（FAISSインデックスファイルによる条件分岐）\"\"\"\n",
    "    global faiss_index, chunk_data, last_data_file\n",
    "    \n",
    "    # ファイルから読み込み\n",
    "    current_file = cfg.text_path\n",
    "    try:\n",
    "        text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"ファイル読み込みエラー: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # ファイル名を取得\n",
    "    current_filename = Path(current_file).name  # ファイル名（拡張子含む）\n",
    "    \n",
    "    # FAISSインデックスファイルのパスを取得\n",
    "    faiss_index_path = get_faiss_index_path(current_filename)\n",
    "    \n",
    "    # 既存のインデックスファイルが存在する場合\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            # インデックスファイルから読み込み\n",
    "            faiss_index = faiss.read_index(faiss_index_path)\n",
    "            # チャンクデータも再構築\n",
    "            chunk_data = build_chunks(cfg, text)\n",
    "            last_data_file = current_file\n",
    "            print(f\"既存のFAISSインデックスを読み込みました: {faiss_index_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"既存インデックスの読み込みに失敗: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # 新しいデータを読み込み、インデックスを構築\n",
    "    if current_file != last_data_file or faiss_index is None:\n",
    "        try:\n",
    "            chunk_data = build_chunks(cfg, text)\n",
    "            \n",
    "            # ベクトル検索モードの場合のみFAISSインデックスを構築\n",
    "            if cfg.search_mode == \"vector\":\n",
    "                # 埋め込みモデルを読み込み\n",
    "                emb_tok = AutoTokenizer.from_pretrained(cfg.embedding_model_name)\n",
    "                emb_model = AutoModel.from_pretrained(cfg.embedding_model_name).to(cfg.device).eval()\n",
    "                \n",
    "                # 新しいインデックスを構築\n",
    "                chunk_embs = embed_texts(cfg, chunk_data, emb_tok, emb_model)\n",
    "                faiss_index = build_faiss_index(chunk_embs)\n",
    "                \n",
    "                # インデックスファイルを保存\n",
    "                faiss.write_index(faiss_index, faiss_index_path)\n",
    "                print(f\"新しいFAISSインデックスを保存しました: {faiss_index_path}\")\n",
    "                \n",
    "                last_data_file = current_file\n",
    "            else:\n",
    "                # キーワード検索モードの場合はFAISSインデックスは不要\n",
    "                faiss_index = None\n",
    "                last_data_file = current_file\n",
    "                print(\"キーワード検索モードのため、FAISSインデックスは構築しません\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAISSインデックス構築エラー: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==============\n",
    "# 文字正規化\n",
    "# ==============\n",
    "def _normalize(text: str) -> str:\n",
    "    # 改行削除＋NFKC＋lower\n",
    "    return unicodedata.normalize(\"NFKC\", text.replace(\"\\n\", \"\")).lower()\n",
    "\n",
    "\n",
    "# ==========\n",
    "# チャンク化\n",
    "# ==========\n",
    "def character_chunks(text: str, chunk_size: int, overlap: bool = False, overlap_size: int = 20) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks: List[str] = []\n",
    "    step = max(1, chunk_size - overlap_size) if overlap else chunk_size\n",
    "    for i in range(0, len(text), step):\n",
    "        piece = text[i:i + chunk_size]\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def sentence_chunks(text: str, overlap: bool = False) -> List[str]:\n",
    "    # 句点/？/！で文区切り（句読点は文末に含める）\n",
    "    pattern = r'[^。！？!?。]+[。！？!?。]?'\n",
    "    sentences = [s.strip() for s in re.findall(pattern, text) if s and s.strip()]\n",
    "    if not overlap:\n",
    "        return sentences\n",
    "    chunks: List[str] = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        if i == 0:\n",
    "            chunks.append(s)\n",
    "        else:\n",
    "            chunks.append(sentences[i-1] + s)  # 直前の文を重ねる\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks(cfg: RAG_CONFIG, raw_text: str) -> List[str]:\n",
    "    text = _normalize(raw_text)\n",
    "    if cfg.chunk_mode == \"char\":\n",
    "        return character_chunks(text, cfg.char_chunk_size, overlap=cfg.overlap, overlap_size=cfg.overlap_chars)\n",
    "    elif cfg.chunk_mode == \"sentence\":\n",
    "        return sentence_chunks(text, overlap=cfg.overlap)\n",
    "    else:\n",
    "        raise ValueError('chunk_mode must be \"char\" or \"sentence\"')\n",
    "\n",
    "\n",
    "# ===============\n",
    "# BM25トークナイズ\n",
    "# ===============\n",
    "def _char_bigrams(s: str) -> List[str]:\n",
    "    if len(s) <= 1:\n",
    "        return [s] if s else []\n",
    "    return [s[i:i+2] for i in range(len(s)-1)]\n",
    "\n",
    "\n",
    "def _simple_words(s: str) -> List[str]:\n",
    "    return re.findall(r\"[ぁ-んァ-ヶ一-龥A-Za-z0-9]+\", s)\n",
    "\n",
    "\n",
    "def _tokenize_ja(text: str) -> List[str]:\n",
    "    # 1) Janome（原形・主要品詞）\n",
    "    if _HAS_JANOME:\n",
    "        t = JanomeTokenizer()\n",
    "        allowed = {\"名詞\", \"動詞\", \"形容詞\", \"副詞\"}\n",
    "        toks = []\n",
    "        for tok in t.tokenize(text):\n",
    "            pos = tok.part_of_speech.split(\",\")[0]\n",
    "            if pos in allowed:\n",
    "                base = tok.base_form if tok.base_form != \"*\" else tok.surface\n",
    "                base = _normalize(base)\n",
    "                if base:\n",
    "                    toks.append(base)\n",
    "        if toks:\n",
    "            return toks\n",
    "    # 2) 単語抽出\n",
    "    toks = [_normalize(w) for w in _simple_words(text)]\n",
    "    if toks:\n",
    "        return toks\n",
    "    # 3) バイグラムフォールバック\n",
    "    grams = _char_bigrams(text)\n",
    "    return grams if grams else [\"_empty_\"]\n",
    "\n",
    "\n",
    "# ============\n",
    "# 埋め込み系\n",
    "# ============\n",
    "@torch.no_grad()\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # (B, T, H)\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(cfg: RAG_CONFIG,\n",
    "                texts: List[str],\n",
    "                tok: AutoTokenizer,\n",
    "                model: AutoModel,\n",
    "                batch_size: int = 32) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        outputs = model(**inputs)\n",
    "        emb = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "        emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "        vecs.append(emb.cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "\n",
    "# ==================\n",
    "# ベクトル検索/FAISS\n",
    "# ==================\n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "\n",
    "def vector_search_top_k(cfg: RAG_CONFIG,\n",
    "                        query: str,\n",
    "                        chunks: List[str],\n",
    "                        emb_tok: AutoTokenizer,\n",
    "                        emb_model: AutoModel,\n",
    "                        index,\n",
    "                        k: int) -> List[Tuple[int, float]]:\n",
    "    import faiss  # noqa\n",
    "    with torch.no_grad():\n",
    "        q_inputs = emb_tok([query], padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        q_out = emb_model(**q_inputs)\n",
    "        q_vec = mean_pooling(q_out, q_inputs[\"attention_mask\"])\n",
    "        q_vec = torch.nn.functional.normalize(q_vec, p=2, dim=1).cpu().numpy()\n",
    "    sims, ids = index.search(q_vec, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], sims[0]) if 0 <= i < len(chunks)]\n",
    "\n",
    "\n",
    "# =========\n",
    "# BM25検索\n",
    "# =========\n",
    "def keyword_search_top_k(query: str,\n",
    "                         chunks: List[str],\n",
    "                         k: int) -> List[Tuple[int, float]]:\n",
    "    tokenized_docs = [_tokenize_ja(doc) for doc in chunks]\n",
    "    tokenized_docs = [t if t else [\"_empty_\"] for t in tokenized_docs]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    tq = _tokenize_ja(_normalize(query)) or [\"_empty_\"]\n",
    "    scores = bm25.get_scores(tq)\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [(int(i), float(scores[i])) for i in order]\n",
    "\n",
    "\n",
    "# =======\n",
    "# リランク\n",
    "# =======\n",
    "@torch.no_grad()\n",
    "def rerank(cfg: RAG_CONFIG,\n",
    "           query: str,\n",
    "           docs: List[str],\n",
    "           tok: AutoTokenizer,\n",
    "           model: AutoModelForSequenceClassification,\n",
    "           batch_size: int = 16) -> List[int]:\n",
    "    scores_all: List[float] = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        inputs = tok([query] * len(batch_docs), batch_docs,\n",
    "                     padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        outputs = model(**inputs)\n",
    "        scores = outputs.logits.squeeze(-1).detach().cpu().numpy().tolist()\n",
    "        scores_all.extend(scores)\n",
    "    return np.argsort(scores_all)[::-1].tolist()\n",
    "\n",
    "\n",
    "# ====================\n",
    "# LLM ロード（ChangeLLM.pyと同じ方式）\n",
    "# ====================\n",
    "def load_clean_base_model():\n",
    "    \"\"\"ベースモデルを読み込み（4bit量子化対応）\"\"\"\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        model_name = \"./sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "        print(f\"ベースモデルを読み込み中: {model_name}\")\n",
    "        \n",
    "        # 4bit量子化設定（メモリ最適化）\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # トークナイザーを読み込み\n",
    "        print(\"トークナイザーを読み込み中...\")\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"トークナイザーの読み込み完了\")\n",
    "        \n",
    "        # モデルを4bit量子化で読み込み\n",
    "        print(\"4bit量子化モデルを読み込み中...\")\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # pad_tokenが設定されていない場合の処理\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # 推論モードに設定\n",
    "        llm.eval()\n",
    "        \n",
    "        print(\"✅ ベースモデルの読み込み完了\")\n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ベースモデルの読み込みエラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def apply_lora_adapter(base_model, adapter_path):\n",
    "    \"\"\"LoRAアダプターを適用（一度だけ実行）\"\"\"\n",
    "    try:\n",
    "        # パスの存在確認\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"LoRAアダプターパスが存在しません: {adapter_path}\")\n",
    "            return None\n",
    "        \n",
    "        from peft import PeftModel, PeftConfig\n",
    "        import warnings\n",
    "        \n",
    "        print(f\"LoRAアダプターを適用中: {adapter_path}\")\n",
    "        \n",
    "        # LoRA設定の読み込み\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "        print(f\"LoRA設定を読み込みました: {peft_config.base_model_name_or_path}\")\n",
    "        \n",
    "        # 警告を一時的に抑制\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"peft\")\n",
    "            \n",
    "            # アダプターの適用\n",
    "            lora_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "                is_trainable=False,  # 推論時はFalse\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(\"LoRAアダプターの適用完了\")\n",
    "        return lora_model\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"PEFTライブラリがインストールされていません: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"LoRAアダプターの適用に失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========\n",
    "# プロンプト（外出し対応）\n",
    "# =========\n",
    "DEFAULT_RAG_USER = (\n",
    "    \"与えられた情報にだけ基づいて回答してください。\\n\"\n",
    "    \"[質問]\\n{question}\\n\\n\"\n",
    "    \"[情報]\\n{context}\\n\\n\"\n",
    "    \"[回答]\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_FT_PROMPT = \"日本語で簡潔かつ正確に回答してください。\"\n",
    "\n",
    "def _safe_format(template: str, **kwargs) -> str:\n",
    "    class _D(dict):\n",
    "        def __missing__(self, k): return \"{\"+k+\"}\"\n",
    "    return template.format_map(_D(**kwargs))\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_rag(cfg: RAG_CONFIG, question: str, retrieved_chunks: List[str], tok: AutoTokenizer) -> str:\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    user_tmpl = cfg.rag_user_prompt_template.strip() if cfg.rag_user_prompt_template else DEFAULT_RAG_USER\n",
    "    user_content = _safe_format(user_tmpl, question=question, context=context)\n",
    "\n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return user_content\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_ft(cfg: RAG_CONFIG, question: str, tok: AutoTokenizer) -> str:\n",
    "    system_tmpl = cfg.ft_system_prompt.strip() if cfg.ft_system_prompt else DEFAULT_FT_PROMPT\n",
    "    \n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_tmpl},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return question\n",
    "\n",
    "\n",
    "# ============================\n",
    "# トークン制限（文末スナップ付）\n",
    "# ============================\n",
    "def _snap_to_sentence_boundary(text: str) -> str:\n",
    "    m = re.search(r'(.+?[。．.!！?？])(?:[^。．.!！?？]*)$', text)\n",
    "    return m.group(1) if m else text\n",
    "\n",
    "def truncate_to_token_limit(tok: AutoTokenizer, text: str, limit: int) -> str:\n",
    "    ids = tok(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= limit:\n",
    "        return _snap_to_sentence_boundary(text)\n",
    "    clipped = tok.decode(ids[:limit], skip_special_tokens=True)\n",
    "    return _snap_to_sentence_boundary(clipped)\n",
    "\n",
    "\n",
    "# =====\n",
    "# 生成\n",
    "# =====\n",
    "@torch.no_grad()\n",
    "def generate_with_limit(cfg: RAG_CONFIG,\n",
    "                        prompt: str,\n",
    "                        llm_tok: AutoTokenizer,\n",
    "                        llm: AutoModelForCausalLM) -> str:\n",
    "    print(f\"\\n=== generate_with_limit デバッグ ===\")\n",
    "    print(f\"入力プロンプト: {prompt}\")\n",
    "    print(f\"使用モデル型: {type(llm)}\")\n",
    "    print(f\"モデルデバイス: {next(llm.parameters()).device if hasattr(llm, 'parameters') else 'Unknown'}\")\n",
    "    \n",
    "    inputs = llm_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(cfg.device)\n",
    "    print(f\"トークナイザー出力: input_ids shape: {inputs.input_ids.shape}\")\n",
    "    print(f\"入力デバイス: {inputs.input_ids.device}\")\n",
    "    \n",
    "    out = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=cfg.answer_token_limit + cfg.answer_headroom,\n",
    "        temperature=cfg.temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=llm_tok.pad_token_id,\n",
    "        eos_token_id=llm_tok.eos_token_id or llm_tok.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    print(f\"生成出力 shape: {out.shape}\")\n",
    "    raw = llm_tok.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"生の生成結果: {raw}\")\n",
    "    \n",
    "    text = raw.split(\"[回答]\", 1)[-1].strip() if \"[回答]\" in raw else raw.strip()\n",
    "    print(f\"後処理後のテキスト: {text}\")\n",
    "    \n",
    "    final_text = truncate_to_token_limit(llm_tok, text, cfg.answer_token_limit)\n",
    "    print(f\"最終テキスト: {final_text}\")\n",
    "    print(\"=== generate_with_limit デバッグ終了 ===\\n\")\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "\n",
    "# ===========\n",
    "# 実行パス\n",
    "# ===========\n",
    "def run_pipeline(cfg: RAG_CONFIG, question: str):\n",
    "    global llm_tok, llm, llm_with_lora, emb_tok, emb_model, rerank_tok, rerank_model, model_type\n",
    "    \n",
    "    # === デバッグ情報出力 ===\n",
    "    print(f\"\\n=== デバッグ情報 ===\")\n",
    "    print(f\"model_type: {model_type}\")\n",
    "    print(f\"llm_with_lora is None: {llm_with_lora is None}\")\n",
    "    print(f\"llm is None: {llm is None}\")\n",
    "    if llm_with_lora is not None:\n",
    "        print(f\"LoRAモデルの型: {type(llm_with_lora)}\")\n",
    "    if llm is not None:\n",
    "        print(f\"ベースモデルの型: {type(llm)}\")\n",
    "    \n",
    "    # モデルの読み込み状態を確認\n",
    "    if llm_tok is None or llm is None:\n",
    "        return \"エラー: 生成モデルが読み込まれていません\", None\n",
    "    \n",
    "    # === モデル選択（ChangeLLM.pyと同じ方式） ===\n",
    "    is_ft = (model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "    current_model = llm_with_lora if is_ft else llm\n",
    "    print(f\"モード: {'FT' if is_ft else 'ベース'}モデル\")\n",
    "    \n",
    "    if is_ft:\n",
    "        print(f\"✅ LoRA適用済みモデルを使用: {type(current_model)}\")\n",
    "        # LoRAモデルの詳細情報を確認\n",
    "        if hasattr(current_model, 'peft_config'):\n",
    "            print(f\"   - LoRA設定: {current_model.peft_config}\")\n",
    "        if hasattr(current_model, 'base_model'):\n",
    "            print(f\"   - ベースモデル型: {type(current_model.base_model)}\")\n",
    "    else:\n",
    "        print(f\"✅ ベースモデルを使用: {type(current_model)}\")\n",
    "    \n",
    "    print(f\"最終的に使用するモデル: {type(current_model)}\")\n",
    "    print(f\"モデルのデバイス: {next(current_model.parameters()).device if hasattr(current_model, 'parameters') else 'Unknown'}\")\n",
    "    print(\"=== デバッグ情報終了 ===\\n\")\n",
    "    \n",
    "    # === FT モード：RAGを通さず q のみ ===\n",
    "    if cfg.gen_mode == \"FT\":\n",
    "        prompt = build_prompt_ft(cfg, question, llm_tok)\n",
    "        print(f\"\\n=== FTモード プロンプト ===\")\n",
    "        print(f\"プロンプト: {prompt}\")\n",
    "        print(f\"プロンプト長: {len(prompt)}文字\")\n",
    "        print(\"=== プロンプト終了 ===\\n\")\n",
    "        \n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "        print(\"\\n=== 質問 ===\")\n",
    "        print(question)\n",
    "        print(\"\\n=== 回答 ===\")\n",
    "        print(answer)\n",
    "        return answer, None\n",
    "\n",
    "    # === RAG モード ===\n",
    "    print(f\"RAGモードで処理を開始します...\")\n",
    "    # 1) RAGデータの更新確認（共通のFAISSインデックスを使用）\n",
    "    if not update_rag_data(cfg):\n",
    "        return \"エラー: RAGデータの更新に失敗しました\", None\n",
    "    \n",
    "    # グローバル変数から取得\n",
    "    global faiss_index, chunk_data\n",
    "    \n",
    "    # 2) 検索（5件）\n",
    "    if cfg.search_mode == \"vector\":\n",
    "        if faiss_index is None:\n",
    "            return \"エラー: ベクトル検索モードですがFAISSインデックスが構築されていません\", None\n",
    "        \n",
    "        # 事前構築されたインデックスを使用\n",
    "        first_stage = vector_search_top_k(cfg, question, chunk_data, emb_tok, emb_model, faiss_index, k=cfg.top_k_retrieve)\n",
    "    elif cfg.search_mode == \"keyword\":\n",
    "        first_stage = keyword_search_top_k(question, chunk_data, k=cfg.top_k_retrieve)\n",
    "    else:\n",
    "        raise ValueError('search_mode must be \"vector\" or \"keyword\"')\n",
    "\n",
    "    cand_ids = [i for i, _ in first_stage]\n",
    "    candidates = [chunk_data[i] for i in cand_ids]\n",
    "\n",
    "    # 3) リランク（5→3） or そのまま3\n",
    "    if cfg.rerank_mode and len(candidates) > 0:\n",
    "        order = rerank(cfg, question, candidates, rerank_tok, rerank_model)\n",
    "        cand_ids = [cand_ids[i] for i in order]\n",
    "        candidates = [candidates[i] for i in order]\n",
    "\n",
    "    final_ids = cand_ids[:cfg.top_k_final]\n",
    "    retrieved = [chunk_data[i] for i in final_ids]\n",
    "\n",
    "    # 4) 生成\n",
    "    prompt = build_prompt_rag(cfg, question, retrieved, llm_tok)\n",
    "    answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "\n",
    "    # ===== ログ（全文表示）=====\n",
    "    print(f\"\\n=== 検索結果（{cfg.search_mode}, Top3表示／実際はTop{cfg.top_k_retrieve}取得, \"\n",
    "          f\"CHUNK_MODE={cfg.chunk_mode}, OVER_LAP={cfg.overlap}）===\\n\")\n",
    "    for rank, (i, s) in enumerate(first_stage[:cfg.top_k_final], 1):\n",
    "        print(f\"[検索Top{rank}] id={i}, score={s:.6f}\")\n",
    "        print(chunk_data[i])  # 全文表示\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    if cfg.rerank_mode:\n",
    "        print(\"\\n=== リランク後（Top3採用, 全文）===\\n\")\n",
    "        for rank, i in enumerate(final_ids, 1):\n",
    "            print(f\"[RerankTop{rank}] id={i}\")\n",
    "            print(chunk_data[i])\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n=== 質問 ===\")\n",
    "    print(question)\n",
    "    print(\"\\n=== 回答 ===\")\n",
    "    print(answer)\n",
    "    \n",
    "    # 検索結果情報を構築\n",
    "    references_info = []\n",
    "    for rank, i in enumerate(final_ids, 1):\n",
    "        chunk_text = chunk_data[i]\n",
    "        # 長すぎる場合は短縮\n",
    "        if len(chunk_text) > 200:\n",
    "            chunk_text = chunk_text[:200] + \"...\"\n",
    "        references_info.append(f\"**参考文献{rank}** (ID: {i})\\n{chunk_text}\")\n",
    "    \n",
    "    references_text = \"\\n\\n\".join(references_info)\n",
    "    \n",
    "    return answer, references_text\n",
    "\n",
    "\n",
    "# ================\n",
    "# Gradio UI\n",
    "# ================\n",
    "cfg = RAG_CONFIG()  # 基本設定（適宜書き換え）\n",
    "\n",
    "with gr.Blocks(title=\"Last Chatbot\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "    gr.Markdown(\"## Last Chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ===== 左ペイン（設定） =====\n",
    "        with gr.Column(scale=1, min_width=360):\n",
    "            with gr.Accordion(\"モデル & 実行設定\", open=True):\n",
    "                # チェックボックス群\n",
    "                cb_no_rag   = gr.Checkbox(label=\"RAGなし\", value=False, interactive=True)\n",
    "                cb_sentence = gr.Checkbox(label=\"文単位チャンク分け\", value=(cfg.chunk_mode==\"sentence\"))\n",
    "                cb_overlap  = gr.Checkbox(label=\"オーバーラップ\", value=cfg.overlap)\n",
    "                cb_rerank   = gr.Checkbox(label=\"リランキング\", value=cfg.rerank_mode)\n",
    "\n",
    "                # 検索モード（ベクトル / キーワード）\n",
    "                rd_search = gr.Radio(\n",
    "                    choices=[\"ベクトル\", \"キーワード\"],\n",
    "                    value=\"ベクトル\" if cfg.search_mode==\"vector\" else \"キーワード\",\n",
    "                    label=\"検索モード\",\n",
    "                )\n",
    "\n",
    "                # 数値入力（初期状態では表示）\n",
    "                num_chunk = gr.Number(label=\"チャンクサイズ（文字数）\", value=cfg.char_chunk_size, precision=0, interactive=True, visible=True)\n",
    "                num_overlap = gr.Number(label=\"オーバーラップサイズ（文字数）\", value=cfg.overlap_chars, precision=0, interactive=True, visible=cfg.overlap)\n",
    "\n",
    "                # モデル選択（ChangeLLM.pyと同じ方式）\n",
    "                model_radio = gr.Radio(\n",
    "                    choices=[\"original\", \"finetuning\"],\n",
    "                    value=\"original\",\n",
    "                    label=\"モデル選択\",\n",
    "                    info=\"original：デフォルトのsarashina2.2-3b-instruct-v0.1モデルを使用、finetuning：設定したファインチューニングモデルを使用\"\n",
    "                )\n",
    "\n",
    "                # プロンプト（編集可）\n",
    "                with gr.Accordion(\"プロンプト（編集可）\", open=True):\n",
    "                    tb_prompt = gr.Textbox(\n",
    "                        label=\"システムプロンプト\",\n",
    "                        value=DEFAULT_RAG_USER,  # 初期はデフォルトRAGプロンプト\n",
    "                        lines=8,\n",
    "                        max_lines=20,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    btn_reset_prompt = gr.Button(\"🔄 リセット\", size=\"sm\", variant=\"secondary\")\n",
    "\n",
    "                # RAGデータ入力\n",
    "                rag_data_accordion = gr.Accordion(\"RAGデータ設定\", open=True)\n",
    "                with rag_data_accordion:\n",
    "                    # ファイルパス入力\n",
    "                    tb_file_path = gr.Textbox(\n",
    "                        label=\"ファイルパス\",\n",
    "                        value=cfg.text_path,\n",
    "                        placeholder=\"例: ./data.txt\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "\n",
    "                # 表示制御関数\n",
    "                def _toggle_chunk_visibility(is_sentence: bool, is_overlap: bool):\n",
    "                    # チャンクサイズの表示制御\n",
    "                    chunk_vis = gr.update(visible=not is_sentence)\n",
    "                    # 文単位の場合はオーバーラップサイズを非表示\n",
    "                    if is_sentence:\n",
    "                        overlap_vis = gr.update(visible=False)\n",
    "                    else:\n",
    "                        overlap_vis = gr.update(visible=is_overlap)\n",
    "                    return chunk_vis, overlap_vis\n",
    "\n",
    "                def _toggle_overlap(is_overlap: bool, is_sentence: bool):\n",
    "                    # 文単位チャンク分けの場合はオーバーラップサイズを非表示\n",
    "                    if is_sentence:\n",
    "                        return gr.update(visible=False)\n",
    "                    return gr.update(visible=is_overlap)\n",
    "\n",
    "                def _on_no_rag_change(is_no_rag: bool):\n",
    "                    # RAGなし時はチャンク関連の設定を非表示・非アクティブ化\n",
    "                    if is_no_rag:\n",
    "                        # チャンクサイズ関連を非表示\n",
    "                        chunk_vis = gr.update(visible=False)\n",
    "                        overlap_vis = gr.update(visible=False)\n",
    "                        # 検索関連の設定を非表示・非アクティブ化\n",
    "                        sentence_interactive = gr.update(visible=False, interactive=False)\n",
    "                        overlap_interactive = gr.update(visible=False, interactive=False)\n",
    "                        rerank_interactive = gr.update(visible=False, interactive=False)\n",
    "                        search_visible = gr.update(visible=False)  # 検索モードを非表示\n",
    "                        # プロンプトをFT用に変更\n",
    "                        prompt_content = DEFAULT_FT_PROMPT\n",
    "                        print(f\"RAGモードをFTモードに変更: プロンプトを{DEFAULT_FT_PROMPT}に更新\")\n",
    "                    else:\n",
    "                        # RAGモード時は通常表示・アクティブ\n",
    "                        chunk_vis = gr.update(visible=not (cfg.chunk_mode==\"sentence\"))\n",
    "                        overlap_vis = gr.update(visible=cfg.overlap)\n",
    "                        sentence_interactive = gr.update(visible=True, interactive=True)\n",
    "                        overlap_interactive = gr.update(visible=True, interactive=True)\n",
    "                        rerank_interactive = gr.update(visible=True, interactive=True)\n",
    "                        search_visible = gr.update(visible=True)  # 検索モードを表示\n",
    "                        # プロンプトをRAG用に変更\n",
    "                        prompt_content = DEFAULT_RAG_USER\n",
    "                        print(f\"RAGモードをRAGモードに変更: プロンプトを{DEFAULT_RAG_USER}に更新\")\n",
    "                    \n",
    "                    return (chunk_vis, overlap_vis, \n",
    "                            sentence_interactive, overlap_interactive, rerank_interactive, \n",
    "                            search_visible, prompt_content)\n",
    "\n",
    "\n",
    "\n",
    "                cb_sentence.change(_toggle_chunk_visibility, inputs=[cb_sentence, cb_overlap], outputs=[num_chunk, num_overlap])\n",
    "                cb_overlap.change(_toggle_overlap, inputs=[cb_overlap, cb_sentence], outputs=num_overlap)\n",
    "                # RAGモード変更時のイベントハンドラー\n",
    "                def on_rag_mode_change(rag_mode):\n",
    "                    if rag_mode:\n",
    "                        cfg.gen_mode = \"FT\"\n",
    "                    else:\n",
    "                        cfg.gen_mode = \"RAG\"\n",
    "                    return rag_mode\n",
    "                \n",
    "                # プロンプトリセット機能\n",
    "                def reset_prompt_to_default(is_no_rag: bool):\n",
    "                    \"\"\"現在のモードに応じてデフォルトプロンプトにリセット\"\"\"\n",
    "                    if is_no_rag:\n",
    "                        # FTモードの場合\n",
    "                        return DEFAULT_FT_PROMPT\n",
    "                    else:\n",
    "                        # RAGモードの場合\n",
    "                        return DEFAULT_RAG_USER\n",
    "                \n",
    "                cb_no_rag.change(_on_no_rag_change, inputs=cb_no_rag, outputs=[\n",
    "                    num_chunk, num_overlap, \n",
    "                    cb_sentence, cb_overlap, cb_rerank, rd_search, tb_prompt\n",
    "                ])\n",
    "                \n",
    "                # リセットボタンのクリックイベント\n",
    "                btn_reset_prompt.click(\n",
    "                    fn=reset_prompt_to_default,\n",
    "                    inputs=[cb_no_rag],\n",
    "                    outputs=[tb_prompt]\n",
    "                )\n",
    "                \n",
    "                # モデル選択の変更を監視\n",
    "                def on_model_change(model_mode):\n",
    "                    global model_type\n",
    "                    print(f\"\\n=== モデル選択変更 ===\")\n",
    "                    print(f\"変更前のmodel_type: {model_type}\")\n",
    "                    print(f\"変更後のmodel_type: {model_mode}\")\n",
    "                    model_type = model_mode\n",
    "                    print(f\"✅ model_typeを更新しました: {model_type}\")\n",
    "                    print(\"=== モデル選択変更終了 ===\\n\")\n",
    "                \n",
    "                model_radio.change(\n",
    "                    fn=on_model_change,\n",
    "                    inputs=[model_radio],\n",
    "                    outputs=[]\n",
    "                )\n",
    "                \n",
    "\n",
    "\n",
    "        # ===== 右ペイン（チャット） =====\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n",
    "            tb_question = gr.Textbox(label=\"質問\", placeholder=\"ここに質問を入力\")\n",
    "            btn_send = gr.Button(\"送信\", variant=\"primary\")\n",
    "\n",
    "            # 実行関数\n",
    "            def _run_chat(\n",
    "                q, history,\n",
    "                is_no_rag, is_sentence, is_overlap, is_rerank,\n",
    "                search_choice,\n",
    "                chunk_size, overlap_size,\n",
    "                model_mode, user_prompt, file_path\n",
    "            ):\n",
    "                if not q or not str(q).strip():\n",
    "                    return (history or []) + [[q, \"エラー: 質問を入力してください。\"]], \"\"  # ← 送信後クリア\n",
    "\n",
    "                # cfg を組み立て\n",
    "                run_cfg = RAG_CONFIG()\n",
    "                # RAGモードの判定（ChangeLLM.pyと同じ方式）\n",
    "                run_cfg.gen_mode = \"FT\" if is_no_rag else \"RAG\"\n",
    "                run_cfg.chunk_mode = \"sentence\" if is_sentence else \"char\"\n",
    "                run_cfg.overlap = bool(is_overlap)\n",
    "                # 検索モードの反映\n",
    "                run_cfg.search_mode = \"vector\" if (\"ベクトル\" in search_choice) else \"keyword\"\n",
    "                \n",
    "                print(f\"RAGモード設定: {'FT' if is_no_rag else 'RAG'}\")\n",
    "                print(f\"チャンクモード: {run_cfg.chunk_mode}\")\n",
    "                print(f\"検索モード: {run_cfg.search_mode}\")\n",
    "\n",
    "                # 数値バリデーション\n",
    "                if run_cfg.chunk_mode == \"char\":\n",
    "                    try:\n",
    "                        run_cfg.char_chunk_size = int(chunk_size)\n",
    "                        if run_cfg.char_chunk_size <= 0:\n",
    "                            return (history or []) + [[q, \"エラー: チャンクサイズは1以上で指定してください。\"]], \"\"\n",
    "                    except Exception:\n",
    "                        return (history or []) + [[q, \"エラー: チャンクサイズの数値が不正です。\"]], \"\"\n",
    "                try:\n",
    "                    run_cfg.overlap_chars = int(overlap_size) if is_overlap else run_cfg.overlap_chars\n",
    "                    if is_overlap and run_cfg.chunk_mode == \"char\" and run_cfg.overlap_chars >= run_cfg.char_chunk_size:\n",
    "                        return (history or []) + [[q, \"エラー: オーバーラップサイズはチャンクサイズ未満にしてください。\"]], \"\"\n",
    "                except Exception:\n",
    "                    return (history or []) + [[q, \"エラー: オーバーラップサイズの数値が不正です。\"]], \"\"\n",
    "\n",
    "                run_cfg.rerank_mode = bool(is_rerank)\n",
    "                \n",
    "                # グローバル変数のモデルタイプを更新\n",
    "                global model_type\n",
    "                print(f\"\\n=== 実行時のデバッグ情報 ===\")\n",
    "                print(f\"変更前のmodel_type: {model_type}\")\n",
    "                print(f\"UIから受け取ったmodel_mode: {model_mode}\")\n",
    "                model_type = model_mode\n",
    "                print(f\"✅ model_typeを更新しました: {model_type}\")\n",
    "                print(f\"llm_with_lora is None: {llm_with_lora is None}\")\n",
    "                print(f\"run_cfg.gen_mode: {run_cfg.gen_mode}\")\n",
    "                \n",
    "                # モデル選択の状態を確認\n",
    "                is_ft = (model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "                print(f\"使用予定モデル: {'LoRA適用済み' if is_ft else 'ベース'}モデル\")\n",
    "                print(\"=== 実行時デバッグ情報終了 ===\\n\")\n",
    "                \n",
    "                # プロンプト適用：現在のモードに応じて片方に入れる（ChangeLLM.pyと同じ方式）\n",
    "                if run_cfg.gen_mode == \"FT\":\n",
    "                    run_cfg.ft_system_prompt = user_prompt or \"\"\n",
    "                    run_cfg.rag_user_prompt_template = \"\"  # RAG側はデフォルトに戻す\n",
    "                    print(f\"FTモード用プロンプトを適用: {run_cfg.ft_system_prompt}\")\n",
    "                else:\n",
    "                    run_cfg.rag_user_prompt_template = user_prompt or \"\"\n",
    "                    run_cfg.ft_system_prompt = \"\"   # FT側はデフォルトに戻す\n",
    "                    print(f\"RAGモード用プロンプトを適用: {run_cfg.rag_user_prompt_template}\")\n",
    "\n",
    "                # RAGなし時の必須チェック（ChangeLLM.pyと同じ方式）\n",
    "                if run_cfg.gen_mode == \"FT\" and model_type == \"finetuning\" and llm_with_lora is None:\n",
    "                    print(f\"⚠️ エラー: RAGなし時はLoRAアダプターが適用されている必要があります\")\n",
    "                    print(f\"   - gen_mode: {run_cfg.gen_mode}\")\n",
    "                    print(f\"   - model_type: {model_type}\")\n",
    "                    print(f\"   - llm_with_lora is None: {llm_with_lora is None}\")\n",
    "                    return (history or []) + [[q, \"エラー: RAGなし時はLoRAアダプターが適用されている必要があります。\"]], \"\"\n",
    "                \n",
    "                # RAGモード時のファイルパス更新\n",
    "                if run_cfg.gen_mode == \"RAG\" and file_path and file_path.strip():\n",
    "                    run_cfg.text_path = file_path.strip()\n",
    "                    print(f\"RAGデータファイルパスを更新: {run_cfg.text_path}\")\n",
    "\n",
    "                # 実行\n",
    "                print(f\"\\n=== 実行開始 ===\")\n",
    "                print(f\"最終的な設定:\")\n",
    "                print(f\"  - gen_mode: {run_cfg.gen_mode}\")\n",
    "                print(f\"  - model_type: {model_type}\")\n",
    "                print(f\"  - chunk_mode: {run_cfg.chunk_mode}\")\n",
    "                print(f\"  - search_mode: {run_cfg.search_mode}\")\n",
    "                print(\"=== 実行開始終了 ===\\n\")\n",
    "                \n",
    "                try:\n",
    "                    result = run_pipeline(run_cfg, q)\n",
    "                    if isinstance(result, tuple):\n",
    "                        ans, references = result\n",
    "                    else:\n",
    "                        ans, references = result, None\n",
    "                except Exception as e:\n",
    "                    ans = f\"実行時エラー: {e}\"\n",
    "                    references = None\n",
    "\n",
    "                # RAGモードの時は参考文献情報も含める（ChangeLLM.pyと同じ方式）\n",
    "                if run_cfg.gen_mode == \"RAG\" and references:\n",
    "                    full_answer = f\"{ans}\\n\\n---\\n**参考文献**\\n{references}\"\n",
    "                    print(f\"RAGモード: 参考文献情報を含めて回答を表示\")\n",
    "                else:\n",
    "                    full_answer = ans\n",
    "                    print(f\"FTモード: 参考文献なしで回答を表示\")\n",
    "\n",
    "                # ← 送信後は質問欄をクリアするため \"\" を返す\n",
    "                return (history or []) + [[q, full_answer]], \"\"\n",
    "\n",
    "            # 出力を chatbot と tb_question（空文字でクリア）に\n",
    "            btn_send.click(\n",
    "                _run_chat,\n",
    "                inputs=[\n",
    "                    tb_question, chatbot,\n",
    "                    cb_no_rag, cb_sentence, cb_overlap, cb_rerank,\n",
    "                    rd_search,\n",
    "                    num_chunk, num_overlap,\n",
    "                    model_radio, tb_prompt, tb_file_path\n",
    "                ],\n",
    "                outputs=[chatbot, tb_question]\n",
    "            )\n",
    "\n",
    "            # Enterでも送信して同様にクリア\n",
    "            tb_question.submit(\n",
    "                _run_chat,\n",
    "                inputs=[\n",
    "                    tb_question, chatbot,\n",
    "                    cb_no_rag, cb_sentence, cb_overlap, cb_rerank,\n",
    "                    rd_search,\n",
    "                    num_chunk, num_overlap,\n",
    "                    model_radio, tb_prompt, tb_file_path\n",
    "                ],\n",
    "                outputs=[chatbot, tb_question]\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== チャットボット起動 ===\")\n",
    "    \n",
    "    # 1. 生成モデル（sarashina）を読み込み\n",
    "    print(\"1/3: 生成モデル（sarashina）を読み込み中\")\n",
    "    print(\"   - ベースモデルを読み込み中\")\n",
    "    llm_tok, llm = load_clean_base_model()\n",
    "    if llm_tok is None or llm is None:\n",
    "        print(\"エラー: ベースモデルの読み込みに失敗しました\")\n",
    "        exit(1)\n",
    "    print(\"✅ ベースモデルの読み込み完了\")\n",
    "    \n",
    "    # 2. 埋め込みモデル（bge-m3）を読み込み\n",
    "    print(\"2/3: 埋め込みモデルを読み込み中\")\n",
    "    try:\n",
    "        emb_tok = AutoTokenizer.from_pretrained(\"./bge-m3\")\n",
    "        emb_model = AutoModel.from_pretrained(\"./bge-m3\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        print(\"✅ 埋め込みモデルの読み込み完了\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: 埋め込みモデルの読み込みに失敗しました: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3. リランクモデルを読み込み\n",
    "    print(\"3/3: リランクモデルを読み込み中\")\n",
    "    try:\n",
    "        rerank_tok = AutoTokenizer.from_pretrained(\"./japanese-bge-reranker-v2-m3-v1\")\n",
    "        rerank_model = AutoModelForSequenceClassification.from_pretrained(\"./japanese-bge-reranker-v2-m3-v1\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        print(\"✅ リランクモデルの読み込み完了\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: リランクモデルの読み込みに失敗しました: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🎉 すべてのモデルの読み込みが完了しました！\")\n",
    "    \n",
    "    # 4. LoRAアダプターを事前に適用\n",
    "    print(\"4/4: LoRAアダプターを事前適用中...\")\n",
    "    print(f\"LoRAアダプターパス: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "    print(f\"パスの存在確認: {os.path.exists(DEFAULT_LORA_ADAPTER_PATH)}\")\n",
    "    \n",
    "    if os.path.exists(DEFAULT_LORA_ADAPTER_PATH):\n",
    "        try:\n",
    "            print(f\"LoRAアダプターを適用中...\")\n",
    "            llm_with_lora = apply_lora_adapter(llm, DEFAULT_LORA_ADAPTER_PATH)\n",
    "            if llm_with_lora is not None:\n",
    "                print(\"✅ LoRAアダプターの事前適用完了\")\n",
    "                print(f\"   - ベースモデル: {cfg.llm_model_name}\")\n",
    "                print(f\"   - LoRAアダプター: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "                print(f\"   - LoRAモデルの型: {type(llm_with_lora)}\")\n",
    "                \n",
    "                # LoRAモデルの詳細情報を確認\n",
    "                if hasattr(llm_with_lora, 'peft_config'):\n",
    "                    print(f\"   - LoRA設定: {llm_with_lora.peft_config}\")\n",
    "                if hasattr(llm_with_lora, 'base_model'):\n",
    "                    print(f\"   - ベースモデル型: {type(llm_with_lora.base_model)}\")\n",
    "                \n",
    "                # モデルのパラメータ数を確認\n",
    "                try:\n",
    "                    total_params = sum(p.numel() for p in llm_with_lora.parameters())\n",
    "                    trainable_params = sum(p.numel() for p in llm_with_lora.parameters() if p.requires_grad)\n",
    "                    print(f\"   - 総パラメータ数: {total_params:,}\")\n",
    "                    print(f\"   - 学習可能パラメータ数: {trainable_params:,}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   - パラメータ数確認エラー: {e}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠️ LoRAアダプターの適用に失敗、ベースモデルのみで続行します\")\n",
    "                llm_with_lora = None\n",
    "        except Exception as e:\n",
    "            print(f\"警告: LoRAアダプターの事前適用に失敗しました: {e}\")\n",
    "            print(\"オリジナルモデルのみで続行します\")\n",
    "            llm_with_lora = None\n",
    "    else:\n",
    "        print(\"LoRAアダプターパスが存在しないため、オリジナルモデルのみで続行します\")\n",
    "        llm_with_lora = None\n",
    "    \n",
    "    # 最終的なモデル状態を確認\n",
    "    print(f\"\\n=== 最終的なモデル状態 ===\")\n",
    "    print(f\"llm (ベースモデル): {type(llm) if llm is not None else 'None'}\")\n",
    "    print(f\"llm_with_lora (LoRAモデル): {type(llm_with_lora) if llm_with_lora is not None else 'None'}\")\n",
    "    print(f\"model_type: {model_type}\")\n",
    "    print(\"=== モデル状態確認終了 ===\\n\")\n",
    "    \n",
    "    print(\"🎉 すべてのモデルの読み込みが完了しました！\")\n",
    "    print(\"UIを起動しています...\")\n",
    "    \n",
    "    port = find_available_port(7890)\n",
    "    if port is None:\n",
    "        print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ポート {port} で起動します...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ポート {port} が使用中です。別のポートを試します...\")\n",
    "            # 別のポートで再試行\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ポート {port} で再起動します...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"起動中にエラーが発生しました: {e}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibaraki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
