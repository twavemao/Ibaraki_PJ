{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c318f9",
   "metadata": {},
   "source": [
    "### 第1セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb7fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGデータの設定\n",
    "DATA_FILE_PATH = \"大谷翔平_raw.txt\"  # この変数を変更してデータファイルを指定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582923e4",
   "metadata": {},
   "source": [
    "### 第2セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092034a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニング設定\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./checkpoint-160\"  # チェックポイントのパス（例: \"./chackpoint-100\"）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeae983",
   "metadata": {},
   "source": [
    "### 第3セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f5f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\ibaraki\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== チャットボット起動 ===\n",
      "1/3: 生成モデル（sarashina）を読み込み中\n",
      "   - オリジナルモデルを読み込み中\n",
      "オリジナルモデルを読み込み中: ./sbintuitions/sarashina2.2-3B-instruct-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd903a8ac724673b671191a0bfa0780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ オリジナルモデルの読み込み完了\n",
      "✅ オリジナルモデルの読み込み完了\n",
      "2/3: 埋め込みモデルを読み込み中\n",
      "✅ 埋め込みモデルの読み込み完了\n",
      "3/3: LoRAアダプターを事前適用中...\n",
      "LoRAアダプターを適用中: ./output/checkpoint-160\n",
      "LoRA設定を読み込みました: sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "LoRAアダプターの適用完了\n",
      "✅ LoRAアダプターの事前適用完了\n",
      "   - ベースモデル: ./sbintuitions/sarashina2.2-3B-instruct-v0.1\n",
      "   - LoRAアダプター: ./output/checkpoint-160\n",
      "🎉 すべてのモデルの読み込みが完了しました！\n",
      "UIを起動しています...\n",
      "ポート 7880 で起動します...\n",
      "* Running on local URL:  http://0.0.0.0:7880\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論処理を開始しています...\n",
      "モード: ベースモデル + RAG\n",
      "処理時間: 16.30秒\n",
      "推論処理を開始しています...\n",
      "モード: ベースモデルのみ\n",
      "処理時間: 1.85秒\n",
      "推論処理を開始しています...\n",
      "モード: FTモデルのみ\n",
      "処理時間: 1.97秒\n",
      "推論処理を開始しています...\n",
      "モード: ベースモデルのみ\n",
      "処理時間: 7.95秒\n",
      "推論処理を開始しています...\n",
      "モード: FTモデルのみ\n",
      "処理時間: 8.50秒\n",
      "推論処理を開始しています...\n",
      "モード: FTモデル + RAG\n",
      "処理時間: 7.64秒\n",
      "推論処理を開始しています...\n",
      "モード: ベースモデル + RAG\n",
      "処理時間: 12.45秒\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import faiss\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time  # 時間計測用\n",
    "import pykakasi\n",
    "import socket\n",
    "\n",
    "# OpenMPの問題を解決するための環境変数設定\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# ==============\n",
    "# グローバル設定（外部で変更可能）\n",
    "# ==============\n",
    "# 使用するデータファイルのパスをここで設定\n",
    "DATA_FILE_PATH = \"大谷翔平_raw.txt\"  # この変数を変更してデータファイルを指定\n",
    "\n",
    "# デフォルトモデルパスの設定\n",
    "DEFAULT_LLM_MODEL_NAME = \"./sbintuitions/sarashina2.2-3B-instruct-v0.1\"\n",
    "DEFAULT_EMB_MODEL_NAME = \"./bge-m3\"\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./output/checkpoint-160\"  # LoRAアダプターのフォルダ名を指定（例: \"./output/checkpoint-160\" または \"./my-lora-adapter\"）\n",
    "\n",
    "class ChangeLLMChatbot:\n",
    "    def __init__(self):\n",
    "        # データファイルパス（外部で設定可能）\n",
    "        self.text_path = DATA_FILE_PATH\n",
    "        \n",
    "        # デフォルトモデルパスの設定\n",
    "        self.llm_model_name = DEFAULT_LLM_MODEL_NAME\n",
    "        self.emb_model_name = DEFAULT_EMB_MODEL_NAME\n",
    "        self.lora_adapter_path = DEFAULT_LORA_ADAPTER_PATH\n",
    "        \n",
    "        # モデルタイプ : original or finetuning\n",
    "        self.model_type = \"original\"\n",
    "        \n",
    "        # 生成設定\n",
    "        self.temperature = 0.7\n",
    "        self.answer_token_limit = 200\n",
    "        self.answer_headroom = 50\n",
    "        self.top_k_retrieve = 3\n",
    "        self.gen_mode = \"RAG\"  # \"RAG\" or \"FT\"\n",
    "        \n",
    "        # メモリ最適化設定\n",
    "        self.chunk_size = 100\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # RAG設定\n",
    "        self.rag_user_prompt_template = \"\"\n",
    "        self.use_chat_template = True\n",
    "        \n",
    "        # ファインチューニング設定\n",
    "        self.ft_system_prompt = \"\"\n",
    "        \n",
    "        # 実行環境\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 設定インスタンスを作成\n",
    "cfg = ChangeLLMChatbot()\n",
    "\n",
    "\n",
    "\n",
    "# グローバル変数でモデルを保持\n",
    "llm_tok = None\n",
    "llm = None  # ベースモデル（常に保持）\n",
    "llm_with_lora = None  # LoRA適用済みモデル（一度だけ作成）\n",
    "emb_tok = None\n",
    "emb_model = None\n",
    "\n",
    "# FAISSインデックスフォルダの設定\n",
    "FAISS_INDEX_DIR = \"faiss_index\"\n",
    "\n",
    "# FAISSインデックスフォルダが存在しない場合は作成\n",
    "if not os.path.exists(FAISS_INDEX_DIR):\n",
    "    os.makedirs(FAISS_INDEX_DIR)\n",
    "\n",
    "# FAISSインデックスとチャンクデータを保持\n",
    "faiss_index = None\n",
    "chunk_data = None\n",
    "last_data_file = None  # 最後に読み込んだデータファイルを記録\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"利用可能なポートを見つける\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# ==============\n",
    "# RAGデータ管理\n",
    "# ==============\n",
    "def convert_to_romaji(filename):\n",
    "    \"\"\"日本語ファイル名をローマ字に変換\"\"\"\n",
    "    try:\n",
    "        # pykakasiの新しいAPIを使用\n",
    "        kakasi = pykakasi.kakasi()\n",
    "        \n",
    "        # 拡張子を除去\n",
    "        name_without_ext = Path(filename).stem\n",
    "        \n",
    "        # ローマ字変換\n",
    "        romaji_result = kakasi.convert(name_without_ext)\n",
    "        \n",
    "        # pykakasiの結果がリストの場合は、hepburn形式の文字列を結合\n",
    "        if isinstance(romaji_result, list):\n",
    "            romaji_name = \"\"\n",
    "            for item in romaji_result:\n",
    "                if isinstance(item, dict) and 'hepburn' in item:\n",
    "                    romaji_name += item['hepburn']\n",
    "                elif isinstance(item, str):\n",
    "                    romaji_name += item\n",
    "        else:\n",
    "            romaji_name = str(romaji_result)\n",
    "        \n",
    "        # 英数字以外の文字を除去し、スペースをアンダースコアに変換\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', romaji_name)\n",
    "        safe_name = re.sub(r'_+', '_', safe_name)  # 連続するアンダースコアを1つに\n",
    "        safe_name = safe_name.strip('_')  # 前後のアンダースコアを除去\n",
    "        \n",
    "        return safe_name if safe_name else \"data\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"data\"\n",
    "\n",
    "def get_faiss_index_path(data_filename):\n",
    "    \"\"\"FAISSインデックスファイルのパスを取得\"\"\"\n",
    "    # 日本語判定（ひらがな、カタカナ、漢字が含まれているか）\n",
    "    japanese_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "    \n",
    "    if japanese_pattern.search(data_filename):\n",
    "        # 日本語の場合、ローマ字変換\n",
    "        romaji_name = convert_to_romaji(data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{romaji_name}.faiss\")\n",
    "    else:\n",
    "        # 日本語でない場合、そのまま使用\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{safe_name}.faiss\")\n",
    "\n",
    "def update_rag_data():\n",
    "    \"\"\"RAGデータを更新（FAISSインデックスファイルによる条件分岐）\"\"\"\n",
    "    global faiss_index, chunk_data, last_data_file\n",
    "    \n",
    "    current_file = cfg.text_path\n",
    "    current_filename = Path(current_file).name  # ファイル名（拡張子含む）\n",
    "    \n",
    "    # FAISSインデックスファイルのパスを取得\n",
    "    faiss_index_path = get_faiss_index_path(current_filename)\n",
    "    \n",
    "    # 既存のインデックスファイルが存在する場合\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            # インデックスファイルから読み込み\n",
    "            faiss_index = faiss.read_index(faiss_index_path)\n",
    "            # チャンクデータも再構築\n",
    "            text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "            chunk_data = create_chunks(text)\n",
    "            last_data_file = current_file\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # 新しいデータを読み込み、インデックスを構築\n",
    "    if current_file != last_data_file or faiss_index is None:\n",
    "        try:\n",
    "            text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "            chunk_data = create_chunks(text)\n",
    "            \n",
    "            # 新しいインデックスを構築\n",
    "            chunk_embs = embed_texts(chunk_data, emb_tok, emb_model)\n",
    "            faiss_index = build_faiss_index(chunk_embs)\n",
    "            \n",
    "            # インデックスファイルを保存\n",
    "            faiss.write_index(faiss_index, faiss_index_path)\n",
    "            \n",
    "            last_data_file = current_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==============\n",
    "# 文字正規化\n",
    "# ==============\n",
    "def _normalize(text: str) -> str:\n",
    "    # 改行削除＋NFKC＋lower\n",
    "    return unicodedata.normalize(\"NFKC\", text.replace(\"\\n\", \"\")).lower()\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int = None, overlap: bool = False, overlap_size: int = 20) -> List[str]:\n",
    "    #  改行削除＋NFKC＋lower\n",
    "    text = _normalize(text)\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # 設定値を使用\n",
    "    if chunk_size is None:\n",
    "        chunk_size = cfg.chunk_size\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    step = max(1, chunk_size - overlap_size) if overlap else chunk_size\n",
    "    for i in range(0, len(text), step):\n",
    "        piece = text[i:i + chunk_size]\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "    return chunks\n",
    "    \n",
    "def load_generation_model():\n",
    "    \"\"\"生成モデルを読み込み（4bit量子化対応）\"\"\"\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        # 常にベースモデル（sarashina）を読み込み\n",
    "        model_name = cfg.llm_model_name\n",
    "        print(f\"モデルパス: {model_name}\")\n",
    "        \n",
    "        # 4bit量子化設定（メモリ最適化）\n",
    "        print(\"4bit量子化設定を作成中...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,  # 二重量子化を有効化（AI_chatbot.pyと同じ）\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # トークナイザーを読み込み\n",
    "        print(\"トークナイザーを読み込み中...\")\n",
    "        global llm_tok\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"トークナイザーの読み込み完了\")\n",
    "        \n",
    "        # モデルを4bit量子化で読み込み（LoRAアダプターを無視）\n",
    "        print(\"4bit量子化モデルを読み込み中...\")\n",
    "        global llm\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,  # AI_chatbot.pyと同じ\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            # LoRAアダプターを無視してベースモデルのみを読み込み\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # pad_tokenが設定されていない場合の処理\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # vocab と lm_head の出力次元を一致＋可能ならtie\n",
    "        vocab_size = llm_tok.vocab_size\n",
    "        head = getattr(llm, \"lm_head\", None)\n",
    "        head_out = getattr(head, \"out_features\", None) if head is not None else None\n",
    "        if head_out is not None and head_out != vocab_size:\n",
    "            print(f\"[WARN] lm_head({head_out}) != vocab({vocab_size}) → resize\")\n",
    "            llm.resize_token_embeddings(vocab_size)\n",
    "        try:\n",
    "            llm.tie_weights()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if hasattr(llm, \"lm_head\") and hasattr(llm, \"model\") and hasattr(llm.model, \"embed_tokens\"):\n",
    "                    llm.lm_head.weight = llm.model.embed_tokens.weight\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        print(\"ベースモデル（sarashina）の読み込み完了\")\n",
    "        \n",
    "        # 推論モードに設定\n",
    "        llm.eval()\n",
    "        \n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"モデルの読み込みエラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def apply_lora_adapter(base_model, adapter_path):\n",
    "    \"\"\"LoRAアダプターを適用（一度だけ実行）\"\"\"\n",
    "    try:\n",
    "        # パスの存在確認\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"LoRAアダプターパスが存在しません: {adapter_path}\")\n",
    "            return None\n",
    "        \n",
    "        from peft import PeftModel, PeftConfig\n",
    "        import warnings\n",
    "        \n",
    "        print(f\"LoRAアダプターを適用中: {adapter_path}\")\n",
    "        \n",
    "        # LoRA設定の読み込み\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "        print(f\"LoRA設定を読み込みました: {peft_config.base_model_name_or_path}\")\n",
    "        \n",
    "        # 警告を一時的に抑制\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"peft\")\n",
    "            \n",
    "            # アダプターの適用\n",
    "            lora_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "                is_trainable=False,  # 推論時はFalse\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(\"LoRAアダプターの適用完了\")\n",
    "        return lora_model\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"PEFTライブラリがインストールされていません: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"LoRAアダプターの適用に失敗: {e}\")\n",
    "        return None\n",
    "        \n",
    "# =========\n",
    "# プロンプト（外出し対応）\n",
    "# =========\n",
    "DEFAULT_RAG_USER = (\n",
    "    \"与えられた情報だけで日本語で簡潔かつ正確に回答してください。\\n\"\n",
    "    \"与えられた情報にないことは推測で書かず、わからない場合はわからないと言ってください。\\n\\n\"\n",
    "    \"[質問]\\n{question}\\n\\n\"\n",
    "    \"[情報]\\n{context}\\n\\n\"\n",
    "    \"[回答]\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_FT_PROMPT = \"日本語で簡潔かつ正確に回答してください。根拠が無い推測は避けてください。\"\n",
    "\n",
    "def _safe_format(template: str, **kwargs) -> str:\n",
    "    class _D(dict):\n",
    "        def __missing__(self, k): return \"{\"+k+\"}\"\n",
    "    return template.format_map(_D(**kwargs))\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_rag(cfg, question: str, retrieved_chunks: List[str], tok: AutoTokenizer) -> str:\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    user_tmpl = cfg.rag_user_prompt_template.strip() if cfg.rag_user_prompt_template else DEFAULT_RAG_USER\n",
    "    user_content = _safe_format(user_tmpl, question=question, context=context)\n",
    "\n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return user_content\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_ft(cfg, question: str, tok: AutoTokenizer) -> str:\n",
    "    system_tmpl = cfg.ft_system_prompt.strip() if cfg.ft_system_prompt else DEFAULT_FT_PROMPT\n",
    "    \n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_tmpl},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return question\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"平均プーリングによるベクトル化\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "def embed_texts(texts: List[str], emb_tok, emb_model, batch_size: int = None) -> np.ndarray:\n",
    "    \"\"\"テキストをベクトル化\"\"\"\n",
    "    if emb_tok is None or emb_model is None:\n",
    "        return None\n",
    "    \n",
    "    # 設定値を使用\n",
    "    if batch_size is None:\n",
    "        batch_size = cfg.batch_size\n",
    "    \n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = emb_tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = emb_model(**inputs)\n",
    "            emb = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "            vecs.append(emb.cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "    \n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "\n",
    "def vector_search_top_k(query: str,\n",
    "                        chunks: List[str],\n",
    "                        emb_tok: AutoTokenizer,\n",
    "                        emb_model: AutoModel,\n",
    "                        index,\n",
    "                        k: int) -> List[Tuple[int, float]]:\n",
    "    import faiss  # noqa\n",
    "    with torch.no_grad():\n",
    "        q_inputs = emb_tok([query], padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        q_out = emb_model(**q_inputs)\n",
    "        q_vec = mean_pooling(q_out, q_inputs[\"attention_mask\"])\n",
    "        q_vec = torch.nn.functional.normalize(q_vec, p=2, dim=1).cpu().numpy()\n",
    "    sims, ids = index.search(q_vec, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], sims[0]) if 0 <= i < len(chunks)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _snap_to_sentence_boundary(text: str) -> str:\n",
    "    m = re.search(r'(.+?[。．.!！?？])(?:[^。．.!！?？]*)$', text)\n",
    "    return m.group(1) if m else text\n",
    "\n",
    "def truncate_to_token_limit(tok: AutoTokenizer, text: str, limit: int) -> str:\n",
    "    ids = tok(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= limit:\n",
    "        return _snap_to_sentence_boundary(text)\n",
    "    clipped = tok.decode(ids[:limit], skip_special_tokens=True)\n",
    "    return _snap_to_sentence_boundary(clipped)\n",
    "\n",
    "# =====\n",
    "# 生成\n",
    "# =====\n",
    "@torch.no_grad()\n",
    "def generate_with_limit(cfg,\n",
    "                        prompt: str,\n",
    "                        llm_tok: AutoTokenizer,\n",
    "                        llm: AutoModelForCausalLM) -> str:\n",
    "    inputs = llm_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(cfg.device)\n",
    "    out = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=cfg.answer_token_limit + cfg.answer_headroom,\n",
    "        temperature=cfg.temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=llm_tok.pad_token_id,\n",
    "        eos_token_id=llm_tok.eos_token_id or llm_tok.pad_token_id,\n",
    "    )\n",
    "    raw = llm_tok.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    # プロンプトを除去して回答部分のみを取得\n",
    "    if \"[回答]\" in raw:\n",
    "        # RAGモードの場合\n",
    "        text = raw.split(\"[回答]\", 1)[-1].strip()\n",
    "    else:\n",
    "        # RAGなしモードの場合：プロンプト部分を除去\n",
    "        prompt_tokens = llm_tok(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "        generated_tokens = out[0][len(prompt_tokens):]\n",
    "        text = llm_tok.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return truncate_to_token_limit(llm_tok, text, cfg.answer_token_limit)\n",
    "\n",
    "# モデルの読み込み状態を確認する関数\n",
    "def get_model_status():\n",
    "    \"\"\"現在のモデル読み込み状況を取得\"\"\"\n",
    "    status = {\n",
    "        \"llm_loaded\": llm_tok is not None and llm is not None,\n",
    "        \"emb_loaded\": emb_tok is not None and emb_model is not None,\n",
    "        \"lora_loaded\": llm_with_lora is not None\n",
    "    }\n",
    "    return status\n",
    "\n",
    "def get_model_status_text():\n",
    "    \"\"\"モデル読み込み状況をテキストで取得\"\"\"\n",
    "    status = get_model_status()\n",
    "    \n",
    "    status_text = \"📊 **モデル読み込み状況**:\\n\"\n",
    "    status_text += f\"• 生成モデル: {'✅ 読み込み済み' if status['llm_loaded'] else '❌ 未読み込み'}\\n\"\n",
    "    status_text += f\"• 埋め込みモデル: {'✅ 読み込み済み' if status['emb_loaded'] else '❌ 未読み込み'}\\n\"\n",
    "    status_text += f\"• LoRAアダプター: {'✅ 読み込み済み' if status['lora_loaded'] else '❌ 未読み込み'}\\n\"\n",
    "    \n",
    "    return status_text\n",
    "\n",
    "# 実行パス\n",
    "def run_pipeline(question, rag_mode=\"\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # GPUメモリをクリア\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"推論処理を開始しています...\")\n",
    "    \n",
    "    # グローバル変数から取得（事前読み込み済み）\n",
    "    global llm_tok, llm, emb_tok, emb_model, llm_with_lora\n",
    "    \n",
    "    # モデルの読み込み状態を確認\n",
    "    if llm_tok is None or llm is None:\n",
    "        return \"エラー: 生成モデルが読み込まれていません\", 0.0\n",
    "\n",
    "    # === モデル選択とモード判定 ===\n",
    "    is_ft = (cfg.model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "    current_model = llm_with_lora if is_ft else llm\n",
    "    use_rag = (rag_mode == \"RAGあり\")\n",
    "    \n",
    "    print(f\"モード: {'FT' if is_ft else 'ベース'}モデル{' + RAG' if use_rag else 'のみ'}\")\n",
    "\n",
    "    # === RAG処理（必要な場合のみ） ===\n",
    "    retrieved = None\n",
    "    if use_rag:\n",
    "        if emb_tok is None or emb_model is None:\n",
    "            return \"エラー: RAGモードには埋め込みモデルが必要です\", time.time() - start_time\n",
    "        if not update_rag_data():\n",
    "            return \"エラー: RAGデータの更新に失敗しました\", time.time() - start_time\n",
    "\n",
    "        first_stage = vector_search_top_k(question, chunk_data, emb_tok, emb_model, faiss_index, k=cfg.top_k_retrieve)\n",
    "        cand_ids = [i for i, _ in first_stage]\n",
    "        retrieved = [chunk_data[i] for i in cand_ids]\n",
    "\n",
    "    # === プロンプト構築と推論 ===\n",
    "    if use_rag:\n",
    "        # RAGあり（ベース・FT共通でRAGプロンプトを使用）\n",
    "        prompt = build_prompt_rag(cfg, question, retrieved, llm_tok)\n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "        answer += f\"\\n\\n📚 **参考**:\\n{retrieved[0]}\"\n",
    "    else:\n",
    "        # RAGなし（ベース・FT共通でFTプロンプトを使用）\n",
    "        prompt = build_prompt_ft(cfg, question, llm_tok)\n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "\n",
    "    print(f\"処理時間: {time.time() - start_time:.2f}秒\")\n",
    "    return answer, time.time() - start_time\n",
    "\n",
    "def load_clean_base_model():\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        model_name = cfg.llm_model_name\n",
    "        print(f\"オリジナルモデルを読み込み中: {model_name}\")\n",
    "        \n",
    "        # トークナイザーを読み込み\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # 4bit量子化設定（メモリ最適化）\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # モデルを読み込み（シンプルに）\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # 基本的な設定\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # 推論モードに設定\n",
    "        llm.eval()\n",
    "        \n",
    "        print(\"✅ オリジナルモデルの読み込み完了\")\n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"オリジナルモデルの読み込みエラー: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Gradioインターフェースを作成\n",
    "with gr.Blocks(title=\"ChangeLLMチャットボット\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\"), css=\"\"\"\n",
    "    .large-text textarea {\n",
    "        font-size: 20px !important;\n",
    "    }\n",
    "\"\"\") as demo:\n",
    "    gr.Markdown(\"# ChangeLLMチャットボット 🤖\")\n",
    "    gr.Markdown(\"LLMを切り替えられるチャットボットです。\")\n",
    "    \n",
    "    with gr.Tab(\"チャット\"):\n",
    "        gr.Markdown(\"## 設定\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                rag_mode_radio = gr.Radio(\n",
    "                    choices=[\"RAGあり\", \"RAGなし\"],\n",
    "                    value=\"RAGあり\",\n",
    "                    label=\"RAGモード\",\n",
    "                    info=\"RAGあり：RAGモード、RAGなし：LLMのみ\"\n",
    "                )\n",
    "            \n",
    "            with gr.Column():\n",
    "                model_radio = gr.Radio(\n",
    "                    choices=[\"original\", \"finetuning\"],\n",
    "                    value=\"original\",\n",
    "                    label=\"モデル選択\",\n",
    "                    info=\"original：デフォルトのsarashina2.2-3B-instruct-v0.1モデルを使用、finetuning：LoRAアダプターを適用（DEFAULT_LORA_ADAPTER_PATHで指定）\"\n",
    "                )\n",
    "        \n",
    "        # 現在のデータファイル名を表示\n",
    "        gr.Markdown(f\"**📁 現在のデータファイル**: {DATA_FILE_PATH}\")\n",
    "        \n",
    "        gr.Markdown(\"## チャット\")\n",
    "        \n",
    "        # チャット履歴を表示\n",
    "        chatbot = gr.Chatbot(label=\"Chatbot\", type=\"messages\")\n",
    "        \n",
    "        # メッセージ入力\n",
    "        msg = gr.Textbox(label=\"質問\", placeholder=\"質問を入力してください\")\n",
    "        \n",
    "        # 送信ボタン\n",
    "        submit_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "        \n",
    "        def respond(message, history, rag_mode, model_mode):\n",
    "            if message.strip() == \"\":\n",
    "                return \"\", history\n",
    "            # モデルタイプを更新\n",
    "            cfg.model_type = model_mode\n",
    "            response, response_time = run_pipeline(message, rag_mode)\n",
    "            \n",
    "            # 応答時間を含む回答を作成\n",
    "            response_with_time = f\"{response}\\n\\n⏱️ 応答時間: {response_time:.2f}秒\"\n",
    "            \n",
    "            # type=\"messages\"の場合、historyは辞書のリスト形式\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": response_with_time})\n",
    "            \n",
    "            return \"\", history\n",
    "        \n",
    "        # ラジオボタン選択変更時のイベントハンドラー\n",
    "        def on_model_change(model_type):\n",
    "            if model_type == \"original\":\n",
    "                cfg.model_type = \"original\"\n",
    "            else:\n",
    "                cfg.model_type = \"finetuning\"\n",
    "        \n",
    "\n",
    "        \n",
    "        def on_rag_mode_change(rag_mode):\n",
    "            if rag_mode == \"RAGなし\":\n",
    "                cfg.gen_mode = \"FT\"\n",
    "            else:\n",
    "                cfg.gen_mode = \"RAG\"\n",
    "            return rag_mode\n",
    "        \n",
    "        \n",
    "        \n",
    "        # イベントハンドラー\n",
    "        submit_btn.click(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, rag_mode_radio, model_radio],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Enterキーでも送信\n",
    "        msg.submit(\n",
    "            fn=respond,\n",
    "            inputs=[msg, chatbot, rag_mode_radio, model_radio],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        # ラジオボタンの選択変更を監視\n",
    "        model_radio.change(\n",
    "            fn=on_model_change,\n",
    "            inputs=[model_radio],\n",
    "            outputs=[]\n",
    "        )\n",
    "        \n",
    "        rag_mode_radio.change(\n",
    "            fn=on_rag_mode_change,\n",
    "            inputs=[rag_mode_radio],\n",
    "            outputs=[rag_mode_radio]\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "# アプリケーションを起動\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== チャットボット起動 ===\")\n",
    "    \n",
    "    # 1. 生成モデル（sarashina）を読み込み\n",
    "    print(\"1/3: 生成モデル（sarashina）を読み込み中\")\n",
    "    print(\"   - オリジナルモデルを読み込み中\")\n",
    "    llm_tok, llm = load_clean_base_model()\n",
    "    if llm_tok is None or llm is None:\n",
    "        print(\"エラー: オリジナルモデルの読み込みに失敗しました\")\n",
    "        exit(1)\n",
    "    print(\"✅ オリジナルモデルの読み込み完了\")\n",
    "    \n",
    "    # 2. 埋め込みモデル（bge-m3）を読み込み\n",
    "    print(\"2/3: 埋め込みモデルを読み込み中\")\n",
    "    try:\n",
    "        emb_tok = AutoTokenizer.from_pretrained(cfg.emb_model_name)\n",
    "        emb_model = AutoModel.from_pretrained(cfg.emb_model_name).to(cfg.device).eval()\n",
    "        print(\"✅ 埋め込みモデルの読み込み完了\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: 埋め込みモデルの読み込みに失敗しました: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3. LoRAアダプターを事前に適用\n",
    "    print(\"3/3: LoRAアダプターを事前適用中...\")\n",
    "    if os.path.exists(DEFAULT_LORA_ADAPTER_PATH):\n",
    "        try:\n",
    "            llm_with_lora = apply_lora_adapter(llm, DEFAULT_LORA_ADAPTER_PATH)\n",
    "            if llm_with_lora is not None:\n",
    "                print(\"✅ LoRAアダプターの事前適用完了\")\n",
    "                print(f\"   - ベースモデル: {cfg.llm_model_name}\")\n",
    "                print(f\"   - LoRAアダプター: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "            else:\n",
    "                print(\"⚠️ LoRAアダプターの適用に失敗、ベースモデルのみで続行します\")\n",
    "                llm_with_lora = None\n",
    "        except Exception as e:\n",
    "            print(f\"警告: LoRAアダプターの事前適用に失敗しました: {e}\")\n",
    "            print(\"オリジナルモデルのみで続行します\")\n",
    "            llm_with_lora = None\n",
    "    else:\n",
    "        print(\"LoRAアダプターパスが存在しないため、オリジナルモデルのみで続行します\")\n",
    "        llm_with_lora = None\n",
    "    \n",
    "    print(\"🎉 すべてのモデルの読み込みが完了しました！\")\n",
    "    print(\"UIを起動しています...\")\n",
    "    \n",
    "    port = find_available_port(7880)\n",
    "    if port is None:\n",
    "        print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ポート {port} で起動します...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ポート {port} が使用中です。別のポートを試します...\")\n",
    "            # 別のポートで再試行\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ポート {port} で再起動します...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"起動中にエラーが発生しました: {e}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
