{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11507b0",
   "metadata": {},
   "source": [
    "### 第1セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c596c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_17220\\2855017450.py:69: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ポート 7873 で起動します...\n",
      "* Running on local URL:  http://0.0.0.0:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"利用可能なポートを見つける\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "class DictionaryChatbot:\n",
    "    def __init__(self):\n",
    "        # ルールを定義（キーワード：応答）\n",
    "        self.rules = {\n",
    "            \"こんにちは\": \"こんにちは！何かお手伝いできますか？\",\n",
    "            \"お疲れ様\": \"お疲れ様です！\",\n",
    "            \"ありがとう\": \"どういたしまして！\",\n",
    "            \"さようなら\": \"さようなら！またお話ししましょう！\"\n",
    "        }\n",
    "    \n",
    "    def find_response(self, user_input):\n",
    "        \"\"\"ユーザーの入力から適切な応答を見つける\"\"\"\n",
    "        if not user_input.strip():\n",
    "            return \"何か入力してください。\"\n",
    "        \n",
    "        user_input = user_input.strip()\n",
    "        \n",
    "        # 完全一致をチェック\n",
    "        if user_input in self.rules:\n",
    "            return self.rules[user_input]\n",
    "        \n",
    "        # デフォルト応答\n",
    "        return \"申し訳ございません。その質問にはお答えできません。別の質問をしてください。\"\n",
    "    \n",
    "    def add_rule(self, key, response):\n",
    "        \"\"\"新しいルールを追加\"\"\"\n",
    "        self.rules[key] = response\n",
    "        return f\"新しいルールを追加しました: {key} → {response}\"\n",
    "    \n",
    "    def show_rules(self):\n",
    "        \"\"\"現在のルールを表形式で表示\"\"\"\n",
    "        rules_data = []\n",
    "        for i, (key, response) in enumerate(self.rules.items()):\n",
    "            rules_data.append([i+1, key, response])\n",
    "        return rules_data\n",
    "\n",
    "\n",
    "def add_new_rule(key, response):\n",
    "    \"\"\"新しいルールを追加\"\"\"\n",
    "    if key and response:\n",
    "        result = chatbot.add_rule(key, response)\n",
    "        # 入力欄を空にするために空文字列を返す\n",
    "        return result, \"\", \"\", chatbot.show_rules()\n",
    "    return \"キーワードと応答の両方を入力してください。\", key, response, chatbot.show_rules()\n",
    "\n",
    "\n",
    "# チャットボットのインスタンスを作成\n",
    "chatbot = DictionaryChatbot()\n",
    "\n",
    "# Gradioインターフェースを作成\n",
    "with gr.Blocks(title=\"辞書型チャットボット\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "    gr.Markdown(\"# 辞書型チャットボット 🤖\")\n",
    "    \n",
    "    with gr.Tab(\"チャット\"):\n",
    "        chatbot_interface = gr.Chatbot(\n",
    "            label=\"Chatbot\",\n",
    "            height=400,\n",
    "            value=[]\n",
    "        )\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            label=\"質問\",\n",
    "            placeholder=\"質問を入力\",\n",
    "            lines=1,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        submit_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "        \n",
    "        def user_input(message, history):\n",
    "            \"\"\"ユーザー入力の処理\"\"\"\n",
    "            try:\n",
    "                # 入力の検証\n",
    "                if not message.strip():\n",
    "                    return \"\", history\n",
    "                \n",
    "                # 履歴を適切な形式に変換\n",
    "                formatted_history = []\n",
    "                if history and len(history) > 0:\n",
    "                    for user_msg, ai_msg in history:\n",
    "                        formatted_history.append([user_msg, ai_msg])\n",
    "                \n",
    "                # 応答を生成\n",
    "                response = chatbot.find_response(message)\n",
    "                \n",
    "                # 履歴に追加\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, response])\n",
    "                \n",
    "                return \"\", history\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"入力処理中にエラーが発生しました: {str(e)}\"\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, error_msg])\n",
    "                return \"\", history\n",
    "    \n",
    "    with gr.Tab(\"ルール管理\"):\n",
    "        gr.Markdown(\"## 現在のルール一覧\")\n",
    "        rules_display = gr.Dataframe(\n",
    "            headers=[\"ID\", \"キーワード\", \"応答\"],\n",
    "            datatype=[\"number\", \"str\", \"str\"],\n",
    "            col_count=(3, \"fixed\"),\n",
    "            value=chatbot.show_rules(),\n",
    "            interactive=False\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"## 新しいルールを追加\")\n",
    "        with gr.Row():\n",
    "            key_input = gr.Textbox(label=\"キーワード\", placeholder=\"例：お疲れ様\")\n",
    "            response_input = gr.Textbox(label=\"応答\", placeholder=\"例：お疲れ様です！\")\n",
    "        add_button = gr.Button(\"ルールを追加\", variant=\"primary\")\n",
    "        add_result = gr.Textbox(label=\"結果\", interactive=False)\n",
    "        \n",
    "        # イベントハンドラー\n",
    "        add_button.click(\n",
    "            fn=add_new_rule,\n",
    "            inputs=[key_input, response_input],\n",
    "            outputs=[add_result, key_input, response_input, rules_display]\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"使い方\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ## 使い方\n",
    "        \n",
    "        ### チャット機能\n",
    "        - キーワードと完全に一致する場合、対応する応答を返します\n",
    "        \n",
    "        ### ルール管理\n",
    "        - 新しいルールを追加できます\n",
    "        - 現在のルール一覧を確認できます\n",
    "        \n",
    "        ### 例\n",
    "        - 「こんにちは」→「こんにちは！何かお手伝いできますか？」\n",
    "        \"\"\")\n",
    "        \n",
    "    \n",
    "    submit_btn.click(\n",
    "        user_input,\n",
    "        inputs=[msg, chatbot_interface],\n",
    "        outputs=[msg, chatbot_interface]\n",
    "    )\n",
    "    \n",
    "    msg.submit(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "\n",
    "# アプリケーションを起動\n",
    "if __name__ == \"__main__\":\n",
    "    # 利用可能なポートを探す\n",
    "    port = find_available_port(7870)\n",
    "    if port is None:\n",
    "        print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ポート {port} で起動します...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ポート {port} が使用中です。別のポートを試します...\")\n",
    "            # 別のポートで再試行\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"エラー: 利用可能なポートが見つかりませんでした。\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ポート {port} で再起動します...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"起動中にエラーが発生しました: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f5279",
   "metadata": {},
   "source": [
    "### 第2セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64044628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル ./sbintuitions/sarashina2.2-3B-instruct-v0.1 を4bit量子化でロード中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e4702dfd7d40c1b9cb3a5b061a2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_17220\\836626979.py:132: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルのロードが完了しました。デバイス: cuda (4bit量子化)\n",
      "ポート 7860 で起動します...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import socket\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"利用可能なポートを見つける\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "class SarashinaChatbot:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"./sbintuitions/sarashina2.2-3B-instruct-v0.1\"\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"モデルとトークナイザーをロード（4bit量子化）\"\"\"\n",
    "        try:\n",
    "            print(f\"モデル {self.model_name} を4bit量子化でロード中...\")\n",
    "            \n",
    "            # トークナイザーのロード\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            # pad_tokenの設定\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # 4bit量子化の設定\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            # モデルのロード\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # モデル設定の更新\n",
    "            if self.model.config.pad_token_id is None:\n",
    "                self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "            \n",
    "            print(f\"モデルのロードが完了しました。デバイス: {self.device} (4bit量子化)\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"エラー: モデルファイルが見つかりません: {self.model_name}\")\n",
    "            print(\"モデルパスが正しいか確認してください。\")\n",
    "        except ImportError as e:\n",
    "            print(f\"エラー: 必要なライブラリがインストールされていません: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"モデルのロード中にエラーが発生しました: {e}\")\n",
    "            print(\"モデルファイルが正しいパスに存在することを確認してください。\")\n",
    "    \n",
    "    def generate_response(self, message, history):\n",
    "        \"\"\"ユーザーのメッセージに対する応答を生成\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return \"モデルがロードされていません。モデルファイルを確認してください。\"\n",
    "        \n",
    "        try:\n",
    "            # 入力の検証\n",
    "            if not message or not message.strip():\n",
    "                return \"メッセージが空です。何か入力してください。\"\n",
    "            \n",
    "            # 会話履歴を含めたプロンプトの構築\n",
    "            prompt = \"\"\n",
    "            if history and len(history) > 0:\n",
    "                for i, (user_msg, ai_msg) in enumerate(history):\n",
    "                    prompt += f\"質問{i+1}: {user_msg}\\n回答{i+1}: {ai_msg}\\n\\n\"\n",
    "            \n",
    "            prompt += f\"質問: {message}\\n回答: \"\n",
    "            \n",
    "            # 入力のトークン化\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # 応答の生成\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # 応答のデコード\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # プロンプト部分を除去して応答のみを取得\n",
    "            if prompt in response:\n",
    "                response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            # 空の応答の場合の処理\n",
    "            if not response:\n",
    "                return \"応答を生成できませんでした。もう一度試してください。\"\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            return \"GPUメモリが不足しています。モデルを再起動してください。\"\n",
    "        except Exception as e:\n",
    "            return f\"応答生成中にエラーが発生しました: {str(e)}\"\n",
    "\n",
    "def create_chatbot_interface():\n",
    "    \"\"\"Gradioインターフェースを作成\"\"\"\n",
    "    chatbot = SarashinaChatbot()\n",
    "    \n",
    "    # Gradioインターフェースの定義\n",
    "    with gr.Blocks(title=\"シンプルAI チャットボット\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "        gr.Markdown(\"# 🤖 シンプルAI チャットボット\")\n",
    "        gr.Markdown(\"sarashina2.2-3B-instruct-v0.1モデルを使用したチャットボットです。\")\n",
    "        gr.Markdown(\"会話の履歴はAI出力に反映されません\")\n",
    "        \n",
    "        # チャット履歴\n",
    "        chatbot_interface = gr.Chatbot(\n",
    "            label=\"Chatbot\",\n",
    "            height=400,\n",
    "            value=[]\n",
    "        )\n",
    "        \n",
    "        # 入力フィールド\n",
    "        msg = gr.Textbox(\n",
    "            label=\"質問\",\n",
    "            placeholder=\"質問を入力してください\",\n",
    "            lines=1,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        # 送信ボタン\n",
    "        submit_btn = gr.Button(\"送信\", variant=\"primary\")\n",
    "        \n",
    "        def user_input(message, history):\n",
    "            \"\"\"ユーザー入力の処理\"\"\"\n",
    "            try:\n",
    "                # 入力の検証\n",
    "                if not message or not str(message).strip():\n",
    "                    return \"\", history\n",
    "                \n",
    "                # 履歴を適切な形式に変換\n",
    "                formatted_history = []\n",
    "                if history and len(history) > 0:\n",
    "                    for user_msg, ai_msg in history:\n",
    "                        formatted_history.append([user_msg, ai_msg])\n",
    "                \n",
    "                # 応答を生成\n",
    "                response = chatbot.generate_response(message, formatted_history)\n",
    "                \n",
    "                # 履歴に追加\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, response])\n",
    "                \n",
    "                return \"\", history\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"入力処理中にエラーが発生しました: {str(e)}\"\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, error_msg])\n",
    "                return \"\", history\n",
    "        \n",
    "        # イベントハンドラーの設定\n",
    "        submit_btn.click(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "        \n",
    "        # エンターキーで送信\n",
    "        msg.submit(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # チャットボットインターフェースを作成して起動\n",
    "    demo = create_chatbot_interface()\n",
    "    \n",
    "    # 利用可能なポートを探す（7860から10件確認）\n",
    "    port = find_available_port(7860, 10)\n",
    "    if port is None:\n",
    "        print(\"エラー: ポート7860-7869で利用可能なポートが見つかりませんでした。\")\n",
    "        print(\"他のアプリケーションを終了してから再試行してください。\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ポート {port} で起動します...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"起動中にエラーが発生しました: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibaraki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
