{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c3a3c0",
   "metadata": {},
   "source": [
    "### ç¬¬1ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4300a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PATH =\"å¤§è°·ç¿”å¹³_raw.txt\"  # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef8768",
   "metadata": {},
   "source": [
    "### ç¬¬2ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62a33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_ADAPTER_PATH = \"./checkpoint-160\"  # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6f9dc",
   "metadata": {},
   "source": [
    "### ç¬¬3ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1eb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_24716\\2245983033.py:887: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n",
      "D:\\temp\\ipykernel_24716\\2245983033.py:887: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆèµ·å‹• ===\n",
      "1/3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: ./sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...\n",
      "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcbc1371991453b9242507490497e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "2/3: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "3/3: ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\n",
      "âœ… ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\n",
      "ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "4/4: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰é©ç”¨ä¸­...\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹: ./output/checkpoint-160\n",
      "ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª: True\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­...\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­: ./output/checkpoint-160\n",
      "LoRAè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨å®Œäº†\n",
      "âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨å®Œäº†\n",
      "   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: ./sbintuitions/sarashina2.2-3b-instruct-v0.1\n",
      "   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: ./output/checkpoint-160\n",
      "   - LoRAãƒ¢ãƒ‡ãƒ«ã®å‹: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "   - LoRAè¨­å®š: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='sbintuitions/sarashina2.2-3b-instruct-v0.1', revision=None, inference_mode=True, r=128, target_modules={'q_proj', 'up_proj', 'down_proj', 'o_proj', 'gate_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‹: <class 'peft.tuners.lora.model.LoraModel'>\n",
      "   - ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 2,154,990,080\n",
      "   - å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 0\n",
      "\n",
      "=== æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ ===\n",
      "llm (ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«): <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "llm_with_lora (LoRAãƒ¢ãƒ‡ãƒ«): <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "model_type: original\n",
      "=== ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèªçµ‚äº† ===\n",
      "\n",
      "ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
      "UIã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...\n",
      "ãƒãƒ¼ãƒˆ 7894 ã§èµ·å‹•ã—ã¾ã™...\n",
      "* Running on local URL:  http://0.0.0.0:7894\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\n",
      "å¤‰æ›´å‰ã®model_type: original\n",
      "å¤‰æ›´å¾Œã®model_type: finetuning\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: finetuning\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\n",
      "å¤‰æ›´å‰ã®model_type: finetuning\n",
      "å¤‰æ›´å¾Œã®model_type: original\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: original\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\n",
      "\n",
      "RAGãƒ¢ãƒ¼ãƒ‰ã‚’FTãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã«æ›´æ–°\n",
      "\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\n",
      "å¤‰æ›´å‰ã®model_type: original\n",
      "å¤‰æ›´å¾Œã®model_type: finetuning\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: finetuning\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\n",
      "å¤‰æ›´å‰ã®model_type: finetuning\n",
      "å¤‰æ›´å¾Œã®model_type: original\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: original\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\n",
      "\n",
      "RAGãƒ¢ãƒ¼ãƒ‰è¨­å®š: FT\n",
      "ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: char\n",
      "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: vector\n",
      "\n",
      "=== å®Ÿè¡Œæ™‚ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\n",
      "å¤‰æ›´å‰ã®model_type: original\n",
      "UIã‹ã‚‰å—ã‘å–ã£ãŸmodel_mode: original\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: original\n",
      "llm_with_lora is None: False\n",
      "run_cfg.gen_mode: FT\n",
      "ä½¿ç”¨äºˆå®šãƒ¢ãƒ‡ãƒ«: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«\n",
      "=== å®Ÿè¡Œæ™‚ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\n",
      "\n",
      "FTãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©ç”¨: ã‚ãªãŸã¯å¤§è°·ç¿”å¹³ã«ã¤ã„ã¦çŸ¥ã‚Šå°½ãã—ã¦ã„ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚\n",
      "æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
      "\n",
      "=== å®Ÿè¡Œé–‹å§‹ ===\n",
      "æœ€çµ‚çš„ãªè¨­å®š:\n",
      "  - gen_mode: FT\n",
      "  - model_type: original\n",
      "  - chunk_mode: char\n",
      "  - search_mode: vector\n",
      "=== å®Ÿè¡Œé–‹å§‹çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\n",
      "model_type: original\n",
      "llm_with_lora is None: False\n",
      "llm is None: False\n",
      "LoRAãƒ¢ãƒ‡ãƒ«ã®å‹: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‹: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "ãƒ¢ãƒ¼ãƒ‰: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«\n",
      "âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "=== ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== FTãƒ¢ãƒ¼ãƒ‰ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\n",
      "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 6æ–‡å­—\n",
      "=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ‚äº† ===\n",
      "\n",
      "\n",
      "=== generate_with_limit ãƒ‡ãƒãƒƒã‚° ===\n",
      "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å‹: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‡ºåŠ›: input_ids shape: torch.Size([1, 3])\n",
      "å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "ç”Ÿæˆå‡ºåŠ› shape: torch.Size([1, 127])\n",
      "ç”Ÿã®ç”Ÿæˆçµæœ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "**å›ç­”4:**  \n",
      "ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ã¨ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã®å¹´é½¢å·®ã¯19æ­³ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å½¼ã‚‰ã®æ­£ç¢ºãªå‡ºç”Ÿå¹´ãŒä¸ãˆã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ãŒ1917å¹´ã«ç”Ÿã¾ã‚Œã¦ã„ãŸå ´åˆã€ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã¯1936å¹´ã«ç”Ÿã¾ã‚ŒãŸã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€å½¼ã‚‰ã¯29æ­³å·®ã¨ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã§ã¯å½¼ã‚‰ã®å¹´é½¢å·®ãŒ19æ­³ã§ã‚ã‚‹ã¨ã®ã¿è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\n",
      "å¾Œå‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "**å›ç­”4:**  \n",
      "ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ã¨ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã®å¹´é½¢å·®ã¯19æ­³ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å½¼ã‚‰ã®æ­£ç¢ºãªå‡ºç”Ÿå¹´ãŒä¸ãˆã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ãŒ1917å¹´ã«ç”Ÿã¾ã‚Œã¦ã„ãŸå ´åˆã€ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã¯1936å¹´ã«ç”Ÿã¾ã‚ŒãŸã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€å½¼ã‚‰ã¯29æ­³å·®ã¨ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã§ã¯å½¼ã‚‰ã®å¹´é½¢å·®ãŒ19æ­³ã§ã‚ã‚‹ã¨ã®ã¿è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\n",
      "æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ: ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ã¨ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã®å¹´é½¢å·®ã¯19æ­³ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å½¼ã‚‰ã®æ­£ç¢ºãªå‡ºç”Ÿå¹´ãŒä¸ãˆã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ãŒ1917å¹´ã«ç”Ÿã¾ã‚Œã¦ã„ãŸå ´åˆã€ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã¯1936å¹´ã«ç”Ÿã¾ã‚ŒãŸã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€å½¼ã‚‰ã¯29æ­³å·®ã¨ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã§ã¯å½¼ã‚‰ã®å¹´é½¢å·®ãŒ19æ­³ã§ã‚ã‚‹ã¨ã®ã¿è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\n",
      "=== generate_with_limit ãƒ‡ãƒãƒƒã‚°çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== è³ªå• ===\n",
      "ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "=== å›ç­” ===\n",
      "ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ã¨ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã®å¹´é½¢å·®ã¯19æ­³ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å½¼ã‚‰ã®æ­£ç¢ºãªå‡ºç”Ÿå¹´ãŒä¸ãˆã‚‰ã‚Œã¦ã„ãªã„ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€ã‚¸ãƒ§ãƒ³ãƒ»Fãƒ»ã‚±ãƒãƒ‡ã‚£ãŒ1917å¹´ã«ç”Ÿã¾ã‚Œã¦ã„ãŸå ´åˆã€ãƒªãƒ³ãƒ‰ãƒ³ãƒ»Bãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã¯1936å¹´ã«ç”Ÿã¾ã‚ŒãŸã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€å½¼ã‚‰ã¯29æ­³å·®ã¨ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã§ã¯å½¼ã‚‰ã®å¹´é½¢å·®ãŒ19æ­³ã§ã‚ã‚‹ã¨ã®ã¿è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€å…·ä½“çš„ãªå¹´é½¢å·®ã‚’å¹´å˜ä½ã§ç‰¹å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\n",
      "FTãƒ¢ãƒ¼ãƒ‰: å‚è€ƒæ–‡çŒ®ãªã—ã§å›ç­”ã‚’è¡¨ç¤º\n",
      "\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\n",
      "å¤‰æ›´å‰ã®model_type: original\n",
      "å¤‰æ›´å¾Œã®model_type: finetuning\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: finetuning\n",
      "=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\n",
      "\n",
      "RAGãƒ¢ãƒ¼ãƒ‰è¨­å®š: FT\n",
      "ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: char\n",
      "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: vector\n",
      "\n",
      "=== å®Ÿè¡Œæ™‚ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\n",
      "å¤‰æ›´å‰ã®model_type: finetuning\n",
      "UIã‹ã‚‰å—ã‘å–ã£ãŸmodel_mode: finetuning\n",
      "âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: finetuning\n",
      "llm_with_lora is None: False\n",
      "run_cfg.gen_mode: FT\n",
      "ä½¿ç”¨äºˆå®šãƒ¢ãƒ‡ãƒ«: LoRAé©ç”¨æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n",
      "=== å®Ÿè¡Œæ™‚ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\n",
      "\n",
      "FTãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©ç”¨: ã‚ãªãŸã¯å¤§è°·ç¿”å¹³ã«ã¤ã„ã¦çŸ¥ã‚Šå°½ãã—ã¦ã„ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚\n",
      "æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
      "\n",
      "=== å®Ÿè¡Œé–‹å§‹ ===\n",
      "æœ€çµ‚çš„ãªè¨­å®š:\n",
      "  - gen_mode: FT\n",
      "  - model_type: finetuning\n",
      "  - chunk_mode: char\n",
      "  - search_mode: vector\n",
      "=== å®Ÿè¡Œé–‹å§‹çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\n",
      "model_type: finetuning\n",
      "llm_with_lora is None: False\n",
      "llm is None: False\n",
      "LoRAãƒ¢ãƒ‡ãƒ«ã®å‹: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‹: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "ãƒ¢ãƒ¼ãƒ‰: FTãƒ¢ãƒ‡ãƒ«\n",
      "âœ… LoRAé©ç”¨æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "   - LoRAè¨­å®š: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='sbintuitions/sarashina2.2-3b-instruct-v0.1', revision=None, inference_mode=True, r=128, target_modules={'q_proj', 'up_proj', 'down_proj', 'o_proj', 'gate_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‹: <class 'peft.tuners.lora.model.LoraModel'>\n",
      "æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "=== ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== FTãƒ¢ãƒ¼ãƒ‰ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\n",
      "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 6æ–‡å­—\n",
      "=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ‚äº† ===\n",
      "\n",
      "\n",
      "=== generate_with_limit ãƒ‡ãƒãƒƒã‚° ===\n",
      "å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å‹: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‡ºåŠ›: input_ids shape: torch.Size([1, 3])\n",
      "å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n",
      "ç”Ÿæˆå‡ºåŠ› shape: torch.Size([1, 205])\n",
      "ç”Ÿã®ç”Ÿæˆçµæœ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "å›ç­”ï¼šç§ã¯Sarashina2ã¨è¨€ã„ã¾ã™ã€‚æœ¬æ—¥ã®æ—¥ä»˜ã¯2023å¹´3æœˆ18æ—¥ã§ã™ã®ã§ã€ç§ã®å¹´é½¢ã¯2023å¹´ã‹ã‚‰æœ¬æ—¥ã®æ—¥ä»˜ã®å¹´ã§ã‚ã‚‹2023ã‚’å¼•ã„ã¦ã€ã•ã‚‰ã«1ã‚’å¼•ãã“ã¨ã§è¨ˆç®—ã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€2023 - 2023 - 1 = 0 - 1 = 20ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®è¨ˆç®—ã¯èª•ç”Ÿæ—¥ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿéš›ã«ã¯ç§ã¯ã¾ã 20æ­³ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ­£ã—ãã¯ã€ç”Ÿã¾ã‚ŒãŸå¹´ã‹ã‚‰è¨ˆç®—ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œæ–‡ã§ã¯å…·ä½“çš„ãªå‡ºç”Ÿå¹´ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ2023å¹´3æœˆ18æ—¥ï¼‰æ™‚ç‚¹ã§ç§ãŒ20æ­³ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã¯è¨€ãˆã¾ã™ãŒã€å…·ä½“çš„ãªå¹´é½¢ã‚’ç®—å‡ºã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã‚‚ã—å‡ºç”Ÿå¹´ãŒ2002å¹´ã§ã‚ã‚Œã°ã€2023å¹´ã«ã¯21æ­³ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã«ã‚ã‚‹ã€Œä½•æ­³ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã¯ã€å…·ä½“çš„ãªå‡ºç”Ÿå¹´ã®æƒ…å ±ãŒãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã«åŸºã¥ã„ã¦ã€Œ20æ­³ä»¥ä¸Šã€ã¨å›ç­”ã™ã‚‹ã®ãŒé©åˆ‡ã§ã™ã€‚\n",
      "å¾Œå‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ: ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "å›ç­”ï¼šç§ã¯Sarashina2ã¨è¨€ã„ã¾ã™ã€‚æœ¬æ—¥ã®æ—¥ä»˜ã¯2023å¹´3æœˆ18æ—¥ã§ã™ã®ã§ã€ç§ã®å¹´é½¢ã¯2023å¹´ã‹ã‚‰æœ¬æ—¥ã®æ—¥ä»˜ã®å¹´ã§ã‚ã‚‹2023ã‚’å¼•ã„ã¦ã€ã•ã‚‰ã«1ã‚’å¼•ãã“ã¨ã§è¨ˆç®—ã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€2023 - 2023 - 1 = 0 - 1 = 20ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®è¨ˆç®—ã¯èª•ç”Ÿæ—¥ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿéš›ã«ã¯ç§ã¯ã¾ã 20æ­³ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ­£ã—ãã¯ã€ç”Ÿã¾ã‚ŒãŸå¹´ã‹ã‚‰è¨ˆç®—ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œæ–‡ã§ã¯å…·ä½“çš„ãªå‡ºç”Ÿå¹´ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ2023å¹´3æœˆ18æ—¥ï¼‰æ™‚ç‚¹ã§ç§ãŒ20æ­³ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã¯è¨€ãˆã¾ã™ãŒã€å…·ä½“çš„ãªå¹´é½¢ã‚’ç®—å‡ºã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã‚‚ã—å‡ºç”Ÿå¹´ãŒ2002å¹´ã§ã‚ã‚Œã°ã€2023å¹´ã«ã¯21æ­³ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã«ã‚ã‚‹ã€Œä½•æ­³ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã¯ã€å…·ä½“çš„ãªå‡ºç”Ÿå¹´ã®æƒ…å ±ãŒãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã«åŸºã¥ã„ã¦ã€Œ20æ­³ä»¥ä¸Šã€ã¨å›ç­”ã™ã‚‹ã®ãŒé©åˆ‡ã§ã™ã€‚\n",
      "æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ: å›ç­”ï¼šç§ã¯Sarashina2ã¨è¨€ã„ã¾ã™ã€‚æœ¬æ—¥ã®æ—¥ä»˜ã¯2023å¹´3æœˆ18æ—¥ã§ã™ã®ã§ã€ç§ã®å¹´é½¢ã¯2023å¹´ã‹ã‚‰æœ¬æ—¥ã®æ—¥ä»˜ã®å¹´ã§ã‚ã‚‹2023ã‚’å¼•ã„ã¦ã€ã•ã‚‰ã«1ã‚’å¼•ãã“ã¨ã§è¨ˆç®—ã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€2023 - 2023 - 1 = 0 - 1 = 20ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®è¨ˆç®—ã¯èª•ç”Ÿæ—¥ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿéš›ã«ã¯ç§ã¯ã¾ã 20æ­³ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ­£ã—ãã¯ã€ç”Ÿã¾ã‚ŒãŸå¹´ã‹ã‚‰è¨ˆç®—ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œæ–‡ã§ã¯å…·ä½“çš„ãªå‡ºç”Ÿå¹´ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ2023å¹´3æœˆ18æ—¥ï¼‰æ™‚ç‚¹ã§ç§ãŒ20æ­³ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã¯è¨€ãˆã¾ã™ãŒã€å…·ä½“çš„ãªå¹´é½¢ã‚’ç®—å‡ºã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã‚‚ã—å‡ºç”Ÿå¹´ãŒ2002å¹´ã§ã‚ã‚Œã°ã€2023å¹´ã«ã¯21æ­³ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã«ã‚ã‚‹ã€Œä½•æ­³ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã¯ã€å…·ä½“çš„ãªå‡ºç”Ÿå¹´ã®æƒ…å ±ãŒãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã«åŸºã¥ã„ã¦ã€Œ20æ­³ä»¥ä¸Šã€ã¨å›ç­”ã™ã‚‹ã®ãŒé©åˆ‡ã§ã™ã€‚\n",
      "=== generate_with_limit ãƒ‡ãƒãƒƒã‚°çµ‚äº† ===\n",
      "\n",
      "\n",
      "=== è³ªå• ===\n",
      "ä½•æ­³ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "=== å›ç­” ===\n",
      "å›ç­”ï¼šç§ã¯Sarashina2ã¨è¨€ã„ã¾ã™ã€‚æœ¬æ—¥ã®æ—¥ä»˜ã¯2023å¹´3æœˆ18æ—¥ã§ã™ã®ã§ã€ç§ã®å¹´é½¢ã¯2023å¹´ã‹ã‚‰æœ¬æ—¥ã®æ—¥ä»˜ã®å¹´ã§ã‚ã‚‹2023ã‚’å¼•ã„ã¦ã€ã•ã‚‰ã«1ã‚’å¼•ãã“ã¨ã§è¨ˆç®—ã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€2023 - 2023 - 1 = 0 - 1 = 20ã§ã™ã€‚ã—ã‹ã—ã€ã“ã®è¨ˆç®—ã¯èª•ç”Ÿæ—¥ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿéš›ã«ã¯ç§ã¯ã¾ã 20æ­³ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ­£ã—ãã¯ã€ç”Ÿã¾ã‚ŒãŸå¹´ã‹ã‚‰è¨ˆç®—ã—ç›´ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å•é¡Œæ–‡ã§ã¯å…·ä½“çš„ãªå‡ºç”Ÿå¹´ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ï¼ˆ2023å¹´3æœˆ18æ—¥ï¼‰æ™‚ç‚¹ã§ç§ãŒ20æ­³ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã¯è¨€ãˆã¾ã™ãŒã€å…·ä½“çš„ãªå¹´é½¢ã‚’ç®—å‡ºã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ã‚‚ã—å‡ºç”Ÿå¹´ãŒ2002å¹´ã§ã‚ã‚Œã°ã€2023å¹´ã«ã¯21æ­³ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å•é¡Œæ–‡ã«ã‚ã‚‹ã€Œä½•æ­³ã§ã™ã‹ï¼Ÿã€ã¨ã„ã†è³ªå•ã«å¯¾ã—ã¦ã¯ã€å…·ä½“çš„ãªå‡ºç”Ÿå¹´ã®æƒ…å ±ãŒãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã«åŸºã¥ã„ã¦ã€Œ20æ­³ä»¥ä¸Šã€ã¨å›ç­”ã™ã‚‹ã®ãŒé©åˆ‡ã§ã™ã€‚\n",
      "FTãƒ¢ãƒ¼ãƒ‰: å‚è€ƒæ–‡çŒ®ãªã—ã§å›ç­”ã‚’è¡¨ç¤º\n",
      "RAGãƒ¢ãƒ¼ãƒ‰ã‚’RAGãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã«ã ã‘åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
      "[è³ªå•]\n",
      "{question}\n",
      "\n",
      "[æƒ…å ±]\n",
      "{context}\n",
      "\n",
      "[å›ç­”]\n",
      "ã«æ›´æ–°\n",
      "RAGãƒ¢ãƒ¼ãƒ‰ã‚’FTãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã«æ›´æ–°\n",
      "RAGãƒ¢ãƒ¼ãƒ‰ã‚’RAGãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã«ã ã‘åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
      "[è³ªå•]\n",
      "{question}\n",
      "\n",
      "[æƒ…å ±]\n",
      "{context}\n",
      "\n",
      "[å›ç­”]\n",
      "ã«æ›´æ–°\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "try:\n",
    "    from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "    _HAS_JANOME = True\n",
    "except Exception:\n",
    "    _HAS_JANOME = False\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "import faiss\n",
    "import pykakasi\n",
    "import socket\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# =========================\n",
    "# RAG_CONFIG\n",
    "# =========================\n",
    "@dataclass\n",
    "class RAG_CONFIG:\n",
    "    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "    text_path: str = \"ã‚«ãƒ–ãƒˆãƒ ã‚·_raw.txt\"  # åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "\n",
    "    # ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰: \"RAG\" or \"FT\"\n",
    "    gen_mode: str = \"RAG\"\n",
    "\n",
    "    # ãƒãƒ£ãƒ³ã‚¯é–¢é€£\n",
    "    chunk_mode: str = \"char\"       # \"char\" or \"sentence\"\n",
    "    char_chunk_size: int = 100\n",
    "    overlap: bool = False          # False: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—\n",
    "    overlap_chars: int = 20        # charãƒ¢ãƒ¼ãƒ‰ç”¨ã®é‡ãªã‚Šæ–‡å­—æ•°\n",
    "\n",
    "    # æ¤œç´¢/ãƒªãƒ©ãƒ³ã‚¯\n",
    "    search_mode: str = \"vector\"    # \"vector\" or \"keyword\"\n",
    "    top_k_retrieve: int = 5        # æ¤œç´¢æ®µéšã®å€™è£œæ•°\n",
    "    top_k_final: int = 3           # LLMã¸æ¸¡ã™ä»¶æ•°\n",
    "    rerank_mode: bool = False      # False: ãƒªãƒ©ãƒ³ã‚¯ãªã— / True: ãƒªãƒ©ãƒ³ã‚¯ 5â†’3\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ï¼ˆç”Ÿæˆï¼‰\n",
    "    llm_model_name: str = \"./sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "    lora_adapter: str = \"\"  # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ã‚¹ï¼ˆä¾‹: \"./my-lora-adapter\"ï¼‰\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¤œç´¢/ãƒªãƒ©ãƒ³ã‚¯ï¼‰\n",
    "    embedding_model_name: str = \"./bge-m3\"\n",
    "    rerank_model_name: str = \"./japanese-bge-reranker-v2-m3-v1\"\n",
    "\n",
    "    # ç”ŸæˆæŒ™å‹•\n",
    "    use_chat_template: bool = False\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    # å›ç­”ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆé€”ä¸­ã‚«ãƒƒãƒˆé˜²æ­¢ã®ä¸¸ã‚è¾¼ã¿ã‚ã‚Šï¼‰\n",
    "    answer_token_limit: int = 256\n",
    "    answer_headroom: int = 64\n",
    "\n",
    "    # å®Ÿè¡Œç’°å¢ƒ\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤–å‡ºã—ï¼ˆç©ºãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰\n",
    "    rag_user_prompt_template: str = \"\"   # placeholders: {question}, {context}\n",
    "    rag_system_prompt: str = \"\"          # system ç”¨ï¼ˆç©ºãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "    ft_user_prompt_template: str = \"\"    # placeholders: {question}\n",
    "    ft_system_prompt: str = \"\"           # system ç”¨ï¼ˆç©ºãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "\n",
    "    # LoRAè¨­å®š\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = None  # Noneãªã‚‰è‡ªå‹•æ¤œå‡º\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "# =========================\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹ã®è¨­å®š\n",
    "DEFAULT_LORA_ADAPTER_PATH = \"./output/checkpoint-160\"  # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’æŒ‡å®šï¼ˆä¾‹: \"./output/checkpoint-160\" ã¾ãŸã¯ \"./my-lora-adapter\"ï¼‰\n",
    "\n",
    "# ç”Ÿæˆãƒ¢ãƒ‡ãƒ«é–¢é€£\n",
    "llm_tok = None\n",
    "llm = None  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå¸¸ã«ä¿æŒï¼‰\n",
    "llm_with_lora = None  # LoRAé©ç”¨æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆä¸€åº¦ã ã‘ä½œæˆï¼‰\n",
    "\n",
    "# åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«é–¢é€£\n",
    "emb_tok = None\n",
    "emb_model = None\n",
    "\n",
    "# ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«é–¢é€£\n",
    "rerank_tok = None\n",
    "rerank_model = None\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— : original or finetuning\n",
    "model_type = \"original\"\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ï¼ˆå…±é€šåŒ–ï¼‰\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã®è¨­å®š\n",
    "FAISS_INDEX_DIR = \"faiss_index\"\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
    "if not os.path.exists(FAISS_INDEX_DIR):\n",
    "    os.makedirs(FAISS_INDEX_DIR)\n",
    "\n",
    "# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ\n",
    "faiss_index = None\n",
    "chunk_data = None\n",
    "last_data_file = None  # æœ€å¾Œã«èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def convert_to_romaji(filename):\n",
    "    \"\"\"æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ­ãƒ¼ãƒå­—ã«å¤‰æ›\"\"\"\n",
    "    try:\n",
    "        # pykakasiã®æ–°ã—ã„APIã‚’ä½¿ç”¨\n",
    "        kakasi = pykakasi.kakasi()\n",
    "        \n",
    "        # æ‹¡å¼µå­ã‚’é™¤å»\n",
    "        name_without_ext = Path(filename).stem\n",
    "        \n",
    "        # ãƒ­ãƒ¼ãƒå­—å¤‰æ›\n",
    "        romaji_result = kakasi.convert(name_without_ext)\n",
    "        \n",
    "        # pykakasiã®çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ã€hepburnå½¢å¼ã®æ–‡å­—åˆ—ã‚’çµåˆ\n",
    "        if isinstance(romaji_result, list):\n",
    "            romaji_name = \"\"\n",
    "            for item in romaji_result:\n",
    "                if isinstance(item, dict) and 'hepburn' in item:\n",
    "                    romaji_name += item['hepburn']\n",
    "                elif isinstance(item, str):\n",
    "                    romaji_name += item\n",
    "        else:\n",
    "            romaji_name = str(romaji_result)\n",
    "        \n",
    "        # è‹±æ•°å­—ä»¥å¤–ã®æ–‡å­—ã‚’é™¤å»ã—ã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', romaji_name)\n",
    "        safe_name = re.sub(r'_+', '_', safe_name)  # é€£ç¶šã™ã‚‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’1ã¤ã«\n",
    "        safe_name = safe_name.strip('_')  # å‰å¾Œã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å»\n",
    "        \n",
    "        return safe_name if safe_name else \"data\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"data\"\n",
    "\n",
    "def get_faiss_index_path(data_filename):\n",
    "    \"\"\"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\"\"\"\n",
    "    # æ—¥æœ¬èªåˆ¤å®šï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰\n",
    "    japanese_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "    \n",
    "    if japanese_pattern.search(data_filename):\n",
    "        # æ—¥æœ¬èªã®å ´åˆã€ãƒ­ãƒ¼ãƒå­—å¤‰æ›\n",
    "        romaji_name = convert_to_romaji(data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{romaji_name}.faiss\")\n",
    "    else:\n",
    "        # æ—¥æœ¬èªã§ãªã„å ´åˆã€ãã®ã¾ã¾ä½¿ç”¨\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9._-]', '_', data_filename)\n",
    "        return os.path.join(FAISS_INDEX_DIR, f\"{safe_name}.faiss\")\n",
    "\n",
    "def update_rag_data(cfg: RAG_CONFIG):\n",
    "    \"\"\"RAGãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆFAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æ¡ä»¶åˆ†å²ï¼‰\"\"\"\n",
    "    global faiss_index, chunk_data, last_data_file\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "    current_file = cfg.text_path\n",
    "    try:\n",
    "        text = Path(current_file).read_text(encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "    current_filename = Path(current_file).name  # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­å«ã‚€ï¼‰\n",
    "    \n",
    "    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    faiss_index_path = get_faiss_index_path(current_filename)\n",
    "    \n",
    "    # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "            faiss_index = faiss.read_index(faiss_index_path)\n",
    "            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚‚å†æ§‹ç¯‰\n",
    "            chunk_data = build_chunks(cfg, text)\n",
    "            last_data_file = current_file\n",
    "            print(f\"æ—¢å­˜ã®FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {faiss_index_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰\n",
    "    if current_file != last_data_file or faiss_index is None:\n",
    "        try:\n",
    "            chunk_data = build_chunks(cfg, text)\n",
    "            \n",
    "            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰\n",
    "            if cfg.search_mode == \"vector\":\n",
    "                # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "                emb_tok = AutoTokenizer.from_pretrained(cfg.embedding_model_name)\n",
    "                emb_model = AutoModel.from_pretrained(cfg.embedding_model_name).to(cfg.device).eval()\n",
    "                \n",
    "                # æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰\n",
    "                chunk_embs = embed_texts(cfg, chunk_data, emb_tok, emb_model)\n",
    "                faiss_index = build_faiss_index(chunk_embs)\n",
    "                \n",
    "                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "                faiss.write_index(faiss_index, faiss_index_path)\n",
    "                print(f\"æ–°ã—ã„FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {faiss_index_path}\")\n",
    "                \n",
    "                last_data_file = current_file\n",
    "            else:\n",
    "                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä¸è¦\n",
    "                faiss_index = None\n",
    "                last_data_file = current_file\n",
    "                print(\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã€FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯æ§‹ç¯‰ã—ã¾ã›ã‚“\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==============\n",
    "# æ–‡å­—æ­£è¦åŒ–\n",
    "# ==============\n",
    "def _normalize(text: str) -> str:\n",
    "    # æ”¹è¡Œå‰Šé™¤ï¼‹NFKCï¼‹lower\n",
    "    return unicodedata.normalize(\"NFKC\", text.replace(\"\\n\", \"\")).lower()\n",
    "\n",
    "\n",
    "# ==========\n",
    "# ãƒãƒ£ãƒ³ã‚¯åŒ–\n",
    "# ==========\n",
    "def character_chunks(text: str, chunk_size: int, overlap: bool = False, overlap_size: int = 20) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks: List[str] = []\n",
    "    step = max(1, chunk_size - overlap_size) if overlap else chunk_size\n",
    "    for i in range(0, len(text), step):\n",
    "        piece = text[i:i + chunk_size]\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def sentence_chunks(text: str, overlap: bool = False) -> List[str]:\n",
    "    # å¥ç‚¹/ï¼Ÿ/ï¼ã§æ–‡åŒºåˆ‡ã‚Šï¼ˆå¥èª­ç‚¹ã¯æ–‡æœ«ã«å«ã‚ã‚‹ï¼‰\n",
    "    pattern = r'[^ã€‚ï¼ï¼Ÿ!?ã€‚]+[ã€‚ï¼ï¼Ÿ!?ã€‚]?'\n",
    "    sentences = [s.strip() for s in re.findall(pattern, text) if s and s.strip()]\n",
    "    if not overlap:\n",
    "        return sentences\n",
    "    chunks: List[str] = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        if i == 0:\n",
    "            chunks.append(s)\n",
    "        else:\n",
    "            chunks.append(sentences[i-1] + s)  # ç›´å‰ã®æ–‡ã‚’é‡ã­ã‚‹\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunks(cfg: RAG_CONFIG, raw_text: str) -> List[str]:\n",
    "    text = _normalize(raw_text)\n",
    "    if cfg.chunk_mode == \"char\":\n",
    "        return character_chunks(text, cfg.char_chunk_size, overlap=cfg.overlap, overlap_size=cfg.overlap_chars)\n",
    "    elif cfg.chunk_mode == \"sentence\":\n",
    "        return sentence_chunks(text, overlap=cfg.overlap)\n",
    "    else:\n",
    "        raise ValueError('chunk_mode must be \"char\" or \"sentence\"')\n",
    "\n",
    "\n",
    "# ===============\n",
    "# BM25ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "# ===============\n",
    "def _char_bigrams(s: str) -> List[str]:\n",
    "    if len(s) <= 1:\n",
    "        return [s] if s else []\n",
    "    return [s[i:i+2] for i in range(len(s)-1)]\n",
    "\n",
    "\n",
    "def _simple_words(s: str) -> List[str]:\n",
    "    return re.findall(r\"[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¥A-Za-z0-9]+\", s)\n",
    "\n",
    "\n",
    "def _tokenize_ja(text: str) -> List[str]:\n",
    "    # 1) Janomeï¼ˆåŸå½¢ãƒ»ä¸»è¦å“è©ï¼‰\n",
    "    if _HAS_JANOME:\n",
    "        t = JanomeTokenizer()\n",
    "        allowed = {\"åè©\", \"å‹•è©\", \"å½¢å®¹è©\", \"å‰¯è©\"}\n",
    "        toks = []\n",
    "        for tok in t.tokenize(text):\n",
    "            pos = tok.part_of_speech.split(\",\")[0]\n",
    "            if pos in allowed:\n",
    "                base = tok.base_form if tok.base_form != \"*\" else tok.surface\n",
    "                base = _normalize(base)\n",
    "                if base:\n",
    "                    toks.append(base)\n",
    "        if toks:\n",
    "            return toks\n",
    "    # 2) å˜èªæŠ½å‡º\n",
    "    toks = [_normalize(w) for w in _simple_words(text)]\n",
    "    if toks:\n",
    "        return toks\n",
    "    # 3) ãƒã‚¤ã‚°ãƒ©ãƒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    grams = _char_bigrams(text)\n",
    "    return grams if grams else [\"_empty_\"]\n",
    "\n",
    "\n",
    "# ============\n",
    "# åŸ‹ã‚è¾¼ã¿ç³»\n",
    "# ============\n",
    "@torch.no_grad()\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state  # (B, T, H)\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(cfg: RAG_CONFIG,\n",
    "                texts: List[str],\n",
    "                tok: AutoTokenizer,\n",
    "                model: AutoModel,\n",
    "                batch_size: int = 32) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        outputs = model(**inputs)\n",
    "        emb = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "        emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "        vecs.append(emb.cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "\n",
    "# ==================\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢/FAISS\n",
    "# ==================\n",
    "def build_faiss_index(embs: np.ndarray):\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index\n",
    "\n",
    "\n",
    "def vector_search_top_k(cfg: RAG_CONFIG,\n",
    "                        query: str,\n",
    "                        chunks: List[str],\n",
    "                        emb_tok: AutoTokenizer,\n",
    "                        emb_model: AutoModel,\n",
    "                        index,\n",
    "                        k: int) -> List[Tuple[int, float]]:\n",
    "    import faiss  # noqa\n",
    "    with torch.no_grad():\n",
    "        q_inputs = emb_tok([query], padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        q_out = emb_model(**q_inputs)\n",
    "        q_vec = mean_pooling(q_out, q_inputs[\"attention_mask\"])\n",
    "        q_vec = torch.nn.functional.normalize(q_vec, p=2, dim=1).cpu().numpy()\n",
    "    sims, ids = index.search(q_vec, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], sims[0]) if 0 <= i < len(chunks)]\n",
    "\n",
    "\n",
    "# =========\n",
    "# BM25æ¤œç´¢\n",
    "# =========\n",
    "def keyword_search_top_k(query: str,\n",
    "                         chunks: List[str],\n",
    "                         k: int) -> List[Tuple[int, float]]:\n",
    "    tokenized_docs = [_tokenize_ja(doc) for doc in chunks]\n",
    "    tokenized_docs = [t if t else [\"_empty_\"] for t in tokenized_docs]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    tq = _tokenize_ja(_normalize(query)) or [\"_empty_\"]\n",
    "    scores = bm25.get_scores(tq)\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [(int(i), float(scores[i])) for i in order]\n",
    "\n",
    "\n",
    "# =======\n",
    "# ãƒªãƒ©ãƒ³ã‚¯\n",
    "# =======\n",
    "@torch.no_grad()\n",
    "def rerank(cfg: RAG_CONFIG,\n",
    "           query: str,\n",
    "           docs: List[str],\n",
    "           tok: AutoTokenizer,\n",
    "           model: AutoModelForSequenceClassification,\n",
    "           batch_size: int = 16) -> List[int]:\n",
    "    scores_all: List[float] = []\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        inputs = tok([query] * len(batch_docs), batch_docs,\n",
    "                     padding=True, truncation=True, return_tensors=\"pt\").to(cfg.device)\n",
    "        outputs = model(**inputs)\n",
    "        scores = outputs.logits.squeeze(-1).detach().cpu().numpy().tolist()\n",
    "        scores_all.extend(scores)\n",
    "    return np.argsort(scores_all)[::-1].tolist()\n",
    "\n",
    "\n",
    "# ====================\n",
    "# LLM ãƒ­ãƒ¼ãƒ‰ï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "# ====================\n",
    "def load_clean_base_model():\n",
    "    \"\"\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆ4bité‡å­åŒ–å¯¾å¿œï¼‰\"\"\"\n",
    "    global llm_tok, llm\n",
    "    \n",
    "    try:\n",
    "        model_name = \"./sbintuitions/sarashina2.2-3b-instruct-v0.1\"\n",
    "        print(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}\")\n",
    "        \n",
    "        # 4bité‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "        print(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        llm_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’4bité‡å­åŒ–ã§èª­ã¿è¾¼ã¿\n",
    "        print(\"4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # pad_tokenãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®å‡¦ç†\n",
    "        if llm_tok.pad_token_id is None and llm_tok.eos_token_id is not None:\n",
    "            llm_tok.pad_token = llm_tok.eos_token\n",
    "        llm.config.pad_token_id = llm_tok.pad_token_id or llm_tok.eos_token_id\n",
    "        \n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        llm.eval()\n",
    "        \n",
    "        print(\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "        return llm_tok, llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def apply_lora_adapter(base_model, adapter_path):\n",
    "    \"\"\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ï¼ˆä¸€åº¦ã ã‘å®Ÿè¡Œï¼‰\"\"\"\n",
    "    try:\n",
    "        # ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {adapter_path}\")\n",
    "            return None\n",
    "        \n",
    "        from peft import PeftModel, PeftConfig\n",
    "        import warnings\n",
    "        \n",
    "        print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­: {adapter_path}\")\n",
    "        \n",
    "        # LoRAè¨­å®šã®èª­ã¿è¾¼ã¿\n",
    "        peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "        print(f\"LoRAè¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {peft_config.base_model_name_or_path}\")\n",
    "        \n",
    "        # è­¦å‘Šã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"peft\")\n",
    "            \n",
    "            # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨\n",
    "            lora_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "                is_trainable=False,  # æ¨è«–æ™‚ã¯False\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "        \n",
    "        print(\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨å®Œäº†\")\n",
    "        return lora_model\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨ã«å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¤–å‡ºã—å¯¾å¿œï¼‰\n",
    "# =========\n",
    "DEFAULT_RAG_USER = (\n",
    "    \"ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã«ã ã‘åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\\n\"\n",
    "    \"[è³ªå•]\\n{question}\\n\\n\"\n",
    "    \"[æƒ…å ±]\\n{context}\\n\\n\"\n",
    "    \"[å›ç­”]\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_FT_PROMPT = \"æ—¥æœ¬èªã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\"\n",
    "\n",
    "def _safe_format(template: str, **kwargs) -> str:\n",
    "    class _D(dict):\n",
    "        def __missing__(self, k): return \"{\"+k+\"}\"\n",
    "    return template.format_map(_D(**kwargs))\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_rag(cfg: RAG_CONFIG, question: str, retrieved_chunks: List[str], tok: AutoTokenizer) -> str:\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    user_tmpl = cfg.rag_user_prompt_template.strip() if cfg.rag_user_prompt_template else DEFAULT_RAG_USER\n",
    "    user_content = _safe_format(user_tmpl, question=question, context=context)\n",
    "\n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return user_content\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_prompt_ft(cfg: RAG_CONFIG, question: str, tok: AutoTokenizer) -> str:\n",
    "    system_tmpl = cfg.ft_system_prompt.strip() if cfg.ft_system_prompt else DEFAULT_FT_PROMPT\n",
    "    \n",
    "    if cfg.use_chat_template and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_tmpl},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return question\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆæ–‡æœ«ã‚¹ãƒŠãƒƒãƒ—ä»˜ï¼‰\n",
    "# ============================\n",
    "def _snap_to_sentence_boundary(text: str) -> str:\n",
    "    m = re.search(r'(.+?[ã€‚ï¼.!ï¼?ï¼Ÿ])(?:[^ã€‚ï¼.!ï¼?ï¼Ÿ]*)$', text)\n",
    "    return m.group(1) if m else text\n",
    "\n",
    "def truncate_to_token_limit(tok: AutoTokenizer, text: str, limit: int) -> str:\n",
    "    ids = tok(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) <= limit:\n",
    "        return _snap_to_sentence_boundary(text)\n",
    "    clipped = tok.decode(ids[:limit], skip_special_tokens=True)\n",
    "    return _snap_to_sentence_boundary(clipped)\n",
    "\n",
    "\n",
    "# =====\n",
    "# ç”Ÿæˆ\n",
    "# =====\n",
    "@torch.no_grad()\n",
    "def generate_with_limit(cfg: RAG_CONFIG,\n",
    "                        prompt: str,\n",
    "                        llm_tok: AutoTokenizer,\n",
    "                        llm: AutoModelForCausalLM) -> str:\n",
    "    print(f\"\\n=== generate_with_limit ãƒ‡ãƒãƒƒã‚° ===\")\n",
    "    print(f\"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(f\"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å‹: {type(llm)}\")\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹: {next(llm.parameters()).device if hasattr(llm, 'parameters') else 'Unknown'}\")\n",
    "    \n",
    "    inputs = llm_tok(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(cfg.device)\n",
    "    print(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‡ºåŠ›: input_ids shape: {inputs.input_ids.shape}\")\n",
    "    print(f\"å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹: {inputs.input_ids.device}\")\n",
    "    \n",
    "    out = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=cfg.answer_token_limit + cfg.answer_headroom,\n",
    "        temperature=cfg.temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=llm_tok.pad_token_id,\n",
    "        eos_token_id=llm_tok.eos_token_id or llm_tok.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    print(f\"ç”Ÿæˆå‡ºåŠ› shape: {out.shape}\")\n",
    "    raw = llm_tok.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"ç”Ÿã®ç”Ÿæˆçµæœ: {raw}\")\n",
    "    \n",
    "    text = raw.split(\"[å›ç­”]\", 1)[-1].strip() if \"[å›ç­”]\" in raw else raw.strip()\n",
    "    print(f\"å¾Œå‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ: {text}\")\n",
    "    \n",
    "    final_text = truncate_to_token_limit(llm_tok, text, cfg.answer_token_limit)\n",
    "    print(f\"æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ: {final_text}\")\n",
    "    print(\"=== generate_with_limit ãƒ‡ãƒãƒƒã‚°çµ‚äº† ===\\n\")\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "\n",
    "# ===========\n",
    "# å®Ÿè¡Œãƒ‘ã‚¹\n",
    "# ===========\n",
    "def run_pipeline(cfg: RAG_CONFIG, question: str):\n",
    "    global llm_tok, llm, llm_with_lora, emb_tok, emb_model, rerank_tok, rerank_model, model_type\n",
    "    \n",
    "    # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ› ===\n",
    "    print(f\"\\n=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\")\n",
    "    print(f\"model_type: {model_type}\")\n",
    "    print(f\"llm_with_lora is None: {llm_with_lora is None}\")\n",
    "    print(f\"llm is None: {llm is None}\")\n",
    "    if llm_with_lora is not None:\n",
    "        print(f\"LoRAãƒ¢ãƒ‡ãƒ«ã®å‹: {type(llm_with_lora)}\")\n",
    "    if llm is not None:\n",
    "        print(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‹: {type(llm)}\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "    if llm_tok is None or llm is None:\n",
    "        return \"ã‚¨ãƒ©ãƒ¼: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\", None\n",
    "    \n",
    "    # === ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰ ===\n",
    "    is_ft = (model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "    current_model = llm_with_lora if is_ft else llm\n",
    "    print(f\"ãƒ¢ãƒ¼ãƒ‰: {'FT' if is_ft else 'ãƒ™ãƒ¼ã‚¹'}ãƒ¢ãƒ‡ãƒ«\")\n",
    "    \n",
    "    if is_ft:\n",
    "        print(f\"âœ… LoRAé©ç”¨æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {type(current_model)}\")\n",
    "        # LoRAãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèª\n",
    "        if hasattr(current_model, 'peft_config'):\n",
    "            print(f\"   - LoRAè¨­å®š: {current_model.peft_config}\")\n",
    "        if hasattr(current_model, 'base_model'):\n",
    "            print(f\"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‹: {type(current_model.base_model)}\")\n",
    "    else:\n",
    "        print(f\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {type(current_model)}\")\n",
    "    \n",
    "    print(f\"æœ€çµ‚çš„ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«: {type(current_model)}\")\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹: {next(current_model.parameters()).device if hasattr(current_model, 'parameters') else 'Unknown'}\")\n",
    "    print(\"=== ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\\n\")\n",
    "    \n",
    "    # === FT ãƒ¢ãƒ¼ãƒ‰ï¼šRAGã‚’é€šã•ãš q ã®ã¿ ===\n",
    "    if cfg.gen_mode == \"FT\":\n",
    "        prompt = build_prompt_ft(cfg, question, llm_tok)\n",
    "        print(f\"\\n=== FTãƒ¢ãƒ¼ãƒ‰ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===\")\n",
    "        print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "        print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—\")\n",
    "        print(\"=== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ‚äº† ===\\n\")\n",
    "        \n",
    "        answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "        print(\"\\n=== è³ªå• ===\")\n",
    "        print(question)\n",
    "        print(\"\\n=== å›ç­” ===\")\n",
    "        print(answer)\n",
    "        return answer, None\n",
    "\n",
    "    # === RAG ãƒ¢ãƒ¼ãƒ‰ ===\n",
    "    print(f\"RAGãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    # 1) RAGãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ç¢ºèªï¼ˆå…±é€šã®FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ï¼‰\n",
    "    if not update_rag_data(cfg):\n",
    "        return \"ã‚¨ãƒ©ãƒ¼: RAGãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ\", None\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰å–å¾—\n",
    "    global faiss_index, chunk_data\n",
    "    \n",
    "    # 2) æ¤œç´¢ï¼ˆ5ä»¶ï¼‰\n",
    "    if cfg.search_mode == \"vector\":\n",
    "        if faiss_index is None:\n",
    "            return \"ã‚¨ãƒ©ãƒ¼: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒFAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“\", None\n",
    "        \n",
    "        # äº‹å‰æ§‹ç¯‰ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨\n",
    "        first_stage = vector_search_top_k(cfg, question, chunk_data, emb_tok, emb_model, faiss_index, k=cfg.top_k_retrieve)\n",
    "    elif cfg.search_mode == \"keyword\":\n",
    "        first_stage = keyword_search_top_k(question, chunk_data, k=cfg.top_k_retrieve)\n",
    "    else:\n",
    "        raise ValueError('search_mode must be \"vector\" or \"keyword\"')\n",
    "\n",
    "    cand_ids = [i for i, _ in first_stage]\n",
    "    candidates = [chunk_data[i] for i in cand_ids]\n",
    "\n",
    "    # 3) ãƒªãƒ©ãƒ³ã‚¯ï¼ˆ5â†’3ï¼‰ or ãã®ã¾ã¾3\n",
    "    if cfg.rerank_mode and len(candidates) > 0:\n",
    "        order = rerank(cfg, question, candidates, rerank_tok, rerank_model)\n",
    "        cand_ids = [cand_ids[i] for i in order]\n",
    "        candidates = [candidates[i] for i in order]\n",
    "\n",
    "    final_ids = cand_ids[:cfg.top_k_final]\n",
    "    retrieved = [chunk_data[i] for i in final_ids]\n",
    "\n",
    "    # 4) ç”Ÿæˆ\n",
    "    prompt = build_prompt_rag(cfg, question, retrieved, llm_tok)\n",
    "    answer = generate_with_limit(cfg, prompt, llm_tok, current_model)\n",
    "\n",
    "    # ===== ãƒ­ã‚°ï¼ˆå…¨æ–‡è¡¨ç¤ºï¼‰=====\n",
    "    print(f\"\\n=== æ¤œç´¢çµæœï¼ˆ{cfg.search_mode}, Top3è¡¨ç¤ºï¼å®Ÿéš›ã¯Top{cfg.top_k_retrieve}å–å¾—, \"\n",
    "          f\"CHUNK_MODE={cfg.chunk_mode}, OVER_LAP={cfg.overlap}ï¼‰===\\n\")\n",
    "    for rank, (i, s) in enumerate(first_stage[:cfg.top_k_final], 1):\n",
    "        print(f\"[æ¤œç´¢Top{rank}] id={i}, score={s:.6f}\")\n",
    "        print(chunk_data[i])  # å…¨æ–‡è¡¨ç¤º\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    if cfg.rerank_mode:\n",
    "        print(\"\\n=== ãƒªãƒ©ãƒ³ã‚¯å¾Œï¼ˆTop3æ¡ç”¨, å…¨æ–‡ï¼‰===\\n\")\n",
    "        for rank, i in enumerate(final_ids, 1):\n",
    "            print(f\"[RerankTop{rank}] id={i}\")\n",
    "            print(chunk_data[i])\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n=== è³ªå• ===\")\n",
    "    print(question)\n",
    "    print(\"\\n=== å›ç­” ===\")\n",
    "    print(answer)\n",
    "    \n",
    "    # æ¤œç´¢çµæœæƒ…å ±ã‚’æ§‹ç¯‰\n",
    "    references_info = []\n",
    "    for rank, i in enumerate(final_ids, 1):\n",
    "        chunk_text = chunk_data[i]\n",
    "        # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®\n",
    "        if len(chunk_text) > 200:\n",
    "            chunk_text = chunk_text[:200] + \"...\"\n",
    "        references_info.append(f\"**å‚è€ƒæ–‡çŒ®{rank}** (ID: {i})\\n{chunk_text}\")\n",
    "    \n",
    "    references_text = \"\\n\\n\".join(references_info)\n",
    "    \n",
    "    return answer, references_text\n",
    "\n",
    "\n",
    "# ================\n",
    "# Gradio UI\n",
    "# ================\n",
    "cfg = RAG_CONFIG()  # åŸºæœ¬è¨­å®šï¼ˆé©å®œæ›¸ãæ›ãˆï¼‰\n",
    "\n",
    "with gr.Blocks(title=\"Last Chatbot\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "    gr.Markdown(\"## Last Chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ===== å·¦ãƒšã‚¤ãƒ³ï¼ˆè¨­å®šï¼‰ =====\n",
    "        with gr.Column(scale=1, min_width=360):\n",
    "            with gr.Accordion(\"ãƒ¢ãƒ‡ãƒ« & å®Ÿè¡Œè¨­å®š\", open=True):\n",
    "                # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ç¾¤\n",
    "                cb_no_rag   = gr.Checkbox(label=\"RAGãªã—\", value=False, interactive=True)\n",
    "                cb_sentence = gr.Checkbox(label=\"æ–‡å˜ä½ãƒãƒ£ãƒ³ã‚¯åˆ†ã‘\", value=(cfg.chunk_mode==\"sentence\"))\n",
    "                cb_overlap  = gr.Checkbox(label=\"ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—\", value=cfg.overlap)\n",
    "                cb_rerank   = gr.Checkbox(label=\"ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°\", value=cfg.rerank_mode)\n",
    "\n",
    "                # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ™ã‚¯ãƒˆãƒ« / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰\n",
    "                rd_search = gr.Radio(\n",
    "                    choices=[\"ãƒ™ã‚¯ãƒˆãƒ«\", \"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\"],\n",
    "                    value=\"ãƒ™ã‚¯ãƒˆãƒ«\" if cfg.search_mode==\"vector\" else \"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\",\n",
    "                    label=\"æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰\",\n",
    "                )\n",
    "\n",
    "                # æ•°å€¤å…¥åŠ›ï¼ˆåˆæœŸçŠ¶æ…‹ã§ã¯è¡¨ç¤ºï¼‰\n",
    "                num_chunk = gr.Number(label=\"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰\", value=cfg.char_chunk_size, precision=0, interactive=True, visible=True)\n",
    "                num_overlap = gr.Number(label=\"ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰\", value=cfg.overlap_chars, precision=0, interactive=True, visible=cfg.overlap)\n",
    "\n",
    "                # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "                model_radio = gr.Radio(\n",
    "                    choices=[\"original\", \"finetuning\"],\n",
    "                    value=\"original\",\n",
    "                    label=\"ãƒ¢ãƒ‡ãƒ«é¸æŠ\",\n",
    "                    info=\"originalï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®sarashina2.2-3b-instruct-v0.1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã€finetuningï¼šè¨­å®šã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\"\n",
    "                )\n",
    "\n",
    "                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç·¨é›†å¯ï¼‰\n",
    "                with gr.Accordion(\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç·¨é›†å¯ï¼‰\", open=True):\n",
    "                    tb_prompt = gr.Textbox(\n",
    "                        label=\"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\",\n",
    "                        value=DEFAULT_RAG_USER,  # åˆæœŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆRAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "                        lines=8,\n",
    "                        max_lines=20,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    btn_reset_prompt = gr.Button(\"ğŸ”„ ãƒªã‚»ãƒƒãƒˆ\", size=\"sm\", variant=\"secondary\")\n",
    "\n",
    "                # RAGãƒ‡ãƒ¼ã‚¿å…¥åŠ›\n",
    "                rag_data_accordion = gr.Accordion(\"RAGãƒ‡ãƒ¼ã‚¿è¨­å®š\", open=True)\n",
    "                with rag_data_accordion:\n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å…¥åŠ›\n",
    "                    tb_file_path = gr.Textbox(\n",
    "                        label=\"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\",\n",
    "                        value=cfg.text_path,\n",
    "                        placeholder=\"ä¾‹: ./data.txt\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "\n",
    "                # è¡¨ç¤ºåˆ¶å¾¡é–¢æ•°\n",
    "                def _toggle_chunk_visibility(is_sentence: bool, is_overlap: bool):\n",
    "                    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®è¡¨ç¤ºåˆ¶å¾¡\n",
    "                    chunk_vis = gr.update(visible=not is_sentence)\n",
    "                    # æ–‡å˜ä½ã®å ´åˆã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’éè¡¨ç¤º\n",
    "                    if is_sentence:\n",
    "                        overlap_vis = gr.update(visible=False)\n",
    "                    else:\n",
    "                        overlap_vis = gr.update(visible=is_overlap)\n",
    "                    return chunk_vis, overlap_vis\n",
    "\n",
    "                def _toggle_overlap(is_overlap: bool, is_sentence: bool):\n",
    "                    # æ–‡å˜ä½ãƒãƒ£ãƒ³ã‚¯åˆ†ã‘ã®å ´åˆã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’éè¡¨ç¤º\n",
    "                    if is_sentence:\n",
    "                        return gr.update(visible=False)\n",
    "                    return gr.update(visible=is_overlap)\n",
    "\n",
    "                def _on_no_rag_change(is_no_rag: bool):\n",
    "                    # RAGãªã—æ™‚ã¯ãƒãƒ£ãƒ³ã‚¯é–¢é€£ã®è¨­å®šã‚’éè¡¨ç¤ºãƒ»éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–\n",
    "                    if is_no_rag:\n",
    "                        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºé–¢é€£ã‚’éè¡¨ç¤º\n",
    "                        chunk_vis = gr.update(visible=False)\n",
    "                        overlap_vis = gr.update(visible=False)\n",
    "                        # æ¤œç´¢é–¢é€£ã®è¨­å®šã‚’éè¡¨ç¤ºãƒ»éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–\n",
    "                        sentence_interactive = gr.update(visible=False, interactive=False)\n",
    "                        overlap_interactive = gr.update(visible=False, interactive=False)\n",
    "                        rerank_interactive = gr.update(visible=False, interactive=False)\n",
    "                        search_visible = gr.update(visible=False)  # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’éè¡¨ç¤º\n",
    "                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’FTç”¨ã«å¤‰æ›´\n",
    "                        prompt_content = DEFAULT_FT_PROMPT\n",
    "                        print(f\"RAGãƒ¢ãƒ¼ãƒ‰ã‚’FTãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’{DEFAULT_FT_PROMPT}ã«æ›´æ–°\")\n",
    "                    else:\n",
    "                        # RAGãƒ¢ãƒ¼ãƒ‰æ™‚ã¯é€šå¸¸è¡¨ç¤ºãƒ»ã‚¢ã‚¯ãƒ†ã‚£ãƒ–\n",
    "                        chunk_vis = gr.update(visible=not (cfg.chunk_mode==\"sentence\"))\n",
    "                        overlap_vis = gr.update(visible=cfg.overlap)\n",
    "                        sentence_interactive = gr.update(visible=True, interactive=True)\n",
    "                        overlap_interactive = gr.update(visible=True, interactive=True)\n",
    "                        rerank_interactive = gr.update(visible=True, interactive=True)\n",
    "                        search_visible = gr.update(visible=True)  # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’è¡¨ç¤º\n",
    "                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’RAGç”¨ã«å¤‰æ›´\n",
    "                        prompt_content = DEFAULT_RAG_USER\n",
    "                        print(f\"RAGãƒ¢ãƒ¼ãƒ‰ã‚’RAGãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’{DEFAULT_RAG_USER}ã«æ›´æ–°\")\n",
    "                    \n",
    "                    return (chunk_vis, overlap_vis, \n",
    "                            sentence_interactive, overlap_interactive, rerank_interactive, \n",
    "                            search_visible, prompt_content)\n",
    "\n",
    "\n",
    "\n",
    "                cb_sentence.change(_toggle_chunk_visibility, inputs=[cb_sentence, cb_overlap], outputs=[num_chunk, num_overlap])\n",
    "                cb_overlap.change(_toggle_overlap, inputs=[cb_overlap, cb_sentence], outputs=num_overlap)\n",
    "                # RAGãƒ¢ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼\n",
    "                def on_rag_mode_change(rag_mode):\n",
    "                    if rag_mode:\n",
    "                        cfg.gen_mode = \"FT\"\n",
    "                    else:\n",
    "                        cfg.gen_mode = \"RAG\"\n",
    "                    return rag_mode\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒªã‚»ãƒƒãƒˆæ©Ÿèƒ½\n",
    "                def reset_prompt_to_default(is_no_rag: bool):\n",
    "                    \"\"\"ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒªã‚»ãƒƒãƒˆ\"\"\"\n",
    "                    if is_no_rag:\n",
    "                        # FTãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "                        return DEFAULT_FT_PROMPT\n",
    "                    else:\n",
    "                        # RAGãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "                        return DEFAULT_RAG_USER\n",
    "                \n",
    "                cb_no_rag.change(_on_no_rag_change, inputs=cb_no_rag, outputs=[\n",
    "                    num_chunk, num_overlap, \n",
    "                    cb_sentence, cb_overlap, cb_rerank, rd_search, tb_prompt\n",
    "                ])\n",
    "                \n",
    "                # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆ\n",
    "                btn_reset_prompt.click(\n",
    "                    fn=reset_prompt_to_default,\n",
    "                    inputs=[cb_no_rag],\n",
    "                    outputs=[tb_prompt]\n",
    "                )\n",
    "                \n",
    "                # ãƒ¢ãƒ‡ãƒ«é¸æŠã®å¤‰æ›´ã‚’ç›£è¦–\n",
    "                def on_model_change(model_mode):\n",
    "                    global model_type\n",
    "                    print(f\"\\n=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´ ===\")\n",
    "                    print(f\"å¤‰æ›´å‰ã®model_type: {model_type}\")\n",
    "                    print(f\"å¤‰æ›´å¾Œã®model_type: {model_mode}\")\n",
    "                    model_type = model_mode\n",
    "                    print(f\"âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: {model_type}\")\n",
    "                    print(\"=== ãƒ¢ãƒ‡ãƒ«é¸æŠå¤‰æ›´çµ‚äº† ===\\n\")\n",
    "                \n",
    "                model_radio.change(\n",
    "                    fn=on_model_change,\n",
    "                    inputs=[model_radio],\n",
    "                    outputs=[]\n",
    "                )\n",
    "                \n",
    "\n",
    "\n",
    "        # ===== å³ãƒšã‚¤ãƒ³ï¼ˆãƒãƒ£ãƒƒãƒˆï¼‰ =====\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(height=520, bubble_full_width=False, show_copy_button=True)\n",
    "            tb_question = gr.Textbox(label=\"è³ªå•\", placeholder=\"ã“ã“ã«è³ªå•ã‚’å…¥åŠ›\")\n",
    "            btn_send = gr.Button(\"é€ä¿¡\", variant=\"primary\")\n",
    "\n",
    "            # å®Ÿè¡Œé–¢æ•°\n",
    "            def _run_chat(\n",
    "                q, history,\n",
    "                is_no_rag, is_sentence, is_overlap, is_rerank,\n",
    "                search_choice,\n",
    "                chunk_size, overlap_size,\n",
    "                model_mode, user_prompt, file_path\n",
    "            ):\n",
    "                if not q or not str(q).strip():\n",
    "                    return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\"]], \"\"  # â† é€ä¿¡å¾Œã‚¯ãƒªã‚¢\n",
    "\n",
    "                # cfg ã‚’çµ„ã¿ç«‹ã¦\n",
    "                run_cfg = RAG_CONFIG()\n",
    "                # RAGãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®šï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "                run_cfg.gen_mode = \"FT\" if is_no_rag else \"RAG\"\n",
    "                run_cfg.chunk_mode = \"sentence\" if is_sentence else \"char\"\n",
    "                run_cfg.overlap = bool(is_overlap)\n",
    "                # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®åæ˜ \n",
    "                run_cfg.search_mode = \"vector\" if (\"ãƒ™ã‚¯ãƒˆãƒ«\" in search_choice) else \"keyword\"\n",
    "                \n",
    "                print(f\"RAGãƒ¢ãƒ¼ãƒ‰è¨­å®š: {'FT' if is_no_rag else 'RAG'}\")\n",
    "                print(f\"ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: {run_cfg.chunk_mode}\")\n",
    "                print(f\"æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: {run_cfg.search_mode}\")\n",
    "\n",
    "                # æ•°å€¤ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "                if run_cfg.chunk_mode == \"char\":\n",
    "                    try:\n",
    "                        run_cfg.char_chunk_size = int(chunk_size)\n",
    "                        if run_cfg.char_chunk_size <= 0:\n",
    "                            return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯1ä»¥ä¸Šã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\"]], \"\"\n",
    "                    except Exception:\n",
    "                        return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æ•°å€¤ãŒä¸æ­£ã§ã™ã€‚\"]], \"\"\n",
    "                try:\n",
    "                    run_cfg.overlap_chars = int(overlap_size) if is_overlap else run_cfg.overlap_chars\n",
    "                    if is_overlap and run_cfg.chunk_mode == \"char\" and run_cfg.overlap_chars >= run_cfg.char_chunk_size:\n",
    "                        return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã¯ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœªæº€ã«ã—ã¦ãã ã•ã„ã€‚\"]], \"\"\n",
    "                except Exception:\n",
    "                    return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã®æ•°å€¤ãŒä¸æ­£ã§ã™ã€‚\"]], \"\"\n",
    "\n",
    "                run_cfg.rerank_mode = bool(is_rerank)\n",
    "                \n",
    "                # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ›´æ–°\n",
    "                global model_type\n",
    "                print(f\"\\n=== å®Ÿè¡Œæ™‚ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===\")\n",
    "                print(f\"å¤‰æ›´å‰ã®model_type: {model_type}\")\n",
    "                print(f\"UIã‹ã‚‰å—ã‘å–ã£ãŸmodel_mode: {model_mode}\")\n",
    "                model_type = model_mode\n",
    "                print(f\"âœ… model_typeã‚’æ›´æ–°ã—ã¾ã—ãŸ: {model_type}\")\n",
    "                print(f\"llm_with_lora is None: {llm_with_lora is None}\")\n",
    "                print(f\"run_cfg.gen_mode: {run_cfg.gen_mode}\")\n",
    "                \n",
    "                # ãƒ¢ãƒ‡ãƒ«é¸æŠã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "                is_ft = (model_type == \"finetuning\" and llm_with_lora is not None)\n",
    "                print(f\"ä½¿ç”¨äºˆå®šãƒ¢ãƒ‡ãƒ«: {'LoRAé©ç”¨æ¸ˆã¿' if is_ft else 'ãƒ™ãƒ¼ã‚¹'}ãƒ¢ãƒ‡ãƒ«\")\n",
    "                print(\"=== å®Ÿè¡Œæ™‚ãƒ‡ãƒãƒƒã‚°æƒ…å ±çµ‚äº† ===\\n\")\n",
    "                \n",
    "                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé©ç”¨ï¼šç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ç‰‡æ–¹ã«å…¥ã‚Œã‚‹ï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "                if run_cfg.gen_mode == \"FT\":\n",
    "                    run_cfg.ft_system_prompt = user_prompt or \"\"\n",
    "                    run_cfg.rag_user_prompt_template = \"\"  # RAGå´ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™\n",
    "                    print(f\"FTãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©ç”¨: {run_cfg.ft_system_prompt}\")\n",
    "                else:\n",
    "                    run_cfg.rag_user_prompt_template = user_prompt or \"\"\n",
    "                    run_cfg.ft_system_prompt = \"\"   # FTå´ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™\n",
    "                    print(f\"RAGãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©ç”¨: {run_cfg.rag_user_prompt_template}\")\n",
    "\n",
    "                # RAGãªã—æ™‚ã®å¿…é ˆãƒã‚§ãƒƒã‚¯ï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "                if run_cfg.gen_mode == \"FT\" and model_type == \"finetuning\" and llm_with_lora is None:\n",
    "                    print(f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: RAGãªã—æ™‚ã¯LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\")\n",
    "                    print(f\"   - gen_mode: {run_cfg.gen_mode}\")\n",
    "                    print(f\"   - model_type: {model_type}\")\n",
    "                    print(f\"   - llm_with_lora is None: {llm_with_lora is None}\")\n",
    "                    return (history or []) + [[q, \"ã‚¨ãƒ©ãƒ¼: RAGãªã—æ™‚ã¯LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\"]], \"\"\n",
    "                \n",
    "                # RAGãƒ¢ãƒ¼ãƒ‰æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ›´æ–°\n",
    "                if run_cfg.gen_mode == \"RAG\" and file_path and file_path.strip():\n",
    "                    run_cfg.text_path = file_path.strip()\n",
    "                    print(f\"RAGãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ›´æ–°: {run_cfg.text_path}\")\n",
    "\n",
    "                # å®Ÿè¡Œ\n",
    "                print(f\"\\n=== å®Ÿè¡Œé–‹å§‹ ===\")\n",
    "                print(f\"æœ€çµ‚çš„ãªè¨­å®š:\")\n",
    "                print(f\"  - gen_mode: {run_cfg.gen_mode}\")\n",
    "                print(f\"  - model_type: {model_type}\")\n",
    "                print(f\"  - chunk_mode: {run_cfg.chunk_mode}\")\n",
    "                print(f\"  - search_mode: {run_cfg.search_mode}\")\n",
    "                print(\"=== å®Ÿè¡Œé–‹å§‹çµ‚äº† ===\\n\")\n",
    "                \n",
    "                try:\n",
    "                    result = run_pipeline(run_cfg, q)\n",
    "                    if isinstance(result, tuple):\n",
    "                        ans, references = result\n",
    "                    else:\n",
    "                        ans, references = result, None\n",
    "                except Exception as e:\n",
    "                    ans = f\"å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼: {e}\"\n",
    "                    references = None\n",
    "\n",
    "                # RAGãƒ¢ãƒ¼ãƒ‰ã®æ™‚ã¯å‚è€ƒæ–‡çŒ®æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼ˆChangeLLM.pyã¨åŒã˜æ–¹å¼ï¼‰\n",
    "                if run_cfg.gen_mode == \"RAG\" and references:\n",
    "                    full_answer = f\"{ans}\\n\\n---\\n**å‚è€ƒæ–‡çŒ®**\\n{references}\"\n",
    "                    print(f\"RAGãƒ¢ãƒ¼ãƒ‰: å‚è€ƒæ–‡çŒ®æƒ…å ±ã‚’å«ã‚ã¦å›ç­”ã‚’è¡¨ç¤º\")\n",
    "                else:\n",
    "                    full_answer = ans\n",
    "                    print(f\"FTãƒ¢ãƒ¼ãƒ‰: å‚è€ƒæ–‡çŒ®ãªã—ã§å›ç­”ã‚’è¡¨ç¤º\")\n",
    "\n",
    "                # â† é€ä¿¡å¾Œã¯è³ªå•æ¬„ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŸã‚ \"\" ã‚’è¿”ã™\n",
    "                return (history or []) + [[q, full_answer]], \"\"\n",
    "\n",
    "            # å‡ºåŠ›ã‚’ chatbot ã¨ tb_questionï¼ˆç©ºæ–‡å­—ã§ã‚¯ãƒªã‚¢ï¼‰ã«\n",
    "            btn_send.click(\n",
    "                _run_chat,\n",
    "                inputs=[\n",
    "                    tb_question, chatbot,\n",
    "                    cb_no_rag, cb_sentence, cb_overlap, cb_rerank,\n",
    "                    rd_search,\n",
    "                    num_chunk, num_overlap,\n",
    "                    model_radio, tb_prompt, tb_file_path\n",
    "                ],\n",
    "                outputs=[chatbot, tb_question]\n",
    "            )\n",
    "\n",
    "            # Enterã§ã‚‚é€ä¿¡ã—ã¦åŒæ§˜ã«ã‚¯ãƒªã‚¢\n",
    "            tb_question.submit(\n",
    "                _run_chat,\n",
    "                inputs=[\n",
    "                    tb_question, chatbot,\n",
    "                    cb_no_rag, cb_sentence, cb_overlap, cb_rerank,\n",
    "                    rd_search,\n",
    "                    num_chunk, num_overlap,\n",
    "                    model_radio, tb_prompt, tb_file_path\n",
    "                ],\n",
    "                outputs=[chatbot, tb_question]\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆèµ·å‹• ===\")\n",
    "    \n",
    "    # 1. ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "    print(\"1/3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆsarashinaï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    print(\"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    llm_tok, llm = load_clean_base_model()\n",
    "    if llm_tok is None or llm is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "        exit(1)\n",
    "    print(\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    \n",
    "    # 2. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆbge-m3ï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "    print(\"2/3: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    try:\n",
    "        emb_tok = AutoTokenizer.from_pretrained(\"./bge-m3\")\n",
    "        emb_model = AutoModel.from_pretrained(\"./bge-m3\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        print(\"âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 3. ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "    print(\"3/3: ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­\")\n",
    "    try:\n",
    "        rerank_tok = AutoTokenizer.from_pretrained(\"./japanese-bge-reranker-v2-m3-v1\")\n",
    "        rerank_model = AutoModelForSequenceClassification.from_pretrained(\"./japanese-bge-reranker-v2-m3-v1\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        print(\"âœ… ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼: ãƒªãƒ©ãƒ³ã‚¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    \n",
    "    # 4. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰ã«é©ç”¨\n",
    "    print(\"4/4: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’äº‹å‰é©ç”¨ä¸­...\")\n",
    "    print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "    print(f\"ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª: {os.path.exists(DEFAULT_LORA_ADAPTER_PATH)}\")\n",
    "    \n",
    "    if os.path.exists(DEFAULT_LORA_ADAPTER_PATH):\n",
    "        try:\n",
    "            print(f\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é©ç”¨ä¸­...\")\n",
    "            llm_with_lora = apply_lora_adapter(llm, DEFAULT_LORA_ADAPTER_PATH)\n",
    "            if llm_with_lora is not None:\n",
    "                print(\"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨å®Œäº†\")\n",
    "                print(f\"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {cfg.llm_model_name}\")\n",
    "                print(f\"   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {DEFAULT_LORA_ADAPTER_PATH}\")\n",
    "                print(f\"   - LoRAãƒ¢ãƒ‡ãƒ«ã®å‹: {type(llm_with_lora)}\")\n",
    "                \n",
    "                # LoRAãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèª\n",
    "                if hasattr(llm_with_lora, 'peft_config'):\n",
    "                    print(f\"   - LoRAè¨­å®š: {llm_with_lora.peft_config}\")\n",
    "                if hasattr(llm_with_lora, 'base_model'):\n",
    "                    print(f\"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‹: {type(llm_with_lora.base_model)}\")\n",
    "                \n",
    "                # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ç¢ºèª\n",
    "                try:\n",
    "                    total_params = sum(p.numel() for p in llm_with_lora.parameters())\n",
    "                    trainable_params = sum(p.numel() for p in llm_with_lora.parameters() if p.requires_grad)\n",
    "                    print(f\"   - ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "                    print(f\"   - å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"âš ï¸ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨ã«å¤±æ•—ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "                llm_with_lora = None\n",
    "        except Exception as e:\n",
    "            print(f\"è­¦å‘Š: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®äº‹å‰é©ç”¨ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "            print(\"ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "            llm_with_lora = None\n",
    "    else:\n",
    "        print(\"LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§ç¶šè¡Œã—ã¾ã™\")\n",
    "        llm_with_lora = None\n",
    "    \n",
    "    # æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’ç¢ºèª\n",
    "    print(f\"\\n=== æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ ===\")\n",
    "    print(f\"llm (ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«): {type(llm) if llm is not None else 'None'}\")\n",
    "    print(f\"llm_with_lora (LoRAãƒ¢ãƒ‡ãƒ«): {type(llm_with_lora) if llm_with_lora is not None else 'None'}\")\n",
    "    print(f\"model_type: {model_type}\")\n",
    "    print(\"=== ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ç¢ºèªçµ‚äº† ===\\n\")\n",
    "    \n",
    "    print(\"ğŸ‰ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"UIã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    port = find_available_port(7890)\n",
    "    if port is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ãƒãƒ¼ãƒˆ {port} ã§èµ·å‹•ã—ã¾ã™...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ãŒä½¿ç”¨ä¸­ã§ã™ã€‚åˆ¥ã®ãƒãƒ¼ãƒˆã‚’è©¦ã—ã¾ã™...\")\n",
    "            # åˆ¥ã®ãƒãƒ¼ãƒˆã§å†è©¦è¡Œ\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ã§å†èµ·å‹•ã—ã¾ã™...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibaraki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
