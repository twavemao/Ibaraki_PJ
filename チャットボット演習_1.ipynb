{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11507b0",
   "metadata": {},
   "source": [
    "### ç¬¬1ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c596c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_17220\\2855017450.py:69: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒãƒ¼ãƒˆ 7873 ã§èµ·å‹•ã—ã¾ã™...\n",
      "* Running on local URL:  http://0.0.0.0:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "class DictionaryChatbot:\n",
    "    def __init__(self):\n",
    "        # ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šå¿œç­”ï¼‰\n",
    "        self.rules = {\n",
    "            \"ã“ã‚“ã«ã¡ã¯\": \"ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ\",\n",
    "            \"ãŠç–²ã‚Œæ§˜\": \"ãŠç–²ã‚Œæ§˜ã§ã™ï¼\",\n",
    "            \"ã‚ã‚ŠãŒã¨ã†\": \"ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼\",\n",
    "            \"ã•ã‚ˆã†ãªã‚‰\": \"ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸãŠè©±ã—ã—ã¾ã—ã‚‡ã†ï¼\"\n",
    "        }\n",
    "    \n",
    "    def find_response(self, user_input):\n",
    "        \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰é©åˆ‡ãªå¿œç­”ã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "        if not user_input.strip():\n",
    "            return \"ä½•ã‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\"\n",
    "        \n",
    "        user_input = user_input.strip()\n",
    "        \n",
    "        # å®Œå…¨ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        if user_input in self.rules:\n",
    "            return self.rules[user_input]\n",
    "        \n",
    "        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”\n",
    "        return \"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãã®è³ªå•ã«ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚åˆ¥ã®è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\"\n",
    "    \n",
    "    def add_rule(self, key, response):\n",
    "        \"\"\"æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ \"\"\"\n",
    "        self.rules[key] = response\n",
    "        return f\"æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {key} â†’ {response}\"\n",
    "    \n",
    "    def show_rules(self):\n",
    "        \"\"\"ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ã‚’è¡¨å½¢å¼ã§è¡¨ç¤º\"\"\"\n",
    "        rules_data = []\n",
    "        for i, (key, response) in enumerate(self.rules.items()):\n",
    "            rules_data.append([i+1, key, response])\n",
    "        return rules_data\n",
    "\n",
    "\n",
    "def add_new_rule(key, response):\n",
    "    \"\"\"æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ \"\"\"\n",
    "    if key and response:\n",
    "        result = chatbot.add_rule(key, response)\n",
    "        # å…¥åŠ›æ¬„ã‚’ç©ºã«ã™ã‚‹ãŸã‚ã«ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "        return result, \"\", \"\", chatbot.show_rules()\n",
    "    return \"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å¿œç­”ã®ä¸¡æ–¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\", key, response, chatbot.show_rules()\n",
    "\n",
    "\n",
    "# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "chatbot = DictionaryChatbot()\n",
    "\n",
    "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
    "with gr.Blocks(title=\"è¾æ›¸å‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "    gr.Markdown(\"# è¾æ›¸å‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ğŸ¤–\")\n",
    "    \n",
    "    with gr.Tab(\"ãƒãƒ£ãƒƒãƒˆ\"):\n",
    "        chatbot_interface = gr.Chatbot(\n",
    "            label=\"Chatbot\",\n",
    "            height=400,\n",
    "            value=[]\n",
    "        )\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            label=\"è³ªå•\",\n",
    "            placeholder=\"è³ªå•ã‚’å…¥åŠ›\",\n",
    "            lines=1,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        submit_btn = gr.Button(\"é€ä¿¡\", variant=\"primary\")\n",
    "        \n",
    "        def user_input(message, history):\n",
    "            \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†\"\"\"\n",
    "            try:\n",
    "                # å…¥åŠ›ã®æ¤œè¨¼\n",
    "                if not message.strip():\n",
    "                    return \"\", history\n",
    "                \n",
    "                # å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›\n",
    "                formatted_history = []\n",
    "                if history and len(history) > 0:\n",
    "                    for user_msg, ai_msg in history:\n",
    "                        formatted_history.append([user_msg, ai_msg])\n",
    "                \n",
    "                # å¿œç­”ã‚’ç”Ÿæˆ\n",
    "                response = chatbot.find_response(message)\n",
    "                \n",
    "                # å±¥æ­´ã«è¿½åŠ \n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, response])\n",
    "                \n",
    "                return \"\", history\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"å…¥åŠ›å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, error_msg])\n",
    "                return \"\", history\n",
    "    \n",
    "    with gr.Tab(\"ãƒ«ãƒ¼ãƒ«ç®¡ç†\"):\n",
    "        gr.Markdown(\"## ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ä¸€è¦§\")\n",
    "        rules_display = gr.Dataframe(\n",
    "            headers=[\"ID\", \"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\", \"å¿œç­”\"],\n",
    "            datatype=[\"number\", \"str\", \"str\"],\n",
    "            col_count=(3, \"fixed\"),\n",
    "            value=chatbot.show_rules(),\n",
    "            interactive=False\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"## æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ \")\n",
    "        with gr.Row():\n",
    "            key_input = gr.Textbox(label=\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\", placeholder=\"ä¾‹ï¼šãŠç–²ã‚Œæ§˜\")\n",
    "            response_input = gr.Textbox(label=\"å¿œç­”\", placeholder=\"ä¾‹ï¼šãŠç–²ã‚Œæ§˜ã§ã™ï¼\")\n",
    "        add_button = gr.Button(\"ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ \", variant=\"primary\")\n",
    "        add_result = gr.Textbox(label=\"çµæœ\", interactive=False)\n",
    "        \n",
    "        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼\n",
    "        add_button.click(\n",
    "            fn=add_new_rule,\n",
    "            inputs=[key_input, response_input],\n",
    "            outputs=[add_result, key_input, response_input, rules_display]\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"ä½¿ã„æ–¹\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ## ä½¿ã„æ–¹\n",
    "        \n",
    "        ### ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½\n",
    "        - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹å ´åˆã€å¯¾å¿œã™ã‚‹å¿œç­”ã‚’è¿”ã—ã¾ã™\n",
    "        \n",
    "        ### ãƒ«ãƒ¼ãƒ«ç®¡ç†\n",
    "        - æ–°ã—ã„ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã§ãã¾ã™\n",
    "        - ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã‚’ç¢ºèªã§ãã¾ã™\n",
    "        \n",
    "        ### ä¾‹\n",
    "        - ã€Œã“ã‚“ã«ã¡ã¯ã€â†’ã€Œã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿã€\n",
    "        \"\"\")\n",
    "        \n",
    "    \n",
    "    submit_btn.click(\n",
    "        user_input,\n",
    "        inputs=[msg, chatbot_interface],\n",
    "        outputs=[msg, chatbot_interface]\n",
    "    )\n",
    "    \n",
    "    msg.submit(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’æ¢ã™\n",
    "    port = find_available_port(7870)\n",
    "    if port is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ãƒãƒ¼ãƒˆ {port} ã§èµ·å‹•ã—ã¾ã™...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except OSError as e:\n",
    "        if \"Address already in use\" in str(e):\n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ãŒä½¿ç”¨ä¸­ã§ã™ã€‚åˆ¥ã®ãƒãƒ¼ãƒˆã‚’è©¦ã—ã¾ã™...\")\n",
    "            # åˆ¥ã®ãƒãƒ¼ãƒˆã§å†è©¦è¡Œ\n",
    "            port = find_available_port(port + 1)\n",
    "            if port is None:\n",
    "                print(\"ã‚¨ãƒ©ãƒ¼: åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"ãƒãƒ¼ãƒˆ {port} ã§å†èµ·å‹•ã—ã¾ã™...\")\n",
    "            demo.launch(\n",
    "                inbrowser=True,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                share=False,\n",
    "                show_error=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f5279",
   "metadata": {},
   "source": [
    "### ç¬¬2ã‚»ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64044628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒ¢ãƒ‡ãƒ« ./sbintuitions/sarashina2.2-3B-instruct-v0.1 ã‚’4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰ä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e4702dfd7d40c1b9cb3a5b061a2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\temp\\ipykernel_17220\\836626979.py:132: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_interface = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ‡ãƒã‚¤ã‚¹: cuda (4bité‡å­åŒ–)\n",
      "ãƒãƒ¼ãƒˆ 7860 ã§èµ·å‹•ã—ã¾ã™...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import socket\n",
    "\n",
    "def find_available_port(start_port=7860, max_attempts=100):\n",
    "    \"\"\"åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
    "    for port in range(start_port, start_port + max_attempts):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "class SarashinaChatbot:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"./sbintuitions/sarashina2.2-3B-instruct-v0.1\"\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆ4bité‡å­åŒ–ï¼‰\"\"\"\n",
    "        try:\n",
    "            print(f\"ãƒ¢ãƒ‡ãƒ« {self.model_name} ã‚’4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "            \n",
    "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            # pad_tokenã®è¨­å®š\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # 4bité‡å­åŒ–ã®è¨­å®š\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®æ›´æ–°\n",
    "            if self.model.config.pad_token_id is None:\n",
    "                self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "            \n",
    "            print(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ‡ãƒã‚¤ã‚¹: {self.device} (4bité‡å­åŒ–)\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_name}\")\n",
    "            print(\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "        except ImportError as e:\n",
    "            print(f\"ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "            print(\"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„ãƒ‘ã‚¹ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "    \n",
    "    def generate_response(self, message, history):\n",
    "        \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            return \"ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\"\n",
    "        \n",
    "        try:\n",
    "            # å…¥åŠ›ã®æ¤œè¨¼\n",
    "            if not message or not message.strip():\n",
    "                return \"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™ã€‚ä½•ã‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\"\n",
    "            \n",
    "            # ä¼šè©±å±¥æ­´ã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰\n",
    "            prompt = \"\"\n",
    "            if history and len(history) > 0:\n",
    "                for i, (user_msg, ai_msg) in enumerate(history):\n",
    "                    prompt += f\"è³ªå•{i+1}: {user_msg}\\nå›ç­”{i+1}: {ai_msg}\\n\\n\"\n",
    "            \n",
    "            prompt += f\"è³ªå•: {message}\\nå›ç­”: \"\n",
    "            \n",
    "            # å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # å¿œç­”ã®ç”Ÿæˆ\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # å¿œç­”ã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦å¿œç­”ã®ã¿ã‚’å–å¾—\n",
    "            if prompt in response:\n",
    "                response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            # ç©ºã®å¿œç­”ã®å ´åˆã®å‡¦ç†\n",
    "            if not response:\n",
    "                return \"å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚\"\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            return \"GPUãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ã€‚\"\n",
    "        except Exception as e:\n",
    "            return f\"å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n",
    "\n",
    "def create_chatbot_interface():\n",
    "    \"\"\"Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\"\"\"\n",
    "    chatbot = SarashinaChatbot()\n",
    "    \n",
    "    # Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®šç¾©\n",
    "    with gr.Blocks(title=\"ã‚·ãƒ³ãƒ—ãƒ«AI ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ\", theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"gray\")) as demo:\n",
    "        gr.Markdown(\"# ğŸ¤– ã‚·ãƒ³ãƒ—ãƒ«AI ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ\")\n",
    "        gr.Markdown(\"sarashina2.2-3B-instruct-v0.1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\")\n",
    "        gr.Markdown(\"ä¼šè©±ã®å±¥æ­´ã¯AIå‡ºåŠ›ã«åæ˜ ã•ã‚Œã¾ã›ã‚“\")\n",
    "        \n",
    "        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´\n",
    "        chatbot_interface = gr.Chatbot(\n",
    "            label=\"Chatbot\",\n",
    "            height=400,\n",
    "            value=[]\n",
    "        )\n",
    "        \n",
    "        # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰\n",
    "        msg = gr.Textbox(\n",
    "            label=\"è³ªå•\",\n",
    "            placeholder=\"è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\",\n",
    "            lines=1,\n",
    "            show_label=True\n",
    "        )\n",
    "        \n",
    "        # é€ä¿¡ãƒœã‚¿ãƒ³\n",
    "        submit_btn = gr.Button(\"é€ä¿¡\", variant=\"primary\")\n",
    "        \n",
    "        def user_input(message, history):\n",
    "            \"\"\"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†\"\"\"\n",
    "            try:\n",
    "                # å…¥åŠ›ã®æ¤œè¨¼\n",
    "                if not message or not str(message).strip():\n",
    "                    return \"\", history\n",
    "                \n",
    "                # å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›\n",
    "                formatted_history = []\n",
    "                if history and len(history) > 0:\n",
    "                    for user_msg, ai_msg in history:\n",
    "                        formatted_history.append([user_msg, ai_msg])\n",
    "                \n",
    "                # å¿œç­”ã‚’ç”Ÿæˆ\n",
    "                response = chatbot.generate_response(message, formatted_history)\n",
    "                \n",
    "                # å±¥æ­´ã«è¿½åŠ \n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, response])\n",
    "                \n",
    "                return \"\", history\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"å…¥åŠ›å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\"\n",
    "                if history is None:\n",
    "                    history = []\n",
    "                history.append([message, error_msg])\n",
    "                return \"\", history\n",
    "        \n",
    "        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š\n",
    "        submit_btn.click(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "        \n",
    "        # ã‚¨ãƒ³ã‚¿ãƒ¼ã‚­ãƒ¼ã§é€ä¿¡\n",
    "        msg.submit(\n",
    "            user_input,\n",
    "            inputs=[msg, chatbot_interface],\n",
    "            outputs=[msg, chatbot_interface]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¦èµ·å‹•\n",
    "    demo = create_chatbot_interface()\n",
    "    \n",
    "    # åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã‚’æ¢ã™ï¼ˆ7860ã‹ã‚‰10ä»¶ç¢ºèªï¼‰\n",
    "    port = find_available_port(7860, 10)\n",
    "    if port is None:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: ãƒãƒ¼ãƒˆ7860-7869ã§åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        print(\"ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"ãƒãƒ¼ãƒˆ {port} ã§èµ·å‹•ã—ã¾ã™...\")\n",
    "    \n",
    "    try:\n",
    "        demo.launch(\n",
    "            inbrowser=True,\n",
    "            server_name=\"127.0.0.1\",\n",
    "            server_port=port,\n",
    "            share=False,\n",
    "            show_error=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibaraki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
